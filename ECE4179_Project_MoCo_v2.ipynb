{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>Data</h2>\n",
    "\n",
    "We will be using a dataset that can be obtained directly from the torchvision package. There are 10 classes and we will be training a CNN for the image classification task. We have training, validation and test sets that are labelled with the class, and a large unlabeled set.\n",
    "\n",
    "We will simulating a low training data scenario by only sampling a small percentage of the labelled data (10%) as training data. The remaining examples will be used as the validation set.\n",
    "\n",
    "To get the labelled data, change the dataset_dir to something suitable for your machine, and execute the following (you will then probably want to wrap the dataset objects in a PyTorch DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = r\"\\\\ad.monash.edu\\home\\User030\\rbea0007\\Documents\\ECE6179\\VS Code\\Course Project\"\n",
    "#For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
    "\n",
    "#All images are 3x96x96\n",
    "image_size = 96\n",
    "#Example batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(image_size, padding=4),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_unlabelled = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.CenterCrop(image_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train]) #500 train, 4500 val\n",
    "\n",
    "#Load test set\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=True) #8000 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the unlabelled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=True) #100,000 unlabelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find later that you want to make changes to how the unlabelled data is loaded. This might require you sub-classing the STL10 class used above or to create your own dataloader similar to the Pytorch one.\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/stl10.html#STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Momentum Contrast Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    def __init__(self, base_encoder, model_type, dim=128, K=8192, m=0.999, T=0.07):\n",
    "        super(MoCo, self).__init__()\n",
    "        self.encoder_q = base_encoder\n",
    "        self.encoder_k = base_encoder\n",
    "\n",
    "        if model_type == 'resnet':\n",
    "            # Replace the final layer to output the desired dimension\n",
    "            self.encoder_q.fc = nn.Linear(self.encoder_q.fc.in_features, dim)\n",
    "            self.encoder_k.fc = nn.Linear(self.encoder_k.fc.in_features, dim)\n",
    "        elif model_type == 'efficientnet':\n",
    "            in_features = self.encoder_q.classifier[1].in_features  # Access the last layer\n",
    "            self.encoder_q.classifier[1] = nn.Linear(in_features, dim)\n",
    "            self.encoder_k.classifier[1] = nn.Linear(in_features, dim)\n",
    "        elif model_type == 'vit':\n",
    "            in_features = self.encoder_q.heads.head.in_features  # Access in_features of ViT head\n",
    "            self.encoder_q.heads.head = nn.Linear(in_features, dim)  # Modify for query\n",
    "            self.encoder_k.heads.head = nn.Linear(in_features, dim)  # Modify for keys\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model: choose 'resnet', 'efficientnet', or 'vit'\")\n",
    "        \n",
    "        \n",
    "        for param in self.encoder_k.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        self.K = K # queue size\n",
    "        self.m = m # momentum\n",
    "        self.T = T # temperature\n",
    "\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))  # Register queue as a buffer\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))  # Pointer for queue\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.encoder_q(x)\n",
    "        q = nn.functional.normalize(q, dim=1)\n",
    "        return q\n",
    "    \n",
    "    @torch.no_grad()   \n",
    "    def update_key_encoder(self):\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def enqueue_and_dequeue(self, keys):\n",
    "        keys = nn.functional.normalize(keys, dim=1)\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr.item())\n",
    "\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.K\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    def contrastive_loss(self, query):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Compute logits\n",
    "        logits = torch.mm(query, self.queue.clone().detach()) / self.T\n",
    "        labels = torch.arange(batch_size).cuda()\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain on unlabelled data\n",
    "def pretrain_model(model, dataloader, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "    model.train()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=1e-4) # get rid of optimiser?\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, _ in dataloader:\n",
    "            images = images.cuda()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Generate augmented views\n",
    "            images_q = images  # Original images as query\n",
    "            ### APPLY AUGMENTATIONS ###\n",
    "            images_k = images \n",
    "            images_k = transforms.functional.hflip(images_k)  # Horizontal flip\n",
    "            images_k = transforms.functional.adjust_brightness(images_k, 1.2)  # Brightness adjustment\n",
    "            images_k = transforms.functional.adjust_contrast(images_k, 1.2)  # Contrast adjustment\n",
    "            images_k = transforms.functional.rotate(images_k, angle=15)  # Rotate\n",
    "           \n",
    "            # Forward pass\n",
    "            query = model(images_q)\n",
    "            query.requires_grad_()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.update_key_encoder()  # Update the key encoder\n",
    "                key = model.encoder_k(images_k)\n",
    "\n",
    "            # Contrastive loss\n",
    "            loss = model.contrastive_loss(query)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Update key encoder\n",
    "            model.enqueue_and_dequeue(key)\n",
    "            #model.update_key_encoder()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader): # Accuracy and Macro f1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    # Macro F1\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"Macro F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model, train_loader, val_loader, num_epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model using resnet18\n",
    "model_resnet = torch.hub.load('pytorch/vision', 'resnet18', weights=\"ResNet18_Weights.IMAGENET1K_V1\")\n",
    "base_encoder = model_resnet\n",
    "model_moco1 = MoCo(base_encoder, model_type='resnet')\n",
    "\n",
    "for name, param in model_resnet.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain the model\n",
    "pretrain_model(model_moco1, unlabelled_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model_moco1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the model\n",
    "num_classes = 10  # 10 classes in STL10\n",
    "model_moco1.encoder_q.fc = nn.Linear(model_moco1.encoder_q.fc.in_features, num_classes) # Adjust fc layer\n",
    "finetune_model(model_moco1, train_loader, valid_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the finetuned model\n",
    "model_moco1.eval()\n",
    "evaluate_model(model_moco1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model using efficientnetb0\n",
    "# May have to clear all outputs and run data/user-defined functions again\n",
    "model_efficient = torch.hub.load('pytorch/vision', 'efficientnet_b0', weights=\"EfficientNet_B0_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "base_encoder = model_efficient\n",
    "model_moco2 = MoCo(base_encoder, model_type='efficientnet')\n",
    "\n",
    "for name, param in model_efficient.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain the model\n",
    "pretrain_model(model_moco2, unlabelled_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model_moco2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the model\n",
    "num_classes = 10  # 10 classes in STL10\n",
    "in_features = model_moco2.encoder_q.classifier[1].in_features # Adjust fc layer\n",
    "model_moco2.encoder_q.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "finetune_model(model_moco2, train_loader, valid_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the finetuned model\n",
    "model_moco2.eval()\n",
    "evaluate_model(model_moco2, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbea0007\\AppData\\Local\\anaconda3\\envs\\ECE6179\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rbea0007\\AppData\\Local\\anaconda3\\envs\\ECE6179\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: class_token, Shape: torch.Size([1, 1, 768])\n",
      "Name: conv_proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Name: conv_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.pos_embedding, Shape: torch.Size([1, 197, 768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.ln.weight, Shape: torch.Size([768])\n",
      "Name: encoder.ln.bias, Shape: torch.Size([768])\n",
      "Name: heads.head.weight, Shape: torch.Size([1000, 768])\n",
      "Name: heads.head.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Set image size to 224x224 to match the input size of ViT\n",
    "transform_train_vit = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_unlabelled_vit = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test_vit = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load train and validation sets without redownloading data\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train_vit, download=False)\n",
    "\n",
    "# Use 10% of the data for training (simulating a low data scenario)\n",
    "num_train = int(len(trainval_set) * 0.1)\n",
    "\n",
    "# Split data into train/validation sets with a fixed random seed\n",
    "torch.manual_seed(0)  # Ensure reproducibility\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set) - num_train])\n",
    "\n",
    "# Load test set without redownloading data\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test_vit, download=False)\n",
    "\n",
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled_vit, download=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained Vision Transformer (ViT) model from torchvision models\n",
    "from torchvision import models\n",
    "\n",
    "model_vit = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Print the model structure to verify the changes\n",
    "for name, param in model_vit.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")\n",
    "\n",
    "base_encoder = model_vit\n",
    "model_moco3 = MoCo(base_encoder, model_type='vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbea0007\\AppData\\Local\\anaconda3\\envs\\ECE6179\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Pretrain the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pretrain_model(model_moco3, unlabelled_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m evaluate_model(model_moco3, test_loader)\n",
      "Cell \u001b[1;32mIn[50], line 33\u001b[0m, in \u001b[0;36mpretrain_model\u001b[1;34m(model, dataloader, num_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m     key \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder_k(images_k)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Contrastive loss\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcontrastive_loss(query)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[49], line 60\u001b[0m, in \u001b[0;36mMoCo.contrastive_loss\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Compute logits\u001b[39;00m\n\u001b[0;32m     59\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m---> 60\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(batch_size)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, labels)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pretrain the model\n",
    "pretrain_model(model_moco3, unlabelled_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model_moco3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the model\n",
    "num_classes = 10  # 10 classes in STL10\n",
    "in_features = model_moco3.encoder_q.heads.head.in_features\n",
    "model_moco3.encoder_q.heads.head = nn.Linear(in_features, num_classes) \n",
    "\n",
    "finetune_model(model_moco3, train_loader, valid_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the finetuned model\n",
    "model_moco3.eval()\n",
    "evaluate_model(model_moco3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>Data</h2>\n",
    "\n",
    "We will be using a dataset that can be obtained directly from the torchvision package. There are 10 classes and we will be training a CNN for the image classification task. We have training, validation and test sets that are labelled with the class, and a large unlabeled set.\n",
    "\n",
    "We will simulating a low training data scenario by only sampling a small percentage of the labelled data (10%) as training data. The remaining examples will be used as the validation set.\n",
    "\n",
    "To get the labelled data, change the dataset_dir to something suitable for your machine, and execute the following (you will then probably want to wrap the dataset objects in a PyTorch DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import time\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = \"../data\"\n",
    "#For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
    "\n",
    "#All images are 3x96x96\n",
    "image_size = 96                 \n",
    "#Example batch size\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print the GPU type and use device</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print the number of processors in cpu</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(image_size, padding=4),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_unlabelled = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.CenterCrop(image_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
    "\n",
    "#Load test set\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the unlabelled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find later that you want to make changes to how the unlabelled data is loaded. This might require you sub-classing the STL10 class used above or to create your own dataloader similar to the Pytorch one.\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/stl10.html#STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Marco F1 Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function to calculate F1 score\n",
    "def test_model_with_f1(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect all predictions and labels for F1-score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate the Macro F1-score for each class\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Alternatively, you can get a detailed report for all classes\n",
    "    report = classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(10)])\n",
    "    \n",
    "    print(f\"Macro F1-score: {f1}\")\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Let's use a ResNet18 architecture for our CNN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: conv1.weight, Shape: torch.Size([64, 3, 7, 7])\n",
      "Name: bn1.weight, Shape: torch.Size([64])\n",
      "Name: bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer2.0.conv1.weight, Shape: torch.Size([128, 64, 3, 3])\n",
      "Name: layer2.0.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.0.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.0.weight, Shape: torch.Size([128, 64, 1, 1])\n",
      "Name: layer2.0.downsample.1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv1.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer3.0.conv1.weight, Shape: torch.Size([256, 128, 3, 3])\n",
      "Name: layer3.0.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.0.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.0.weight, Shape: torch.Size([256, 128, 1, 1])\n",
      "Name: layer3.0.downsample.1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv1.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer4.0.conv1.weight, Shape: torch.Size([512, 256, 3, 3])\n",
      "Name: layer4.0.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.0.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn2.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.0.weight, Shape: torch.Size([512, 256, 1, 1])\n",
      "Name: layer4.0.downsample.1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv1.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn2.bias, Shape: torch.Size([512])\n",
      "Name: fc.weight, Shape: torch.Size([1000, 512])\n",
      "Name: fc.bias, Shape: torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\惟神君/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# We will keep this for later\n",
    "model0 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "\n",
    "for name, param in model0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9302005350589753\n",
      "Epoch [2/10], Loss: 0.3454402476549149\n",
      "Epoch [3/10], Loss: 0.2133716717362404\n",
      "Epoch [4/10], Loss: 0.13532407060265542\n",
      "Epoch [5/10], Loss: 0.08492341302335263\n",
      "Epoch [6/10], Loss: 0.06833139136433601\n",
      "Epoch [7/10], Loss: 0.05621107928454876\n",
      "Epoch [8/10], Loss: 0.04910865388810635\n",
      "Epoch [9/10], Loss: 0.03994848933070898\n",
      "Epoch [10/10], Loss: 0.04816233925521374\n",
      "Test Accuracy: 81.9125%\n"
     ]
    }
   ],
   "source": [
    "# add your code below. Define the number of epochs to train for based on your laptop's performance\n",
    "# on a CPU, my machine takes about 2 minutes per epoch\n",
    "num_epochs=10\n",
    "\n",
    "# Create a new model from the pretrained one\n",
    "model_resnet18 = deepcopy(model0)  # <--- study the code here\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes (e.g., 10 for STL10)\n",
    "num_classes = 10\n",
    "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, num_classes)\n",
    "\n",
    "model = model_resnet18.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_resnet18.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_resnet18.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_resnet18(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model_resnet18.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_resnet18(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print test accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.9125%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(model_resnet18, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.8198189355296931\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.82      0.88      0.85       800\n",
      "     Class 1       0.82      0.87      0.84       800\n",
      "     Class 2       0.85      0.94      0.89       800\n",
      "     Class 3       0.63      0.86      0.73       800\n",
      "     Class 4       0.87      0.81      0.84       800\n",
      "     Class 5       0.78      0.71      0.74       800\n",
      "     Class 6       0.91      0.78      0.84       800\n",
      "     Class 7       0.81      0.81      0.81       800\n",
      "     Class 8       0.98      0.67      0.80       800\n",
      "     Class 9       0.84      0.86      0.85       800\n",
      "\n",
      "    accuracy                           0.82      8000\n",
      "   macro avg       0.83      0.82      0.82      8000\n",
      "weighted avg       0.83      0.82      0.82      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(model_resnet18, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: features.0.0.weight, Shape: torch.Size([32, 3, 3, 3])\n",
      "Name: features.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.0.weight, Shape: torch.Size([32, 1, 3, 3])\n",
      "Name: features.1.0.block.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.1.fc1.weight, Shape: torch.Size([8, 32, 1, 1])\n",
      "Name: features.1.0.block.1.fc1.bias, Shape: torch.Size([8])\n",
      "Name: features.1.0.block.1.fc2.weight, Shape: torch.Size([32, 8, 1, 1])\n",
      "Name: features.1.0.block.1.fc2.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.2.0.weight, Shape: torch.Size([16, 32, 1, 1])\n",
      "Name: features.1.0.block.2.1.weight, Shape: torch.Size([16])\n",
      "Name: features.1.0.block.2.1.bias, Shape: torch.Size([16])\n",
      "Name: features.2.0.block.0.0.weight, Shape: torch.Size([96, 16, 1, 1])\n",
      "Name: features.2.0.block.0.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.0.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.0.weight, Shape: torch.Size([96, 1, 3, 3])\n",
      "Name: features.2.0.block.1.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.2.fc1.weight, Shape: torch.Size([4, 96, 1, 1])\n",
      "Name: features.2.0.block.2.fc1.bias, Shape: torch.Size([4])\n",
      "Name: features.2.0.block.2.fc2.weight, Shape: torch.Size([96, 4, 1, 1])\n",
      "Name: features.2.0.block.2.fc2.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.3.0.weight, Shape: torch.Size([24, 96, 1, 1])\n",
      "Name: features.2.0.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.0.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.2.1.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.0.weight, Shape: torch.Size([144, 1, 3, 3])\n",
      "Name: features.2.1.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.2.1.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.2.1.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.2.1.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.3.0.weight, Shape: torch.Size([24, 144, 1, 1])\n",
      "Name: features.2.1.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.3.0.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.3.0.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.0.weight, Shape: torch.Size([144, 1, 5, 5])\n",
      "Name: features.3.0.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.3.0.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.3.0.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.3.0.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.3.0.weight, Shape: torch.Size([40, 144, 1, 1])\n",
      "Name: features.3.0.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.0.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.3.1.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.0.weight, Shape: torch.Size([240, 1, 5, 5])\n",
      "Name: features.3.1.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.3.1.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.3.1.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.3.1.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.3.0.weight, Shape: torch.Size([40, 240, 1, 1])\n",
      "Name: features.3.1.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.4.0.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.4.0.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.0.weight, Shape: torch.Size([240, 1, 3, 3])\n",
      "Name: features.4.0.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.4.0.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.4.0.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.4.0.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.3.0.weight, Shape: torch.Size([80, 240, 1, 1])\n",
      "Name: features.4.0.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.0.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.1.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.1.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.1.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.1.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.1.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.1.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.2.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.2.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.2.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.2.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.2.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.2.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.5.0.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.5.0.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.0.weight, Shape: torch.Size([480, 1, 5, 5])\n",
      "Name: features.5.0.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.5.0.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.5.0.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.5.0.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.3.0.weight, Shape: torch.Size([112, 480, 1, 1])\n",
      "Name: features.5.0.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.0.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.1.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.1.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.1.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.1.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.1.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.1.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.2.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.2.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.2.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.2.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.2.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.2.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.6.0.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.6.0.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.6.0.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.6.0.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.6.0.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.6.0.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.3.0.weight, Shape: torch.Size([192, 672, 1, 1])\n",
      "Name: features.6.0.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.0.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.1.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.1.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.1.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.1.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.1.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.1.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.2.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.2.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.2.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.2.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.2.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.2.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.3.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.3.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.3.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.3.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.3.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.3.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.7.0.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.7.0.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.0.weight, Shape: torch.Size([1152, 1, 3, 3])\n",
      "Name: features.7.0.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.7.0.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.7.0.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.7.0.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.3.0.weight, Shape: torch.Size([320, 1152, 1, 1])\n",
      "Name: features.7.0.block.3.1.weight, Shape: torch.Size([320])\n",
      "Name: features.7.0.block.3.1.bias, Shape: torch.Size([320])\n",
      "Name: features.8.0.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "Name: features.8.1.weight, Shape: torch.Size([1280])\n",
      "Name: features.8.1.bias, Shape: torch.Size([1280])\n",
      "Name: classifier.1.weight, Shape: torch.Size([1000, 1280])\n",
      "Name: classifier.1.bias, Shape: torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\惟神君/.cache\\torch\\hub\\pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained EfficientNet-B0 model from torchvision hub\n",
    "efficientnetb0 = torch.hub.load('pytorch/vision', 'efficientnet_b0', weights=\"EfficientNet_B0_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "for name, param in efficientnetb0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnetb0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 50\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Modify the last fully connected layer to output 10 classes\n",
    "num_classes = 10\n",
    "efficientnetb0.classifier[1] = nn.Linear(efficientnetb0.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "efficientnetb0 = efficientnetb0.to(device)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss) and optimizer (Adam)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(efficientnetb0.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation code\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Loop through batches in the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation after every epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy}%\")\n",
    "\n",
    "train_model(efficientnetb0, train_loader, valid_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the test function\n",
    "test_model(efficientnetb0, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(efficientnetb0, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image size to 224x224 to match the input size of ViT\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_unlabelled = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load train and validation sets without redownloading data\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=False)\n",
    "\n",
    "# Use 10% of the data for training (simulating a low data scenario)\n",
    "num_train = int(len(trainval_set) * 0.1)\n",
    "\n",
    "# Split data into train/validation sets with a fixed random seed\n",
    "torch.manual_seed(0)  # Ensure reproducibility\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set) - num_train])\n",
    "\n",
    "# Load test set without redownloading data\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)\n",
    "\n",
    "# Create DataLoader for train, validation, and test sets\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: class_token, Shape: torch.Size([1, 1, 768])\n",
      "Name: conv_proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Name: conv_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.pos_embedding, Shape: torch.Size([1, 197, 768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.ln.weight, Shape: torch.Size([768])\n",
      "Name: encoder.ln.bias, Shape: torch.Size([768])\n",
      "Name: heads.head.weight, Shape: torch.Size([1000, 768])\n",
      "Name: heads.head.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Vision Transformer (ViT) model from torchvision models\n",
    "vit = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Print the model structure to verify the changes\n",
    "for name, param in vit.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing layer: heads.head.weight\n",
      "Unfreezing layer: heads.head.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Use your train_loader and valid_loader\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 62\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[0;32m     59\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     60\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 62\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# End time for epoch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Modify the last fully connected layer to match the number of classes (e.g., 10 classes)\n",
    "num_classes = 10\n",
    "vit.heads.head = nn.Linear(vit.heads.head.in_features, num_classes)\n",
    "\n",
    "# Freeze all layers except the last fully connected layer (heads.head)\n",
    "for name, param in vit.named_parameters():\n",
    "    if 'heads.head' in name:  # Only unfreeze the heads.head layer\n",
    "        param.requires_grad = True\n",
    "        print(f\"Unfreezing layer: {name}\")\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "vit = vit.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropy for classification\n",
    "# optimizer = optim.Adam(vit.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(vit.parameters(), lr=0.016)\n",
    "\n",
    "# Training and validation code with timing\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Start time for epoch\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Loop through batches in the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation after every epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # End time for epoch\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy}%, Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "# Use your train_loader and valid_loader\n",
    "train_model(vit, train_loader, valid_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the test function\n",
    "test_model(vit, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(vit, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

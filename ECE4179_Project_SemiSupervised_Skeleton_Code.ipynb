{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>Data</h2>\n",
    "\n",
    "We will be using a dataset that can be obtained directly from the torchvision package. There are 10 classes and we will be training a CNN for the image classification task. We have training, validation and test sets that are labelled with the class, and a large unlabeled set.\n",
    "\n",
    "We will simulating a low training data scenario by only sampling a small percentage of the labelled data (10%) as training data. The remaining examples will be used as the validation set.\n",
    "\n",
    "To get the labelled data, change the dataset_dir to something suitable for your machine, and execute the following (you will then probably want to wrap the dataset objects in a PyTorch DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = \"../../CNN-VAE/data\"\n",
    "#For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
    "\n",
    "#All images are 3x96x96\n",
    "image_size = 96\n",
    "#Example batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(image_size, padding=4),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_unlabelled = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.CenterCrop(image_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
    "\n",
    "#Load test set\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the unlabelled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find later that you want to make changes to how the unlabelled data is loaded. This might require you sub-classing the STL10 class used above or to create your own dataloader similar to the Pytorch one.\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/stl10.html#STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Marco F1 Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function to calculate F1 score\n",
    "def test_model_with_f1(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect all predictions and labels for F1-score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate the Macro F1-score for each class\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Alternatively, you can get a detailed report for all classes\n",
    "    report = classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(10)])\n",
    "    \n",
    "    print(f\"Macro F1-score: {f1}\")\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Let's use a ResNet18 architecture for our CNN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\weita/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\weita\\anaconda3\\envs\\env_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\env_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: conv1.weight, Shape: torch.Size([64, 3, 7, 7])\n",
      "Name: bn1.weight, Shape: torch.Size([64])\n",
      "Name: bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer2.0.conv1.weight, Shape: torch.Size([128, 64, 3, 3])\n",
      "Name: layer2.0.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.0.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.0.weight, Shape: torch.Size([128, 64, 1, 1])\n",
      "Name: layer2.0.downsample.1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv1.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer3.0.conv1.weight, Shape: torch.Size([256, 128, 3, 3])\n",
      "Name: layer3.0.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.0.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.0.weight, Shape: torch.Size([256, 128, 1, 1])\n",
      "Name: layer3.0.downsample.1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv1.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer4.0.conv1.weight, Shape: torch.Size([512, 256, 3, 3])\n",
      "Name: layer4.0.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.0.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn2.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.0.weight, Shape: torch.Size([512, 256, 1, 1])\n",
      "Name: layer4.0.downsample.1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv1.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn2.bias, Shape: torch.Size([512])\n",
      "Name: fc.weight, Shape: torch.Size([1000, 512])\n",
      "Name: fc.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# We will keep this for later\n",
    "model0 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "\n",
    "for name, param in model0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.5088257975876331\n",
      "Epoch [2/10], Loss: 1.0582835879176855\n",
      "Epoch [3/10], Loss: 0.6645623072981834\n",
      "Epoch [4/10], Loss: 0.4580966904759407\n",
      "Epoch [5/10], Loss: 0.320215062238276\n",
      "Epoch [6/10], Loss: 0.33488638303242624\n",
      "Epoch [7/10], Loss: 0.35667672008275986\n",
      "Epoch [8/10], Loss: 0.31174362311139703\n",
      "Epoch [9/10], Loss: 0.28476891526952386\n",
      "Epoch [10/10], Loss: 0.339187182020396\n",
      "Test Accuracy: 60.5125%\n"
     ]
    }
   ],
   "source": [
    "# add your code below. Define the number of epochs to train for based on your laptop's performance\n",
    "# on a CPU, my machine takes about 2 minutes per epoch\n",
    "num_epochs=10\n",
    "\n",
    "# Create a new model from the pretrained one\n",
    "model_resnet18 = deepcopy(model0)  # <--- study the code here\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes (e.g., 10 for STL10)\n",
    "num_classes = 10\n",
    "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, num_classes)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_resnet18.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_resnet18.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_resnet18.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_resnet18(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model_resnet18.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_resnet18(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print test accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 60.5125%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(model_resnet18, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.6034147584709965\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.80      0.66      0.72       800\n",
      "     Class 1       0.79      0.52      0.63       800\n",
      "     Class 2       0.82      0.59      0.68       800\n",
      "     Class 3       0.48      0.54      0.51       800\n",
      "     Class 4       0.51      0.76      0.61       800\n",
      "     Class 5       0.44      0.51      0.47       800\n",
      "     Class 6       0.83      0.34      0.49       800\n",
      "     Class 7       0.59      0.54      0.56       800\n",
      "     Class 8       0.68      0.72      0.70       800\n",
      "     Class 9       0.53      0.87      0.66       800\n",
      "\n",
      "    accuracy                           0.61      8000\n",
      "   macro avg       0.65      0.61      0.60      8000\n",
      "weighted avg       0.65      0.61      0.60      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(model_resnet18, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: features.0.0.weight, Shape: torch.Size([32, 3, 3, 3])\n",
      "Name: features.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.0.weight, Shape: torch.Size([32, 1, 3, 3])\n",
      "Name: features.1.0.block.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.1.fc1.weight, Shape: torch.Size([8, 32, 1, 1])\n",
      "Name: features.1.0.block.1.fc1.bias, Shape: torch.Size([8])\n",
      "Name: features.1.0.block.1.fc2.weight, Shape: torch.Size([32, 8, 1, 1])\n",
      "Name: features.1.0.block.1.fc2.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.2.0.weight, Shape: torch.Size([16, 32, 1, 1])\n",
      "Name: features.1.0.block.2.1.weight, Shape: torch.Size([16])\n",
      "Name: features.1.0.block.2.1.bias, Shape: torch.Size([16])\n",
      "Name: features.2.0.block.0.0.weight, Shape: torch.Size([96, 16, 1, 1])\n",
      "Name: features.2.0.block.0.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.0.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.0.weight, Shape: torch.Size([96, 1, 3, 3])\n",
      "Name: features.2.0.block.1.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.2.fc1.weight, Shape: torch.Size([4, 96, 1, 1])\n",
      "Name: features.2.0.block.2.fc1.bias, Shape: torch.Size([4])\n",
      "Name: features.2.0.block.2.fc2.weight, Shape: torch.Size([96, 4, 1, 1])\n",
      "Name: features.2.0.block.2.fc2.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.3.0.weight, Shape: torch.Size([24, 96, 1, 1])\n",
      "Name: features.2.0.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.0.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.2.1.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.0.weight, Shape: torch.Size([144, 1, 3, 3])\n",
      "Name: features.2.1.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.2.1.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.2.1.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.2.1.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.3.0.weight, Shape: torch.Size([24, 144, 1, 1])\n",
      "Name: features.2.1.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.3.0.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.3.0.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.0.weight, Shape: torch.Size([144, 1, 5, 5])\n",
      "Name: features.3.0.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.3.0.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.3.0.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.3.0.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.3.0.weight, Shape: torch.Size([40, 144, 1, 1])\n",
      "Name: features.3.0.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.0.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.3.1.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.0.weight, Shape: torch.Size([240, 1, 5, 5])\n",
      "Name: features.3.1.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.3.1.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.3.1.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.3.1.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.3.0.weight, Shape: torch.Size([40, 240, 1, 1])\n",
      "Name: features.3.1.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.4.0.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.4.0.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.0.weight, Shape: torch.Size([240, 1, 3, 3])\n",
      "Name: features.4.0.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.4.0.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.4.0.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.4.0.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.3.0.weight, Shape: torch.Size([80, 240, 1, 1])\n",
      "Name: features.4.0.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.0.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.1.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.1.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.1.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.1.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.1.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.1.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.2.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.2.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.2.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.2.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.2.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.2.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.5.0.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.5.0.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.0.weight, Shape: torch.Size([480, 1, 5, 5])\n",
      "Name: features.5.0.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.5.0.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.5.0.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.5.0.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.3.0.weight, Shape: torch.Size([112, 480, 1, 1])\n",
      "Name: features.5.0.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.0.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.1.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.1.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.1.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.1.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.1.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.1.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.2.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.2.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.2.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.2.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.2.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.2.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.6.0.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.6.0.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.6.0.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.6.0.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.6.0.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.6.0.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.3.0.weight, Shape: torch.Size([192, 672, 1, 1])\n",
      "Name: features.6.0.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.0.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.1.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.1.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.1.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.1.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.1.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.1.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.2.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.2.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.2.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.2.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.2.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.2.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.3.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.3.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.3.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.3.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.3.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.3.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.7.0.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.7.0.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.0.weight, Shape: torch.Size([1152, 1, 3, 3])\n",
      "Name: features.7.0.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.7.0.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.7.0.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.7.0.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.3.0.weight, Shape: torch.Size([320, 1152, 1, 1])\n",
      "Name: features.7.0.block.3.1.weight, Shape: torch.Size([320])\n",
      "Name: features.7.0.block.3.1.bias, Shape: torch.Size([320])\n",
      "Name: features.8.0.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "Name: features.8.1.weight, Shape: torch.Size([1280])\n",
      "Name: features.8.1.bias, Shape: torch.Size([1280])\n",
      "Name: classifier.1.weight, Shape: torch.Size([1000, 1280])\n",
      "Name: classifier.1.bias, Shape: torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\weita/.cache\\torch\\hub\\pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained EfficientNet-B0 model from torchvision hub\n",
    "efficientnetb0 = torch.hub.load('pytorch/vision', 'efficientnet_b0', weights=\"EfficientNet_B0_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "for name, param in efficientnetb0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6454911679029465, Accuracy: 69.8%\n",
      "Epoch 2/10, Loss: 0.5324282217770815, Accuracy: 72.2%\n",
      "Epoch 3/10, Loss: 0.26369882468134165, Accuracy: 72.64444444444445%\n",
      "Epoch 4/10, Loss: 0.24109109956771135, Accuracy: 71.77777777777777%\n",
      "Epoch 5/10, Loss: 0.26976340916007757, Accuracy: 71.04444444444445%\n",
      "Epoch 6/10, Loss: 0.32619810523465276, Accuracy: 70.17777777777778%\n",
      "Epoch 7/10, Loss: 0.2994155599735677, Accuracy: 68.62222222222222%\n",
      "Epoch 8/10, Loss: 0.3010925780981779, Accuracy: 71.02222222222223%\n",
      "Epoch 9/10, Loss: 0.17950862017460167, Accuracy: 71.5111111111111%\n",
      "Epoch 10/10, Loss: 0.146677186479792, Accuracy: 71.57777777777778%\n"
     ]
    }
   ],
   "source": [
    "# Modify the last fully connected layer to output 10 classes\n",
    "num_classes = 10\n",
    "efficientnetb0.classifier[1] = nn.Linear(efficientnetb0.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "efficientnetb0 = efficientnetb0.to(device)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss) and optimizer (Adam)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(efficientnetb0.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation code\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Loop through batches in the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation after every epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy}%\")\n",
    "\n",
    "train_model(efficientnetb0, train_loader, valid_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.0625%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(efficientnetb0, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.7225164368992557\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.82      0.84      0.83       800\n",
      "     Class 1       0.79      0.76      0.77       800\n",
      "     Class 2       0.72      0.96      0.82       800\n",
      "     Class 3       0.64      0.67      0.66       800\n",
      "     Class 4       0.66      0.81      0.73       800\n",
      "     Class 5       0.67      0.39      0.49       800\n",
      "     Class 6       0.68      0.81      0.74       800\n",
      "     Class 7       0.66      0.67      0.67       800\n",
      "     Class 8       0.86      0.87      0.86       800\n",
      "     Class 9       0.86      0.53      0.66       800\n",
      "\n",
      "    accuracy                           0.73      8000\n",
      "   macro avg       0.74      0.73      0.72      8000\n",
      "weighted avg       0.74      0.73      0.72      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(efficientnetb0, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image size to 224x224 to match the input size of ViT\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_unlabelled = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load train and validation sets without redownloading data\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=False)\n",
    "\n",
    "# Use 10% of the data for training (simulating a low data scenario)\n",
    "num_train = int(len(trainval_set) * 0.1)\n",
    "\n",
    "# Split data into train/validation sets with a fixed random seed\n",
    "torch.manual_seed(0)  # Ensure reproducibility\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set) - num_train])\n",
    "\n",
    "# Load test set without redownloading data\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)\n",
    "\n",
    "# Create DataLoader for train, validation, and test sets\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=32)\n",
    "valid_loader = DataLoader(val_set, batch_size=32)\n",
    "test_loader = DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\env_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\env_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: class_token, Shape: torch.Size([1, 1, 768])\n",
      "Name: conv_proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Name: conv_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.pos_embedding, Shape: torch.Size([1, 197, 768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.ln.weight, Shape: torch.Size([768])\n",
      "Name: encoder.ln.bias, Shape: torch.Size([768])\n",
      "Name: heads.head.weight, Shape: torch.Size([1000, 768])\n",
      "Name: heads.head.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Vision Transformer (ViT) model from torchvision models\n",
    "vit = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Print the model structure to verify the changes\n",
    "for name, param in vit.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing layer: heads.head.weight\n",
      "Unfreezing layer: heads.head.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\env_gpu\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.67296726629138, Accuracy: 89.37777777777778%\n",
      "Epoch 2/10, Loss: 0.674645284190774, Accuracy: 93.57777777777778%\n",
      "Epoch 3/10, Loss: 0.34950040467083454, Accuracy: 95.88888888888889%\n",
      "Epoch 4/10, Loss: 0.2200820599682629, Accuracy: 96.33333333333333%\n",
      "Epoch 5/10, Loss: 0.1608294164761901, Accuracy: 96.57777777777778%\n",
      "Epoch 6/10, Loss: 0.12484467728063464, Accuracy: 96.88888888888889%\n",
      "Epoch 7/10, Loss: 0.10040531447157264, Accuracy: 97.06666666666666%\n",
      "Epoch 8/10, Loss: 0.08038817113265395, Accuracy: 96.86666666666666%\n",
      "Epoch 9/10, Loss: 0.06875781808048487, Accuracy: 97.28888888888889%\n",
      "Epoch 10/10, Loss: 0.058618630515411496, Accuracy: 96.84444444444445%\n"
     ]
    }
   ],
   "source": [
    "# Modify the last fully connected layer to match the number of classes (e.g., 10 classes)\n",
    "num_classes = 10\n",
    "vit.heads.head = nn.Linear(vit.heads.head.in_features, num_classes)\n",
    "\n",
    "# Freeze all layers except the last fully connected layer (heads.head)\n",
    "for name, param in vit.named_parameters():\n",
    "    if 'heads.head' in name:  # Only unfreeze the heads.head layer\n",
    "        param.requires_grad = True\n",
    "        print(f\"Unfreezing layer: {name}\")\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "vit = vit.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropy for classification\n",
    "optimizer = optim.Adam(vit.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation code\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Loop through batches in the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation after every epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy}%\")\n",
    "\n",
    "# Use your train_loader and valid_loader\n",
    "train_model(vit, train_loader, valid_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.3625%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(vit, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.9736377311179627\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.98      0.98       800\n",
      "     Class 1       0.99      0.97      0.98       800\n",
      "     Class 2       0.99      0.98      0.99       800\n",
      "     Class 3       0.97      0.97      0.97       800\n",
      "     Class 4       0.98      0.93      0.95       800\n",
      "     Class 5       0.94      0.97      0.95       800\n",
      "     Class 6       0.94      0.97      0.95       800\n",
      "     Class 7       0.98      0.99      0.99       800\n",
      "     Class 8       0.98      0.98      0.98       800\n",
      "     Class 9       0.98      0.99      0.99       800\n",
      "\n",
      "    accuracy                           0.97      8000\n",
      "   macro avg       0.97      0.97      0.97      8000\n",
      "weighted avg       0.97      0.97      0.97      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(vit, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

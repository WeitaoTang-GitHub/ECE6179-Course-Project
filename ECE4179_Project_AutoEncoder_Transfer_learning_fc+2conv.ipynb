{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>Data</h2>\n",
    "\n",
    "We will be using a dataset that can be obtained directly from the torchvision package. There are 10 classes and we will be training a CNN for the image classification task. We have training, validation and test sets that are labelled with the class, and a large unlabeled set.\n",
    "\n",
    "We will simulating a low training data scenario by only sampling a small percentage of the labelled data (10%) as training data. The remaining examples will be used as the validation set.\n",
    "\n",
    "To get the labelled data, change the dataset_dir to something suitable for your machine, and execute the following (you will then probably want to wrap the dataset objects in a PyTorch DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = \"../../CNN-VAE/data\"\n",
    "#For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
    "\n",
    "#All images are 3x96x96\n",
    "image_size = 96\n",
    "#Example batch size\n",
    "batch_size = 16\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "# Define the number of classes\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(image_size, padding=4),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_unlabelled = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.CenterCrop(image_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
    "\n",
    "#Load test set\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the unlabelled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the length of unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabelled_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only get the 1/1000 for unlabled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the size of the subset (1/1000 of the full dataset)\n",
    "subset_size = len(unlabelled_set) // 1000  # This will be 100 samples\n",
    "\n",
    "# Randomly select indices for the subset\n",
    "random_indices = random.sample(range(len(unlabelled_set)), subset_size)\n",
    "\n",
    "# Create a subset of the unlabelled dataset\n",
    "unlabelled_subset = Subset(unlabelled_set, random_indices)\n",
    "\n",
    "# Now, create the DataLoader using the subset\n",
    "unlabelled_loader = DataLoader(unlabelled_subset, shuffle=True, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find later that you want to make changes to how the unlabelled data is loaded. This might require you sub-classing the STL10 class used above or to create your own dataloader similar to the Pytorch one.\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/stl10.html#STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function\n",
    "def test_model(model, test_loader):\n",
    "    # Define the device inside the function\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Marco F1 Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function to calculate F1 score\n",
    "def test_model_with_f1(model, test_loader):\n",
    "    # Define the device inside the function\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect all predictions and labels for F1-score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate the Macro F1-score for each class\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Alternatively, you can get a detailed report for all classes\n",
    "    report = classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(10)])\n",
    "    \n",
    "    print(f\"Macro F1-score: {f1}\")\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Let's use a ResNet18 architecture for our CNN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder class with ResNet, EfficientNet, and ViT implementations\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, base_model, model_name='resnet'):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if model_name == 'resnet':\n",
    "            # Directly assign the encoder layers from the base model\n",
    "            self.conv1 = base_model.conv1\n",
    "            self.bn1 = base_model.bn1\n",
    "            self.relu = base_model.relu\n",
    "            self.maxpool = base_model.maxpool\n",
    "            self.layer1 = base_model.layer1\n",
    "            self.layer2 = base_model.layer2\n",
    "            self.layer3 = base_model.layer3\n",
    "            self.layer4 = base_model.layer4\n",
    "            # Encoder output channels\n",
    "            encoder_output_dim = 512\n",
    "\n",
    "            # Define decoder layers\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(encoder_output_dim, 256, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        elif model_name == 'efficientnet':\n",
    "            # Directly assign the encoder layers from the base model\n",
    "            self.features = base_model.features\n",
    "            # Encoder output channels (depends on the EfficientNet version)\n",
    "            encoder_output_dim = 1280  # For EfficientNet-B0\n",
    "\n",
    "            # Define decoder layers\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(encoder_output_dim, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        elif model_name == 'vit':\n",
    "            # Directly assign the encoder layers from the base model\n",
    "            self.conv_proj = base_model.conv_proj  # Patch embedding\n",
    "            self.encoder_layers = base_model.encoder  # Transformer encoder\n",
    "            self.class_token = base_model.class_token  # [CLS] token\n",
    "            self.encoder_norm = base_model.encoder.ln  # Layer norm after encoder\n",
    "            # ViT parameters\n",
    "            self.hidden_dim = base_model.hidden_dim  # Hidden dimension (e.g., 768)\n",
    "            self.image_size = base_model.image_size  # Input image size (e.g., 224)\n",
    "            self.patch_size = base_model.patch_size  # Patch size (e.g., 16)\n",
    "            self.num_patches = (self.image_size // self.patch_size) ** 2  # Number of patches\n",
    "\n",
    "            # Decoder: map encoded tokens back to image patches\n",
    "            self.decoder_linear = nn.Linear(self.hidden_dim, self.patch_size * self.patch_size * 3)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "    def encoder(self, x):\n",
    "        if self.model_name == 'resnet':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            return x\n",
    "\n",
    "        elif self.model_name == 'efficientnet':\n",
    "            x = self.features(x)\n",
    "            return x\n",
    "\n",
    "        elif self.model_name == 'vit':\n",
    "            # Patch embedding\n",
    "            x = self.conv_proj(x)  # [batch_size, hidden_dim, H', W']\n",
    "            x = x.flatten(2).transpose(1, 2)  # [batch_size, num_patches, hidden_dim]\n",
    "            # Add class token\n",
    "            batch_size = x.size(0)\n",
    "            cls_tokens = self.class_token.expand(batch_size, -1, -1)  # [batch_size, 1, hidden_dim]\n",
    "            x = torch.cat((cls_tokens, x), dim=1)  # [batch_size, num_patches+1, hidden_dim]\n",
    "            # Encoder\n",
    "            x = self.encoder_layers(x)\n",
    "            x = self.encoder_norm(x)\n",
    "            return x  # [batch_size, num_patches+1, hidden_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        if self.model_name in ['resnet', 'efficientnet']:\n",
    "            # Decoder expects input of shape [batch_size, encoder_output_dim, H, W]\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "\n",
    "        elif self.model_name == 'vit':\n",
    "            # Remove class token\n",
    "            x = x[:, 1:, :]  # [batch_size, num_patches, hidden_dim]\n",
    "            # Decoder: map tokens back to patches\n",
    "            x = self.decoder_linear(x)  # [batch_size, num_patches, patch_size*patch_size*3]\n",
    "            # Reshape to image patches\n",
    "            batch_size = x.size(0)\n",
    "            x = x.view(batch_size, self.num_patches, 3, self.patch_size, self.patch_size)\n",
    "            # Rearrange patches into images\n",
    "            x = x.permute(0, 2, 1, 3, 4)  # [batch_size, 3, num_patches, patch_size, patch_size]\n",
    "            grid_size = int(math.sqrt(self.num_patches))\n",
    "            x = x.reshape(batch_size, 3, grid_size, grid_size, self.patch_size, self.patch_size)\n",
    "            x = x.permute(0, 1, 2, 4, 3, 5)\n",
    "            x = x.reshape(batch_size, 3, self.image_size, self.image_size)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for Autoencoder\n",
    "def train_autoencoder(autoencoder, unlabelled_loader, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    autoencoder = autoencoder.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    autoencoder.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, _ in unlabelled_loader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(unlabelled_loader)\n",
    "        print(f\"Autoencoder Training Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function with Autoencoder and Transfer Learning\n",
    "def train_model_with_autoencoder_and_grid_search(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    unlabelled_loader,\n",
    "    num_classes,\n",
    "    num_epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    log_filename='training_log.csv',\n",
    "    model_name='resnet',\n",
    "    batch_size=64\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure 'logs' directory exists for saving the best model\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "\n",
    "    ### Phase 1: Initial Training on Labeled Data ###\n",
    "\n",
    "    print(\"Starting Phase 1: Initial Training on Labeled Data...\")\n",
    "    # Copy the model for initial training\n",
    "    initial_model = deepcopy(model).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(initial_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    initial_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = initial_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Initial Training Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    ### Phase 2: Train Autoencoder on Unlabeled Data ###\n",
    "\n",
    "    print(\"Starting Phase 2: Training Autoencoder on Unlabeled Data...\")\n",
    "    # Initialize autoencoder with the initial model's encoder\n",
    "    autoencoder = Autoencoder(initial_model, model_name=model_name).to(device)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder = train_autoencoder(autoencoder, unlabelled_loader, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "\n",
    "    ### Phase 3: Use Encoder Weights from Autoencoder ###\n",
    "\n",
    "    print(\"Starting Phase 3: Updating Model with Autoencoder Encoder...\")\n",
    "    # Update initial_model's encoder weights with autoencoder's encoder weights\n",
    "\n",
    "    # Get the encoder state_dict from the autoencoder\n",
    "    encoder_state_dict = {}\n",
    "    for name, param in autoencoder.state_dict().items():\n",
    "        if name in initial_model.state_dict() and 'decoder' not in name:\n",
    "            encoder_state_dict[name] = param\n",
    "\n",
    "    # Update initial_model's state_dict with the encoder weights\n",
    "    initial_model_state_dict = initial_model.state_dict()\n",
    "    initial_model_state_dict.update(encoder_state_dict)\n",
    "\n",
    "    # Load the updated state_dict into initial_model\n",
    "    initial_model.load_state_dict(initial_model_state_dict)\n",
    "\n",
    "    ### Phase 4: Transfer Learning with Grid Search ###\n",
    "\n",
    "    print(\"Starting Phase 4: Transfer Learning with Grid Search...\")\n",
    "    # Define different layer unfreezing configurations based on model type\n",
    "    if model_name == 'resnet':\n",
    "        unfreeze_configs = {\n",
    "            'fc': ['fc'],\n",
    "            'fc+layer4': ['layer4', 'fc'],\n",
    "            'fc+layer3+layer4': ['layer3', 'layer4', 'fc'],\n",
    "        }\n",
    "    elif model_name == 'efficientnet':\n",
    "        unfreeze_configs = {\n",
    "            'fc': ['classifier.1'],\n",
    "            'fc+features8': ['features.8', 'classifier.1'],\n",
    "            'fc+features7+features8': ['features.7', 'features.8', 'classifier.1'],\n",
    "        }\n",
    "    elif model_name == 'vit':\n",
    "        unfreeze_configs = {\n",
    "            'fc': ['heads.head'],\n",
    "            'fc+encoder11': ['encoder.layers.encoder_layer_11', 'heads.head'],\n",
    "            'fc+encoder10+encoder11': ['encoder.layers.encoder_layer_10', 'encoder.layers.encoder_layer_11', 'heads.head'],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_config = ''\n",
    "    best_model_state = None\n",
    "\n",
    "    # Open log file for recording training progress\n",
    "    with open(os.path.join('logs', log_filename), mode='w', newline='') as log_file:\n",
    "        log_writer = csv.writer(log_file)\n",
    "        log_writer.writerow(['Configuration', 'Epoch', 'Training Loss', 'Validation Macro F1'])\n",
    "\n",
    "        for config_name, layers_to_unfreeze in unfreeze_configs.items():\n",
    "            print(f\"\\nStarting Transfer Learning Phase with configuration: {config_name}\")\n",
    "\n",
    "            # Create a new model by copying the initial model\n",
    "            finetune_model = deepcopy(initial_model)\n",
    "\n",
    "            # Modify the final layer based on the model's layer names\n",
    "            if model_name == 'resnet':\n",
    "                feature_dim = finetune_model.fc.in_features\n",
    "                finetune_model.fc = nn.Linear(feature_dim, num_classes)\n",
    "            elif model_name == 'efficientnet':\n",
    "                feature_dim = finetune_model.classifier[1].in_features\n",
    "                finetune_model.classifier[1] = nn.Linear(feature_dim, num_classes)\n",
    "            elif model_name == 'vit':\n",
    "                feature_dim = finetune_model.heads.head.in_features\n",
    "                finetune_model.heads.head = nn.Linear(feature_dim, num_classes)\n",
    "            else:\n",
    "                raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "            # Freeze all layers first\n",
    "            for param in finetune_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Unfreeze specified layers\n",
    "            for name, param in finetune_model.named_parameters():\n",
    "                for layer_name in layers_to_unfreeze:\n",
    "                    if name.startswith(layer_name):\n",
    "                        param.requires_grad = True\n",
    "                        print(f\"Unfreezing layer: {name}\")\n",
    "\n",
    "            finetune_model = finetune_model.to(device)\n",
    "\n",
    "            # Define loss function and optimizer for fine-tuning\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=learning_rate)\n",
    "\n",
    "            # Fine-tuning training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                finetune_model.train()\n",
    "                running_loss = 0.0\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = finetune_model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "                # Validation\n",
    "                finetune_model.eval()\n",
    "                all_labels = []\n",
    "                all_preds = []\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in valid_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = finetune_model(inputs)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Calculate Macro F1 score\n",
    "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "                # Log results\n",
    "                log_writer.writerow([config_name, epoch + 1, avg_loss, f1])\n",
    "                print(f\"Config: {config_name}, Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Validation Macro F1: {f1:.4f}\")\n",
    "\n",
    "            # Update best model if current config is better\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_config = config_name\n",
    "                best_model_state = deepcopy(finetune_model.state_dict())\n",
    "\n",
    "    print(f\"\\nBest Configuration: {best_config} with Macro F1 Score: {best_f1}\")\n",
    "    # Save the best model\n",
    "    torch.save(best_model_state, os.path.join('logs', f\"best_model_{model_name}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\weita/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: conv1.weight, Shape: torch.Size([64, 3, 7, 7])\n",
      "Name: bn1.weight, Shape: torch.Size([64])\n",
      "Name: bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer2.0.conv1.weight, Shape: torch.Size([128, 64, 3, 3])\n",
      "Name: layer2.0.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.0.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.0.weight, Shape: torch.Size([128, 64, 1, 1])\n",
      "Name: layer2.0.downsample.1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv1.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer3.0.conv1.weight, Shape: torch.Size([256, 128, 3, 3])\n",
      "Name: layer3.0.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.0.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.0.weight, Shape: torch.Size([256, 128, 1, 1])\n",
      "Name: layer3.0.downsample.1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv1.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer4.0.conv1.weight, Shape: torch.Size([512, 256, 3, 3])\n",
      "Name: layer4.0.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.0.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn2.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.0.weight, Shape: torch.Size([512, 256, 1, 1])\n",
      "Name: layer4.0.downsample.1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv1.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn2.bias, Shape: torch.Size([512])\n",
      "Name: fc.weight, Shape: torch.Size([1000, 512])\n",
      "Name: fc.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# We will keep this for later\n",
    "model0 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "\n",
    "for name, param in model0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Phase 1: Initial Training on Labeled Data...\n",
      "Initial Training Epoch [1/10], Loss: 3.5700\n",
      "Initial Training Epoch [2/10], Loss: 1.6348\n",
      "Initial Training Epoch [3/10], Loss: 1.0263\n",
      "Initial Training Epoch [4/10], Loss: 0.9492\n",
      "Initial Training Epoch [5/10], Loss: 1.2221\n",
      "Initial Training Epoch [6/10], Loss: 0.8824\n",
      "Initial Training Epoch [7/10], Loss: 0.6621\n",
      "Initial Training Epoch [8/10], Loss: 0.7683\n",
      "Initial Training Epoch [9/10], Loss: 0.9326\n",
      "Initial Training Epoch [10/10], Loss: 0.7037\n",
      "Starting Phase 2: Training Autoencoder on Unlabeled Data...\n",
      "Autoencoder Training Epoch [1/10], Loss: 0.6369\n",
      "Autoencoder Training Epoch [2/10], Loss: 0.5238\n",
      "Autoencoder Training Epoch [3/10], Loss: 0.4404\n",
      "Autoencoder Training Epoch [4/10], Loss: 0.3997\n",
      "Autoencoder Training Epoch [5/10], Loss: 0.3601\n",
      "Autoencoder Training Epoch [6/10], Loss: 0.3378\n",
      "Autoencoder Training Epoch [7/10], Loss: 0.3268\n",
      "Autoencoder Training Epoch [8/10], Loss: 0.3245\n",
      "Autoencoder Training Epoch [9/10], Loss: 0.3095\n",
      "Autoencoder Training Epoch [10/10], Loss: 0.3050\n",
      "Starting Phase 3: Updating Model with Autoencoder Encoder...\n",
      "Starting Phase 4: Transfer Learning with Grid Search...\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc\n",
      "Unfreezing layer: fc.weight\n",
      "Unfreezing layer: fc.bias\n",
      "Config: fc, Epoch [1/10], Loss: 2.3431, Validation Macro F1: 0.1359\n",
      "Config: fc, Epoch [2/10], Loss: 2.2065, Validation Macro F1: 0.1324\n",
      "Config: fc, Epoch [3/10], Loss: 2.1881, Validation Macro F1: 0.1567\n",
      "Config: fc, Epoch [4/10], Loss: 2.1542, Validation Macro F1: 0.1584\n",
      "Config: fc, Epoch [5/10], Loss: 2.1356, Validation Macro F1: 0.1698\n",
      "Config: fc, Epoch [6/10], Loss: 2.0923, Validation Macro F1: 0.1680\n",
      "Config: fc, Epoch [7/10], Loss: 2.0878, Validation Macro F1: 0.1523\n",
      "Config: fc, Epoch [8/10], Loss: 2.2124, Validation Macro F1: 0.1695\n",
      "Config: fc, Epoch [9/10], Loss: 2.1101, Validation Macro F1: 0.1534\n",
      "Config: fc, Epoch [10/10], Loss: 2.0711, Validation Macro F1: 0.1795\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc+layer4\n",
      "Unfreezing layer: layer4.0.conv1.weight\n",
      "Unfreezing layer: layer4.0.bn1.weight\n",
      "Unfreezing layer: layer4.0.bn1.bias\n",
      "Unfreezing layer: layer4.0.conv2.weight\n",
      "Unfreezing layer: layer4.0.bn2.weight\n",
      "Unfreezing layer: layer4.0.bn2.bias\n",
      "Unfreezing layer: layer4.0.downsample.0.weight\n",
      "Unfreezing layer: layer4.0.downsample.1.weight\n",
      "Unfreezing layer: layer4.0.downsample.1.bias\n",
      "Unfreezing layer: layer4.1.conv1.weight\n",
      "Unfreezing layer: layer4.1.bn1.weight\n",
      "Unfreezing layer: layer4.1.bn1.bias\n",
      "Unfreezing layer: layer4.1.conv2.weight\n",
      "Unfreezing layer: layer4.1.bn2.weight\n",
      "Unfreezing layer: layer4.1.bn2.bias\n",
      "Unfreezing layer: fc.weight\n",
      "Unfreezing layer: fc.bias\n",
      "Config: fc+layer4, Epoch [1/10], Loss: 2.2815, Validation Macro F1: 0.1721\n",
      "Config: fc+layer4, Epoch [2/10], Loss: 2.0149, Validation Macro F1: 0.2857\n",
      "Config: fc+layer4, Epoch [3/10], Loss: 1.8918, Validation Macro F1: 0.2535\n",
      "Config: fc+layer4, Epoch [4/10], Loss: 1.7892, Validation Macro F1: 0.3195\n",
      "Config: fc+layer4, Epoch [5/10], Loss: 1.5268, Validation Macro F1: 0.3550\n",
      "Config: fc+layer4, Epoch [6/10], Loss: 1.3914, Validation Macro F1: 0.3655\n",
      "Config: fc+layer4, Epoch [7/10], Loss: 1.4002, Validation Macro F1: 0.3430\n",
      "Config: fc+layer4, Epoch [8/10], Loss: 1.2923, Validation Macro F1: 0.3451\n",
      "Config: fc+layer4, Epoch [9/10], Loss: 1.1934, Validation Macro F1: 0.3462\n",
      "Config: fc+layer4, Epoch [10/10], Loss: 1.1108, Validation Macro F1: 0.3713\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc+layer3+layer4\n",
      "Unfreezing layer: layer3.0.conv1.weight\n",
      "Unfreezing layer: layer3.0.bn1.weight\n",
      "Unfreezing layer: layer3.0.bn1.bias\n",
      "Unfreezing layer: layer3.0.conv2.weight\n",
      "Unfreezing layer: layer3.0.bn2.weight\n",
      "Unfreezing layer: layer3.0.bn2.bias\n",
      "Unfreezing layer: layer3.0.downsample.0.weight\n",
      "Unfreezing layer: layer3.0.downsample.1.weight\n",
      "Unfreezing layer: layer3.0.downsample.1.bias\n",
      "Unfreezing layer: layer3.1.conv1.weight\n",
      "Unfreezing layer: layer3.1.bn1.weight\n",
      "Unfreezing layer: layer3.1.bn1.bias\n",
      "Unfreezing layer: layer3.1.conv2.weight\n",
      "Unfreezing layer: layer3.1.bn2.weight\n",
      "Unfreezing layer: layer3.1.bn2.bias\n",
      "Unfreezing layer: layer4.0.conv1.weight\n",
      "Unfreezing layer: layer4.0.bn1.weight\n",
      "Unfreezing layer: layer4.0.bn1.bias\n",
      "Unfreezing layer: layer4.0.conv2.weight\n",
      "Unfreezing layer: layer4.0.bn2.weight\n",
      "Unfreezing layer: layer4.0.bn2.bias\n",
      "Unfreezing layer: layer4.0.downsample.0.weight\n",
      "Unfreezing layer: layer4.0.downsample.1.weight\n",
      "Unfreezing layer: layer4.0.downsample.1.bias\n",
      "Unfreezing layer: layer4.1.conv1.weight\n",
      "Unfreezing layer: layer4.1.bn1.weight\n",
      "Unfreezing layer: layer4.1.bn1.bias\n",
      "Unfreezing layer: layer4.1.conv2.weight\n",
      "Unfreezing layer: layer4.1.bn2.weight\n",
      "Unfreezing layer: layer4.1.bn2.bias\n",
      "Unfreezing layer: fc.weight\n",
      "Unfreezing layer: fc.bias\n",
      "Config: fc+layer3+layer4, Epoch [1/10], Loss: 2.2671, Validation Macro F1: 0.2386\n",
      "Config: fc+layer3+layer4, Epoch [2/10], Loss: 1.8522, Validation Macro F1: 0.2759\n",
      "Config: fc+layer3+layer4, Epoch [3/10], Loss: 1.6015, Validation Macro F1: 0.3141\n",
      "Config: fc+layer3+layer4, Epoch [4/10], Loss: 1.5463, Validation Macro F1: 0.3688\n",
      "Config: fc+layer3+layer4, Epoch [5/10], Loss: 1.4043, Validation Macro F1: 0.3888\n",
      "Config: fc+layer3+layer4, Epoch [6/10], Loss: 1.1885, Validation Macro F1: 0.4066\n",
      "Config: fc+layer3+layer4, Epoch [7/10], Loss: 1.0799, Validation Macro F1: 0.3986\n",
      "Config: fc+layer3+layer4, Epoch [8/10], Loss: 0.9382, Validation Macro F1: 0.4464\n",
      "Config: fc+layer3+layer4, Epoch [9/10], Loss: 0.8875, Validation Macro F1: 0.4537\n",
      "Config: fc+layer3+layer4, Epoch [10/10], Loss: 0.8726, Validation Macro F1: 0.4525\n",
      "\n",
      "Best Configuration: fc+layer3+layer4 with Macro F1 Score: 0.4525191747110754\n"
     ]
    }
   ],
   "source": [
    "# Example usage with ResNet18\n",
    "model_resnet18 = deepcopy(model0)  # assuming model0 is a pretrained resnet18\n",
    "model_resnet18 = model_resnet18.to(device)\n",
    "train_model_with_autoencoder_and_grid_search(\n",
    "    model=model_resnet18,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    unlabelled_loader=unlabelled_loader,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    log_filename='resnet18_training_log.csv',\n",
    "    model_name='resnet',\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\weita\\AppData\\Local\\Temp\\ipykernel_34524\\2008848801.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_resnet.load_state_dict(torch.load(f'logs/best_model_resnet.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ResNet18 model\n",
    "best_model_resnet = models.resnet18(pretrained=False)\n",
    "best_model_resnet.fc = nn.Linear(best_model_resnet.fc.in_features, num_classes)\n",
    "best_model_resnet = best_model_resnet.to(device)\n",
    "\n",
    "# Load the best model weights\n",
    "best_model_resnet.load_state_dict(torch.load(f'logs/best_model_resnet.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model_resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 46.4125%\n"
     ]
    }
   ],
   "source": [
    "# Call the test functions\n",
    "test_model(best_model_resnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.4616945313204985\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.62      0.62      0.62       800\n",
      "     Class 1       0.39      0.45      0.42       800\n",
      "     Class 2       0.77      0.41      0.54       800\n",
      "     Class 3       0.35      0.32      0.34       800\n",
      "     Class 4       0.36      0.66      0.46       800\n",
      "     Class 5       0.28      0.33      0.30       800\n",
      "     Class 6       0.56      0.38      0.45       800\n",
      "     Class 7       0.62      0.23      0.33       800\n",
      "     Class 8       0.63      0.64      0.64       800\n",
      "     Class 9       0.45      0.60      0.52       800\n",
      "\n",
      "    accuracy                           0.46      8000\n",
      "   macro avg       0.50      0.46      0.46      8000\n",
      "weighted avg       0.50      0.46      0.46      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_f1(best_model_resnet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: features.0.0.weight, Shape: torch.Size([32, 3, 3, 3])\n",
      "Name: features.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.0.weight, Shape: torch.Size([32, 1, 3, 3])\n",
      "Name: features.1.0.block.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.1.fc1.weight, Shape: torch.Size([8, 32, 1, 1])\n",
      "Name: features.1.0.block.1.fc1.bias, Shape: torch.Size([8])\n",
      "Name: features.1.0.block.1.fc2.weight, Shape: torch.Size([32, 8, 1, 1])\n",
      "Name: features.1.0.block.1.fc2.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.2.0.weight, Shape: torch.Size([16, 32, 1, 1])\n",
      "Name: features.1.0.block.2.1.weight, Shape: torch.Size([16])\n",
      "Name: features.1.0.block.2.1.bias, Shape: torch.Size([16])\n",
      "Name: features.2.0.block.0.0.weight, Shape: torch.Size([96, 16, 1, 1])\n",
      "Name: features.2.0.block.0.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.0.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.0.weight, Shape: torch.Size([96, 1, 3, 3])\n",
      "Name: features.2.0.block.1.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.2.fc1.weight, Shape: torch.Size([4, 96, 1, 1])\n",
      "Name: features.2.0.block.2.fc1.bias, Shape: torch.Size([4])\n",
      "Name: features.2.0.block.2.fc2.weight, Shape: torch.Size([96, 4, 1, 1])\n",
      "Name: features.2.0.block.2.fc2.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.3.0.weight, Shape: torch.Size([24, 96, 1, 1])\n",
      "Name: features.2.0.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.0.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.2.1.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.0.weight, Shape: torch.Size([144, 1, 3, 3])\n",
      "Name: features.2.1.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.2.1.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.2.1.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.2.1.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.3.0.weight, Shape: torch.Size([24, 144, 1, 1])\n",
      "Name: features.2.1.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.3.0.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.3.0.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.0.weight, Shape: torch.Size([144, 1, 5, 5])\n",
      "Name: features.3.0.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.3.0.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.3.0.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.3.0.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.3.0.weight, Shape: torch.Size([40, 144, 1, 1])\n",
      "Name: features.3.0.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.0.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.3.1.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.0.weight, Shape: torch.Size([240, 1, 5, 5])\n",
      "Name: features.3.1.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.3.1.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.3.1.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.3.1.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.3.0.weight, Shape: torch.Size([40, 240, 1, 1])\n",
      "Name: features.3.1.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.4.0.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.4.0.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.0.weight, Shape: torch.Size([240, 1, 3, 3])\n",
      "Name: features.4.0.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.4.0.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.4.0.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.4.0.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.3.0.weight, Shape: torch.Size([80, 240, 1, 1])\n",
      "Name: features.4.0.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.0.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.1.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.1.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.1.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.1.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.1.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.1.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.2.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.2.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.2.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.2.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.2.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.2.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.5.0.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.5.0.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.0.weight, Shape: torch.Size([480, 1, 5, 5])\n",
      "Name: features.5.0.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.5.0.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.5.0.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.5.0.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.3.0.weight, Shape: torch.Size([112, 480, 1, 1])\n",
      "Name: features.5.0.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.0.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.1.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.1.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.1.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.1.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.1.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.1.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.2.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.2.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.2.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.2.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.2.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.2.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.6.0.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.6.0.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.6.0.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.6.0.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.6.0.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.6.0.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.3.0.weight, Shape: torch.Size([192, 672, 1, 1])\n",
      "Name: features.6.0.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.0.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.1.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.1.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.1.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.1.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.1.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.1.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.2.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.2.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.2.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.2.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.2.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.2.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.3.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.3.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.3.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.3.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.3.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.3.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.7.0.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.7.0.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.0.weight, Shape: torch.Size([1152, 1, 3, 3])\n",
      "Name: features.7.0.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.7.0.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.7.0.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.7.0.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.3.0.weight, Shape: torch.Size([320, 1152, 1, 1])\n",
      "Name: features.7.0.block.3.1.weight, Shape: torch.Size([320])\n",
      "Name: features.7.0.block.3.1.bias, Shape: torch.Size([320])\n",
      "Name: features.8.0.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "Name: features.8.1.weight, Shape: torch.Size([1280])\n",
      "Name: features.8.1.bias, Shape: torch.Size([1280])\n",
      "Name: classifier.1.weight, Shape: torch.Size([1000, 1280])\n",
      "Name: classifier.1.bias, Shape: torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\weita/.cache\\torch\\hub\\pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained EfficientNet-B0 model from torchvision hub\n",
    "model1 = torch.hub.load('pytorch/vision', 'efficientnet_b0', weights=\"EfficientNet_B0_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "for name, param in model1.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Phase 1: Initial Training on Labeled Data...\n",
      "Initial Training Epoch [1/10], Loss: 3.9637\n",
      "Initial Training Epoch [2/10], Loss: 1.2338\n",
      "Initial Training Epoch [3/10], Loss: 0.8843\n",
      "Initial Training Epoch [4/10], Loss: 0.6483\n",
      "Initial Training Epoch [5/10], Loss: 0.7510\n",
      "Initial Training Epoch [6/10], Loss: 0.5487\n",
      "Initial Training Epoch [7/10], Loss: 0.4331\n",
      "Initial Training Epoch [8/10], Loss: 0.3330\n",
      "Initial Training Epoch [9/10], Loss: 0.2836\n",
      "Initial Training Epoch [10/10], Loss: 0.3497\n",
      "Starting Phase 2: Training Autoencoder on Unlabeled Data...\n",
      "Autoencoder Training Epoch [1/10], Loss: 0.6489\n",
      "Autoencoder Training Epoch [2/10], Loss: 0.5431\n",
      "Autoencoder Training Epoch [3/10], Loss: 0.4582\n",
      "Autoencoder Training Epoch [4/10], Loss: 0.4067\n",
      "Autoencoder Training Epoch [5/10], Loss: 0.3699\n",
      "Autoencoder Training Epoch [6/10], Loss: 0.3471\n",
      "Autoencoder Training Epoch [7/10], Loss: 0.3314\n",
      "Autoencoder Training Epoch [8/10], Loss: 0.3236\n",
      "Autoencoder Training Epoch [9/10], Loss: 0.3125\n",
      "Autoencoder Training Epoch [10/10], Loss: 0.3190\n",
      "Starting Phase 3: Updating Model with Autoencoder Encoder...\n",
      "Starting Phase 4: Transfer Learning with Grid Search...\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc\n",
      "Unfreezing layer: classifier.1.weight\n",
      "Unfreezing layer: classifier.1.bias\n",
      "Config: fc, Epoch [1/10], Loss: 1.8792, Validation Macro F1: 0.4293\n",
      "Config: fc, Epoch [2/10], Loss: 1.4346, Validation Macro F1: 0.4922\n",
      "Config: fc, Epoch [3/10], Loss: 1.2578, Validation Macro F1: 0.5224\n",
      "Config: fc, Epoch [4/10], Loss: 1.0986, Validation Macro F1: 0.5419\n",
      "Config: fc, Epoch [5/10], Loss: 0.9806, Validation Macro F1: 0.5445\n",
      "Config: fc, Epoch [6/10], Loss: 0.9401, Validation Macro F1: 0.5632\n",
      "Config: fc, Epoch [7/10], Loss: 0.9135, Validation Macro F1: 0.5548\n",
      "Config: fc, Epoch [8/10], Loss: 0.8395, Validation Macro F1: 0.5772\n",
      "Config: fc, Epoch [9/10], Loss: 0.8106, Validation Macro F1: 0.5724\n",
      "Config: fc, Epoch [10/10], Loss: 0.7819, Validation Macro F1: 0.5761\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc+features8\n",
      "Unfreezing layer: features.8.0.weight\n",
      "Unfreezing layer: features.8.1.weight\n",
      "Unfreezing layer: features.8.1.bias\n",
      "Unfreezing layer: classifier.1.weight\n",
      "Unfreezing layer: classifier.1.bias\n",
      "Config: fc+features8, Epoch [1/10], Loss: 1.7392, Validation Macro F1: 0.5153\n",
      "Config: fc+features8, Epoch [2/10], Loss: 0.9924, Validation Macro F1: 0.5564\n",
      "Config: fc+features8, Epoch [3/10], Loss: 0.7882, Validation Macro F1: 0.5854\n",
      "Config: fc+features8, Epoch [4/10], Loss: 0.6898, Validation Macro F1: 0.5915\n",
      "Config: fc+features8, Epoch [5/10], Loss: 0.6363, Validation Macro F1: 0.6096\n",
      "Config: fc+features8, Epoch [6/10], Loss: 0.5858, Validation Macro F1: 0.6088\n",
      "Config: fc+features8, Epoch [7/10], Loss: 0.5714, Validation Macro F1: 0.6249\n",
      "Config: fc+features8, Epoch [8/10], Loss: 0.5152, Validation Macro F1: 0.6168\n",
      "Config: fc+features8, Epoch [9/10], Loss: 0.4794, Validation Macro F1: 0.6238\n",
      "Config: fc+features8, Epoch [10/10], Loss: 0.4176, Validation Macro F1: 0.6272\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc+features7+features8\n",
      "Unfreezing layer: features.7.0.block.0.0.weight\n",
      "Unfreezing layer: features.7.0.block.0.1.weight\n",
      "Unfreezing layer: features.7.0.block.0.1.bias\n",
      "Unfreezing layer: features.7.0.block.1.0.weight\n",
      "Unfreezing layer: features.7.0.block.1.1.weight\n",
      "Unfreezing layer: features.7.0.block.1.1.bias\n",
      "Unfreezing layer: features.7.0.block.2.fc1.weight\n",
      "Unfreezing layer: features.7.0.block.2.fc1.bias\n",
      "Unfreezing layer: features.7.0.block.2.fc2.weight\n",
      "Unfreezing layer: features.7.0.block.2.fc2.bias\n",
      "Unfreezing layer: features.7.0.block.3.0.weight\n",
      "Unfreezing layer: features.7.0.block.3.1.weight\n",
      "Unfreezing layer: features.7.0.block.3.1.bias\n",
      "Unfreezing layer: features.8.0.weight\n",
      "Unfreezing layer: features.8.1.weight\n",
      "Unfreezing layer: features.8.1.bias\n",
      "Unfreezing layer: classifier.1.weight\n",
      "Unfreezing layer: classifier.1.bias\n",
      "Config: fc+features7+features8, Epoch [1/10], Loss: 1.6579, Validation Macro F1: 0.5583\n",
      "Config: fc+features7+features8, Epoch [2/10], Loss: 0.8320, Validation Macro F1: 0.6245\n",
      "Config: fc+features7+features8, Epoch [3/10], Loss: 0.5423, Validation Macro F1: 0.6231\n",
      "Config: fc+features7+features8, Epoch [4/10], Loss: 0.4782, Validation Macro F1: 0.6203\n",
      "Config: fc+features7+features8, Epoch [5/10], Loss: 0.3470, Validation Macro F1: 0.6328\n",
      "Config: fc+features7+features8, Epoch [6/10], Loss: 0.3384, Validation Macro F1: 0.6326\n",
      "Config: fc+features7+features8, Epoch [7/10], Loss: 0.3448, Validation Macro F1: 0.6469\n",
      "Config: fc+features7+features8, Epoch [8/10], Loss: 0.3587, Validation Macro F1: 0.6205\n",
      "Config: fc+features7+features8, Epoch [9/10], Loss: 0.2730, Validation Macro F1: 0.6467\n",
      "Config: fc+features7+features8, Epoch [10/10], Loss: 0.2477, Validation Macro F1: 0.6209\n",
      "\n",
      "Best Configuration: fc+features8 with Macro F1 Score: 0.6271833088481502\n"
     ]
    }
   ],
   "source": [
    "# Example usage with EfficientNetB0\n",
    "model_efficientnetb0 = deepcopy(model1)  # assuming model1 is a pretrained efficientnetb0\n",
    "model_efficientnetb0 = model_efficientnetb0.to(device)\n",
    "# Call the training function\n",
    "train_model_with_autoencoder_and_grid_search(\n",
    "    model=model_efficientnetb0,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    unlabelled_loader=unlabelled_loader,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    log_filename='efficientnetb0_training_log.csv',\n",
    "    model_name='efficientnet',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\weita\\AppData\\Local\\Temp\\ipykernel_34524\\999219682.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_efficientnet.load_state_dict(torch.load(f'logs/best_model_efficientnet.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the EfficientNetB0 model\n",
    "best_model_efficientnet = models.efficientnet_b0(pretrained=False)\n",
    "best_model_efficientnet.classifier[1] = nn.Linear(best_model_efficientnet.classifier[1].in_features, num_classes)\n",
    "best_model_efficientnet = best_model_efficientnet.to(device)\n",
    "\n",
    "# Load the best model weights\n",
    "best_config = 'fc+features8'  # Replace with your best configuration name\n",
    "best_model_efficientnet.load_state_dict(torch.load(f'logs/best_model_efficientnet.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model_efficientnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.4625%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(best_model_efficientnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.6246351501282467\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.65      0.83      0.73       800\n",
      "     Class 1       0.58      0.61      0.60       800\n",
      "     Class 2       0.82      0.82      0.82       800\n",
      "     Class 3       0.53      0.53      0.53       800\n",
      "     Class 4       0.61      0.53      0.57       800\n",
      "     Class 5       0.45      0.51      0.48       800\n",
      "     Class 6       0.68      0.60      0.64       800\n",
      "     Class 7       0.54      0.56      0.55       800\n",
      "     Class 8       0.77      0.60      0.67       800\n",
      "     Class 9       0.68      0.66      0.67       800\n",
      "\n",
      "    accuracy                           0.62      8000\n",
      "   macro avg       0.63      0.62      0.62      8000\n",
      "weighted avg       0.63      0.62      0.62      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(best_model_efficientnet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image size to 224x224 to match the input size of ViT\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_unlabelled = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation sets without redownloading data\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=False)\n",
    "\n",
    "# Use 10% of the data for training (simulating a low data scenario)\n",
    "num_train = int(len(trainval_set) * 0.1)\n",
    "\n",
    "# Split data into train/validation sets with a fixed random seed\n",
    "torch.manual_seed(0)  # Ensure reproducibility\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set) - num_train])\n",
    "\n",
    "# Load test set without redownloading data\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=False)\n",
    "\n",
    "# Determine the size of the subset (1/1000 of the full dataset)\n",
    "subset_size = len(unlabelled_set) // 1000  # This will be 100 samples\n",
    "\n",
    "# Randomly select indices for the subset\n",
    "random_indices = random.sample(range(len(unlabelled_set)), subset_size)\n",
    "\n",
    "# Create a subset of the unlabelled dataset\n",
    "unlabelled_subset = Subset(unlabelled_set, random_indices)\n",
    "\n",
    "# Now, create the DataLoader using the subset\n",
    "unlabelled_loader = DataLoader(unlabelled_subset, shuffle=True, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for train, validation, and test sets\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: class_token, Shape: torch.Size([1, 1, 768])\n",
      "Name: conv_proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Name: conv_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.pos_embedding, Shape: torch.Size([1, 197, 768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.ln.weight, Shape: torch.Size([768])\n",
      "Name: encoder.ln.bias, Shape: torch.Size([768])\n",
      "Name: heads.head.weight, Shape: torch.Size([1000, 768])\n",
      "Name: heads.head.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Vision Transformer (ViT) model from torchvision models\n",
    "model2 = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Print the model structure to verify the changes\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting Phase 1: Initial Training on Labeled Data...\n",
      "Initial Training Epoch [1/10], Loss: 3.0538\n",
      "Initial Training Epoch [2/10], Loss: 2.3295\n",
      "Initial Training Epoch [3/10], Loss: 2.2346\n",
      "Initial Training Epoch [4/10], Loss: 2.1957\n",
      "Initial Training Epoch [5/10], Loss: 2.2515\n",
      "Initial Training Epoch [6/10], Loss: 2.0731\n",
      "Initial Training Epoch [7/10], Loss: 2.1408\n",
      "Initial Training Epoch [8/10], Loss: 2.1249\n",
      "Initial Training Epoch [9/10], Loss: 2.0998\n",
      "Initial Training Epoch [10/10], Loss: 2.0570\n",
      "Starting Phase 2: Training Autoencoder on Unlabeled Data...\n",
      "Autoencoder Training Epoch [1/10], Loss: 0.2217\n",
      "Autoencoder Training Epoch [2/10], Loss: 0.0796\n",
      "Autoencoder Training Epoch [3/10], Loss: 0.0633\n",
      "Autoencoder Training Epoch [4/10], Loss: 0.0515\n",
      "Autoencoder Training Epoch [5/10], Loss: 0.0428\n",
      "Autoencoder Training Epoch [6/10], Loss: 0.0374\n",
      "Autoencoder Training Epoch [7/10], Loss: 0.0319\n",
      "Autoencoder Training Epoch [8/10], Loss: 0.0289\n",
      "Autoencoder Training Epoch [9/10], Loss: 0.0271\n",
      "Autoencoder Training Epoch [10/10], Loss: 0.0251\n",
      "Starting Phase 3: Updating Model with Autoencoder Encoder...\n",
      "Starting Phase 4: Transfer Learning with Grid Search...\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc\n",
      "Unfreezing layer: heads.head.weight\n",
      "Unfreezing layer: heads.head.bias\n",
      "Config: fc, Epoch [1/10], Loss: 2.3466, Validation Macro F1: 0.0487\n",
      "Config: fc, Epoch [2/10], Loss: 2.2720, Validation Macro F1: 0.0554\n",
      "Config: fc, Epoch [3/10], Loss: 2.2156, Validation Macro F1: 0.1284\n",
      "Config: fc, Epoch [4/10], Loss: 2.1927, Validation Macro F1: 0.0623\n",
      "Config: fc, Epoch [5/10], Loss: 2.1637, Validation Macro F1: 0.1413\n",
      "Config: fc, Epoch [6/10], Loss: 2.1340, Validation Macro F1: 0.1517\n",
      "Config: fc, Epoch [7/10], Loss: 2.1245, Validation Macro F1: 0.1974\n",
      "Config: fc, Epoch [8/10], Loss: 2.1141, Validation Macro F1: 0.1625\n",
      "Config: fc, Epoch [9/10], Loss: 2.0923, Validation Macro F1: 0.1037\n",
      "Config: fc, Epoch [10/10], Loss: 2.1013, Validation Macro F1: 0.1673\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc+encoder11\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_1.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_1.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_2.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_2.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "Unfreezing layer: heads.head.weight\n",
      "Unfreezing layer: heads.head.bias\n",
      "Config: fc+encoder11, Epoch [1/10], Loss: 2.3225, Validation Macro F1: 0.1034\n",
      "Config: fc+encoder11, Epoch [2/10], Loss: 2.1391, Validation Macro F1: 0.1183\n",
      "Config: fc+encoder11, Epoch [3/10], Loss: 2.0359, Validation Macro F1: 0.1847\n",
      "Config: fc+encoder11, Epoch [4/10], Loss: 2.0077, Validation Macro F1: 0.1699\n",
      "Config: fc+encoder11, Epoch [5/10], Loss: 1.9656, Validation Macro F1: 0.1696\n",
      "Config: fc+encoder11, Epoch [6/10], Loss: 1.9277, Validation Macro F1: 0.1816\n",
      "Config: fc+encoder11, Epoch [7/10], Loss: 1.9060, Validation Macro F1: 0.2015\n",
      "Config: fc+encoder11, Epoch [8/10], Loss: 1.9064, Validation Macro F1: 0.2420\n",
      "Config: fc+encoder11, Epoch [9/10], Loss: 1.8531, Validation Macro F1: 0.2397\n",
      "Config: fc+encoder11, Epoch [10/10], Loss: 1.9031, Validation Macro F1: 0.2442\n",
      "\n",
      "Starting Transfer Learning Phase with configuration: fc+encoder10+encoder11\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.ln_1.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.ln_1.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.ln_2.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.ln_2.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_1.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_1.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_2.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.ln_2.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "Unfreezing layer: encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "Unfreezing layer: heads.head.weight\n",
      "Unfreezing layer: heads.head.bias\n",
      "Config: fc+encoder10+encoder11, Epoch [1/10], Loss: 2.2787, Validation Macro F1: 0.1280\n",
      "Config: fc+encoder10+encoder11, Epoch [2/10], Loss: 2.1057, Validation Macro F1: 0.1536\n",
      "Config: fc+encoder10+encoder11, Epoch [3/10], Loss: 2.0259, Validation Macro F1: 0.1672\n",
      "Config: fc+encoder10+encoder11, Epoch [4/10], Loss: 2.0089, Validation Macro F1: 0.2144\n",
      "Config: fc+encoder10+encoder11, Epoch [5/10], Loss: 1.9273, Validation Macro F1: 0.1503\n",
      "Config: fc+encoder10+encoder11, Epoch [6/10], Loss: 1.9137, Validation Macro F1: 0.2373\n",
      "Config: fc+encoder10+encoder11, Epoch [7/10], Loss: 1.8824, Validation Macro F1: 0.2173\n",
      "Config: fc+encoder10+encoder11, Epoch [8/10], Loss: 1.8885, Validation Macro F1: 0.1951\n",
      "Config: fc+encoder10+encoder11, Epoch [9/10], Loss: 1.8429, Validation Macro F1: 0.2578\n",
      "Config: fc+encoder10+encoder11, Epoch [10/10], Loss: 1.8326, Validation Macro F1: 0.2517\n",
      "\n",
      "Best Configuration: fc+encoder10+encoder11 with Macro F1 Score: 0.2516600179358479\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Vision Transformer (ViT)\n",
    "model_vit = deepcopy(model2)  # assuming model2 is a pretrained Vision Transformer (ViT)\n",
    "model_vit = model_vit.to(device)\n",
    "\n",
    "# Call the training function\n",
    "train_model_with_autoencoder_and_grid_search(\n",
    "    model=model_vit,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    unlabelled_loader=unlabelled_loader,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    log_filename='vit_training_log.csv',\n",
    "    model_name='vit',\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\weita\\AppData\\Local\\Temp\\ipykernel_34524\\2800187119.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_vit.load_state_dict(torch.load(f'logs/best_model_vit.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ViT model\n",
    "best_model_vit = models.vit_b_16(pretrained=False)\n",
    "best_model_vit.heads.head = nn.Linear(best_model_vit.heads.head.in_features, num_classes)\n",
    "best_model_vit = best_model_vit.to(device)\n",
    "\n",
    "# Load the best model weights\n",
    "best_config = 'fc+encoder11'  # Replace with your best configuration name\n",
    "best_model_vit.load_state_dict(torch.load(f'logs/best_model_vit.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model_vit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 29.925%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(best_model_vit, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.2644254083813572\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.64      0.22      0.33       800\n",
      "     Class 1       0.22      0.47      0.30       800\n",
      "     Class 2       0.41      0.56      0.47       800\n",
      "     Class 3       0.20      0.26      0.23       800\n",
      "     Class 4       0.44      0.23      0.30       800\n",
      "     Class 5       0.00      0.00      0.00       800\n",
      "     Class 6       0.23      0.54      0.32       800\n",
      "     Class 7       0.18      0.01      0.03       800\n",
      "     Class 8       0.39      0.53      0.45       800\n",
      "     Class 9       0.31      0.17      0.22       800\n",
      "\n",
      "    accuracy                           0.30      8000\n",
      "   macro avg       0.30      0.30      0.26      8000\n",
      "weighted avg       0.30      0.30      0.26      8000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\weita\\anaconda3\\envs\\ECE4179_CV\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(best_model_vit, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>Data</h2>\n",
    "\n",
    "We will be using a dataset that can be obtained directly from the torchvision package. There are 10 classes and we will be training a CNN for the image classification task. We have training, validation and test sets that are labelled with the class, and a large unlabeled set.\n",
    "\n",
    "We will simulating a low training data scenario by only sampling a small percentage of the labelled data (10%) as training data. The remaining examples will be used as the validation set.\n",
    "\n",
    "To get the labelled data, change the dataset_dir to something suitable for your machine, and execute the following (you will then probably want to wrap the dataset objects in a PyTorch DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = \"../data\"\n",
    "# For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
    "\n",
    "# All images are 3x96x96\n",
    "image_size = 96\n",
    "# batch size\n",
    "batch_size = 128\n",
    "# number of classes in our case is 10\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print the GPU type and use device</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print the number of processors in cpu</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(image_size, padding=4),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_unlabelled = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.CenterCrop(image_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
    "\n",
    "#Load test set\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the unlabelled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find later that you want to make changes to how the unlabelled data is loaded. This might require you sub-classing the STL10 class used above or to create your own dataloader similar to the Pytorch one.\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/stl10.html#STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function\n",
    "def test_model(model, test_loader):\n",
    "    # Define the device inside the function\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Marco F1 Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test function to calculate F1 score\n",
    "def test_model_with_f1(model, test_loader):\n",
    "    # Define the device inside the function\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect all predictions and labels for F1-score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate the Macro F1-score for each class\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Alternatively, you can get a detailed report for all classes\n",
    "    report = classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(10)])\n",
    "    \n",
    "    print(f\"Macro F1-score: {f1}\")\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_grid_search(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_classes,\n",
    "    learning_rates,\n",
    "    num_epochs_list,\n",
    "    log_filename='training_log.csv',\n",
    "    model_name='resnet',\n",
    "    batch_size=128,\n",
    "    patience=5  # Early stopping patience\n",
    "):\n",
    "    # Ensure 'logs' directory exists for saving the best model\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "\n",
    "    ### Transfer Learning with Grid Search ###\n",
    "\n",
    "    print(\"Starting Transfer Learning with Grid Search...\")\n",
    "    # Define different layer unfreezing configurations based on model type\n",
    "    if model_name == 'resnet':\n",
    "        unfreeze_configs = {\n",
    "            'fc': ['fc'],\n",
    "            'fc+layer4': ['layer4', 'fc'],\n",
    "            'fc+layer3+layer4': ['layer3', 'layer4', 'fc'],\n",
    "        }\n",
    "    elif model_name == 'efficientnet':\n",
    "        unfreeze_configs = {\n",
    "            'fc': ['classifier.1'],\n",
    "            'fc+features8': ['features.8', 'classifier.1'],\n",
    "            'fc+features7+features8': ['features.7', 'features.8', 'classifier.1'],\n",
    "        }\n",
    "    elif model_name == 'vit':\n",
    "        unfreeze_configs = {\n",
    "            'fc': ['heads.head'],\n",
    "            'fc+encoder11': ['encoder.layers.encoder_layer_11', 'heads.head'],\n",
    "            'fc+encoder10+encoder11': ['encoder.layers.encoder_layer_10', 'encoder.layers.encoder_layer_11', 'heads.head'],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_config = ''\n",
    "    best_model_state = None\n",
    "\n",
    "    # Open log file for recording training progress\n",
    "    with open(os.path.join('logs', log_filename), mode='w', newline='') as log_file:\n",
    "        log_writer = csv.writer(log_file)\n",
    "        log_writer.writerow(['Configuration', 'Learning Rate', 'Num Epochs', 'Epoch', 'Training Loss', 'Validation Macro F1', 'Epoch Time (s)'])\n",
    "\n",
    "        for config_name, layers_to_unfreeze in unfreeze_configs.items():\n",
    "            for lr in learning_rates:\n",
    "                for num_epochs in num_epochs_list:\n",
    "                    print(f\"\\nConfiguration: {config_name}, Learning Rate: {lr}, Num Epochs: {num_epochs}\")\n",
    "                    # Create a new model by copying the original model\n",
    "                    finetune_model = deepcopy(model)\n",
    "\n",
    "                    # Modify the final layer based on the model's layer names\n",
    "                    if model_name == 'resnet':\n",
    "                        feature_dim = finetune_model.fc.in_features\n",
    "                        finetune_model.fc = nn.Linear(feature_dim, num_classes)\n",
    "                    elif model_name == 'efficientnet':\n",
    "                        feature_dim = finetune_model.classifier[1].in_features\n",
    "                        finetune_model.classifier[1] = nn.Linear(feature_dim, num_classes)\n",
    "                    elif model_name == 'vit':\n",
    "                        feature_dim = finetune_model.heads.head.in_features\n",
    "                        finetune_model.heads.head = nn.Linear(feature_dim, num_classes)\n",
    "                    else:\n",
    "                        raise ValueError(\"Model name not recognized.\")\n",
    "\n",
    "                    # Freeze all layers first\n",
    "                    for param in finetune_model.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "                    # Unfreeze specified layers\n",
    "                    for name, param in finetune_model.named_parameters():\n",
    "                        for layer_name in layers_to_unfreeze:\n",
    "                            if name.startswith(layer_name):\n",
    "                                param.requires_grad = True\n",
    "\n",
    "                    finetune_model = finetune_model.to(device)\n",
    "\n",
    "                    # Define loss function and optimizer for fine-tuning\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=lr)\n",
    "\n",
    "                    # Learning rate scheduler for fine-tuning\n",
    "                    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=patience, verbose=True)\n",
    "\n",
    "                    # Early stopping variables\n",
    "                    best_f1_local = 0.0\n",
    "                    epochs_no_improve = 0\n",
    "\n",
    "                    # Fine-tuning training loop\n",
    "                    for epoch in range(num_epochs):\n",
    "                        start_time = time.time()  # Start time for the epoch\n",
    "                        finetune_model.train()\n",
    "                        running_loss = 0.0\n",
    "                        for inputs, labels in train_loader:\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = finetune_model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            running_loss += loss.item()\n",
    "                        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "                        # Validation\n",
    "                        finetune_model.eval()\n",
    "                        all_labels = []\n",
    "                        all_preds = []\n",
    "                        with torch.no_grad():\n",
    "                            for inputs, labels in valid_loader:\n",
    "                                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                                outputs = finetune_model(inputs)\n",
    "                                _, predicted = torch.max(outputs, 1)\n",
    "                                all_labels.extend(labels.cpu().numpy())\n",
    "                                all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "                        # Calculate Macro F1 score\n",
    "                        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "                        # Step the scheduler\n",
    "                        scheduler.step(f1)\n",
    "\n",
    "                        # Calculate epoch time\n",
    "                        epoch_time = time.time() - start_time  # Calculate epoch duration\n",
    "\n",
    "                        # Log results\n",
    "                        log_writer.writerow([config_name, lr, num_epochs, epoch + 1, avg_loss, f1, epoch_time])\n",
    "                        print(f\"Config: {config_name}, LR: {lr}, Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Validation Macro F1: {f1:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "                        # Early stopping check\n",
    "                        if f1 > best_f1_local:\n",
    "                            best_f1_local = f1\n",
    "                            epochs_no_improve = 0\n",
    "                            # Save the best model for this configuration\n",
    "                            best_model_state_local = deepcopy(finetune_model.state_dict())\n",
    "                        else:\n",
    "                            epochs_no_improve += 1\n",
    "\n",
    "                        if epochs_no_improve >= patience:\n",
    "                            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                            break\n",
    "\n",
    "                    # Update best model if current config is better\n",
    "                    if best_f1_local > best_f1:\n",
    "                        best_f1 = best_f1_local\n",
    "                        best_config = f\"{config_name}, LR: {lr}, Num Epochs: {num_epochs}\"\n",
    "                        best_model_state = best_model_state_local\n",
    "\n",
    "    print(f\"\\nBest Configuration: {best_config} with Macro F1 Score: {best_f1}\")\n",
    "    # Save the best model\n",
    "    torch.save(best_model_state, os.path.join('logs', f\"best_model_{model_name}.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Let's use a ResNet18 architecture for our CNN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: conv1.weight, Shape: torch.Size([64, 3, 7, 7])\n",
      "Name: bn1.weight, Shape: torch.Size([64])\n",
      "Name: bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.0.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.0.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.0.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv1.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn1.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn1.bias, Shape: torch.Size([64])\n",
      "Name: layer1.1.conv2.weight, Shape: torch.Size([64, 64, 3, 3])\n",
      "Name: layer1.1.bn2.weight, Shape: torch.Size([64])\n",
      "Name: layer1.1.bn2.bias, Shape: torch.Size([64])\n",
      "Name: layer2.0.conv1.weight, Shape: torch.Size([128, 64, 3, 3])\n",
      "Name: layer2.0.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.0.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.0.weight, Shape: torch.Size([128, 64, 1, 1])\n",
      "Name: layer2.0.downsample.1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.0.downsample.1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv1.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn1.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn1.bias, Shape: torch.Size([128])\n",
      "Name: layer2.1.conv2.weight, Shape: torch.Size([128, 128, 3, 3])\n",
      "Name: layer2.1.bn2.weight, Shape: torch.Size([128])\n",
      "Name: layer2.1.bn2.bias, Shape: torch.Size([128])\n",
      "Name: layer3.0.conv1.weight, Shape: torch.Size([256, 128, 3, 3])\n",
      "Name: layer3.0.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.0.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.0.weight, Shape: torch.Size([256, 128, 1, 1])\n",
      "Name: layer3.0.downsample.1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.0.downsample.1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv1.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn1.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn1.bias, Shape: torch.Size([256])\n",
      "Name: layer3.1.conv2.weight, Shape: torch.Size([256, 256, 3, 3])\n",
      "Name: layer3.1.bn2.weight, Shape: torch.Size([256])\n",
      "Name: layer3.1.bn2.bias, Shape: torch.Size([256])\n",
      "Name: layer4.0.conv1.weight, Shape: torch.Size([512, 256, 3, 3])\n",
      "Name: layer4.0.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.0.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.bn2.bias, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.0.weight, Shape: torch.Size([512, 256, 1, 1])\n",
      "Name: layer4.0.downsample.1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.0.downsample.1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv1.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn1.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn1.bias, Shape: torch.Size([512])\n",
      "Name: layer4.1.conv2.weight, Shape: torch.Size([512, 512, 3, 3])\n",
      "Name: layer4.1.bn2.weight, Shape: torch.Size([512])\n",
      "Name: layer4.1.bn2.bias, Shape: torch.Size([512])\n",
      "Name: fc.weight, Shape: torch.Size([1000, 512])\n",
      "Name: fc.bias, Shape: torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\惟神君/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# We will keep this for later\n",
    "model0 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "\n",
    "for name, param in model0.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Transfer Learning with Grid Search...\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/10], Loss: 2.6335, Validation Macro F1: 0.0681, Time: 17.80s\n",
      "Config: fc, LR: 1e-05, Epoch [2/10], Loss: 2.5774, Validation Macro F1: 0.0682, Time: 17.68s\n",
      "Config: fc, LR: 1e-05, Epoch [3/10], Loss: 2.5863, Validation Macro F1: 0.0729, Time: 17.69s\n",
      "Config: fc, LR: 1e-05, Epoch [4/10], Loss: 2.5707, Validation Macro F1: 0.0808, Time: 17.99s\n",
      "Config: fc, LR: 1e-05, Epoch [5/10], Loss: 2.5604, Validation Macro F1: 0.0843, Time: 18.41s\n",
      "Config: fc, LR: 1e-05, Epoch [6/10], Loss: 2.5484, Validation Macro F1: 0.0789, Time: 18.27s\n",
      "Config: fc, LR: 1e-05, Epoch [7/10], Loss: 2.5188, Validation Macro F1: 0.0950, Time: 17.39s\n",
      "Config: fc, LR: 1e-05, Epoch [8/10], Loss: 2.5458, Validation Macro F1: 0.0903, Time: 17.85s\n",
      "Config: fc, LR: 1e-05, Epoch [9/10], Loss: 2.5528, Validation Macro F1: 0.0872, Time: 17.61s\n",
      "Config: fc, LR: 1e-05, Epoch [10/10], Loss: 2.5405, Validation Macro F1: 0.0974, Time: 17.72s\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/15], Loss: 2.5154, Validation Macro F1: 0.0939, Time: 17.67s\n",
      "Config: fc, LR: 1e-05, Epoch [2/15], Loss: 2.5170, Validation Macro F1: 0.0998, Time: 17.46s\n",
      "Config: fc, LR: 1e-05, Epoch [3/15], Loss: 2.4892, Validation Macro F1: 0.0998, Time: 17.56s\n",
      "Config: fc, LR: 1e-05, Epoch [4/15], Loss: 2.4840, Validation Macro F1: 0.0997, Time: 17.53s\n",
      "Config: fc, LR: 1e-05, Epoch [5/15], Loss: 2.5020, Validation Macro F1: 0.0998, Time: 17.55s\n",
      "Config: fc, LR: 1e-05, Epoch [6/15], Loss: 2.4751, Validation Macro F1: 0.1056, Time: 17.59s\n",
      "Config: fc, LR: 1e-05, Epoch [7/15], Loss: 2.4805, Validation Macro F1: 0.1066, Time: 17.58s\n",
      "Config: fc, LR: 1e-05, Epoch [8/15], Loss: 2.4802, Validation Macro F1: 0.1058, Time: 17.55s\n",
      "Config: fc, LR: 1e-05, Epoch [9/15], Loss: 2.4857, Validation Macro F1: 0.1086, Time: 17.31s\n",
      "Config: fc, LR: 1e-05, Epoch [10/15], Loss: 2.4561, Validation Macro F1: 0.1188, Time: 17.83s\n",
      "Config: fc, LR: 1e-05, Epoch [11/15], Loss: 2.4731, Validation Macro F1: 0.1093, Time: 17.81s\n",
      "Config: fc, LR: 1e-05, Epoch [12/15], Loss: 2.4513, Validation Macro F1: 0.1075, Time: 17.69s\n",
      "Config: fc, LR: 1e-05, Epoch [13/15], Loss: 2.4532, Validation Macro F1: 0.1045, Time: 18.04s\n",
      "Config: fc, LR: 1e-05, Epoch [14/15], Loss: 2.4634, Validation Macro F1: 0.0983, Time: 17.81s\n",
      "Config: fc, LR: 1e-05, Epoch [15/15], Loss: 2.4572, Validation Macro F1: 0.1160, Time: 17.65s\n",
      "Early stopping at epoch 15\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/20], Loss: 2.8422, Validation Macro F1: 0.0630, Time: 17.85s\n",
      "Config: fc, LR: 1e-05, Epoch [2/20], Loss: 2.8125, Validation Macro F1: 0.0676, Time: 17.58s\n",
      "Config: fc, LR: 1e-05, Epoch [3/20], Loss: 2.7816, Validation Macro F1: 0.0685, Time: 17.43s\n",
      "Config: fc, LR: 1e-05, Epoch [4/20], Loss: 2.7717, Validation Macro F1: 0.0661, Time: 17.52s\n",
      "Config: fc, LR: 1e-05, Epoch [5/20], Loss: 2.7819, Validation Macro F1: 0.0692, Time: 17.59s\n",
      "Config: fc, LR: 1e-05, Epoch [6/20], Loss: 2.7774, Validation Macro F1: 0.0648, Time: 17.57s\n",
      "Config: fc, LR: 1e-05, Epoch [7/20], Loss: 2.7699, Validation Macro F1: 0.0674, Time: 17.51s\n",
      "Config: fc, LR: 1e-05, Epoch [8/20], Loss: 2.7796, Validation Macro F1: 0.0720, Time: 17.66s\n",
      "Config: fc, LR: 1e-05, Epoch [9/20], Loss: 2.7392, Validation Macro F1: 0.0667, Time: 17.54s\n",
      "Config: fc, LR: 1e-05, Epoch [10/20], Loss: 2.7415, Validation Macro F1: 0.0673, Time: 17.49s\n",
      "Config: fc, LR: 1e-05, Epoch [11/20], Loss: 2.7456, Validation Macro F1: 0.0754, Time: 17.56s\n",
      "Config: fc, LR: 1e-05, Epoch [12/20], Loss: 2.7226, Validation Macro F1: 0.0778, Time: 17.58s\n",
      "Config: fc, LR: 1e-05, Epoch [13/20], Loss: 2.7133, Validation Macro F1: 0.0700, Time: 17.50s\n",
      "Config: fc, LR: 1e-05, Epoch [14/20], Loss: 2.7184, Validation Macro F1: 0.0723, Time: 17.59s\n",
      "Config: fc, LR: 1e-05, Epoch [15/20], Loss: 2.7066, Validation Macro F1: 0.0692, Time: 17.49s\n",
      "Config: fc, LR: 1e-05, Epoch [16/20], Loss: 2.7050, Validation Macro F1: 0.0721, Time: 17.64s\n",
      "Config: fc, LR: 1e-05, Epoch [17/20], Loss: 2.7044, Validation Macro F1: 0.0747, Time: 17.45s\n",
      "Early stopping at epoch 17\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/10], Loss: 2.4594, Validation Macro F1: 0.0940, Time: 17.33s\n",
      "Config: fc, LR: 0.0001, Epoch [2/10], Loss: 2.4017, Validation Macro F1: 0.1079, Time: 17.48s\n",
      "Config: fc, LR: 0.0001, Epoch [3/10], Loss: 2.3795, Validation Macro F1: 0.1213, Time: 17.55s\n",
      "Config: fc, LR: 0.0001, Epoch [4/10], Loss: 2.3503, Validation Macro F1: 0.1300, Time: 17.72s\n",
      "Config: fc, LR: 0.0001, Epoch [5/10], Loss: 2.2728, Validation Macro F1: 0.1447, Time: 17.50s\n",
      "Config: fc, LR: 0.0001, Epoch [6/10], Loss: 2.2734, Validation Macro F1: 0.1584, Time: 17.54s\n",
      "Config: fc, LR: 0.0001, Epoch [7/10], Loss: 2.2432, Validation Macro F1: 0.1616, Time: 17.50s\n",
      "Config: fc, LR: 0.0001, Epoch [8/10], Loss: 2.1950, Validation Macro F1: 0.1781, Time: 17.50s\n",
      "Config: fc, LR: 0.0001, Epoch [9/10], Loss: 2.1728, Validation Macro F1: 0.1854, Time: 17.54s\n",
      "Config: fc, LR: 0.0001, Epoch [10/10], Loss: 2.1099, Validation Macro F1: 0.1930, Time: 17.50s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/15], Loss: 2.4744, Validation Macro F1: 0.1058, Time: 17.49s\n",
      "Config: fc, LR: 0.0001, Epoch [2/15], Loss: 2.4079, Validation Macro F1: 0.1294, Time: 17.53s\n",
      "Config: fc, LR: 0.0001, Epoch [3/15], Loss: 2.3754, Validation Macro F1: 0.1503, Time: 17.51s\n",
      "Config: fc, LR: 0.0001, Epoch [4/15], Loss: 2.2947, Validation Macro F1: 0.1622, Time: 17.52s\n",
      "Config: fc, LR: 0.0001, Epoch [5/15], Loss: 2.2773, Validation Macro F1: 0.1725, Time: 17.53s\n",
      "Config: fc, LR: 0.0001, Epoch [6/15], Loss: 2.2935, Validation Macro F1: 0.1856, Time: 17.51s\n",
      "Config: fc, LR: 0.0001, Epoch [7/15], Loss: 2.2163, Validation Macro F1: 0.1921, Time: 17.52s\n",
      "Config: fc, LR: 0.0001, Epoch [8/15], Loss: 2.2087, Validation Macro F1: 0.2042, Time: 17.54s\n",
      "Config: fc, LR: 0.0001, Epoch [9/15], Loss: 2.1734, Validation Macro F1: 0.2159, Time: 17.55s\n",
      "Config: fc, LR: 0.0001, Epoch [10/15], Loss: 2.1356, Validation Macro F1: 0.2244, Time: 17.52s\n",
      "Config: fc, LR: 0.0001, Epoch [11/15], Loss: 2.1230, Validation Macro F1: 0.2315, Time: 17.51s\n",
      "Config: fc, LR: 0.0001, Epoch [12/15], Loss: 2.0720, Validation Macro F1: 0.2428, Time: 17.53s\n",
      "Config: fc, LR: 0.0001, Epoch [13/15], Loss: 2.0360, Validation Macro F1: 0.2672, Time: 17.55s\n",
      "Config: fc, LR: 0.0001, Epoch [14/15], Loss: 2.0257, Validation Macro F1: 0.2696, Time: 17.50s\n",
      "Config: fc, LR: 0.0001, Epoch [15/15], Loss: 1.9995, Validation Macro F1: 0.2729, Time: 17.57s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/20], Loss: 2.4845, Validation Macro F1: 0.1227, Time: 17.53s\n",
      "Config: fc, LR: 0.0001, Epoch [2/20], Loss: 2.4554, Validation Macro F1: 0.1315, Time: 17.57s\n",
      "Config: fc, LR: 0.0001, Epoch [3/20], Loss: 2.4009, Validation Macro F1: 0.1318, Time: 17.47s\n",
      "Config: fc, LR: 0.0001, Epoch [4/20], Loss: 2.3693, Validation Macro F1: 0.1405, Time: 17.56s\n",
      "Config: fc, LR: 0.0001, Epoch [5/20], Loss: 2.3198, Validation Macro F1: 0.1439, Time: 17.60s\n",
      "Config: fc, LR: 0.0001, Epoch [6/20], Loss: 2.2913, Validation Macro F1: 0.1507, Time: 17.47s\n",
      "Config: fc, LR: 0.0001, Epoch [7/20], Loss: 2.2969, Validation Macro F1: 0.1553, Time: 17.51s\n",
      "Config: fc, LR: 0.0001, Epoch [8/20], Loss: 2.2594, Validation Macro F1: 0.1680, Time: 17.55s\n",
      "Config: fc, LR: 0.0001, Epoch [9/20], Loss: 2.1773, Validation Macro F1: 0.1764, Time: 17.64s\n",
      "Config: fc, LR: 0.0001, Epoch [10/20], Loss: 2.1720, Validation Macro F1: 0.1849, Time: 17.53s\n",
      "Config: fc, LR: 0.0001, Epoch [11/20], Loss: 2.1241, Validation Macro F1: 0.1938, Time: 17.57s\n",
      "Config: fc, LR: 0.0001, Epoch [12/20], Loss: 2.0839, Validation Macro F1: 0.2027, Time: 17.48s\n",
      "Config: fc, LR: 0.0001, Epoch [13/20], Loss: 2.0561, Validation Macro F1: 0.2257, Time: 17.50s\n",
      "Config: fc, LR: 0.0001, Epoch [14/20], Loss: 2.0417, Validation Macro F1: 0.2365, Time: 17.54s\n",
      "Config: fc, LR: 0.0001, Epoch [15/20], Loss: 2.0083, Validation Macro F1: 0.2403, Time: 17.59s\n",
      "Config: fc, LR: 0.0001, Epoch [16/20], Loss: 2.0041, Validation Macro F1: 0.2457, Time: 17.49s\n",
      "Config: fc, LR: 0.0001, Epoch [17/20], Loss: 1.9297, Validation Macro F1: 0.2615, Time: 17.54s\n",
      "Config: fc, LR: 0.0001, Epoch [18/20], Loss: 1.9538, Validation Macro F1: 0.2775, Time: 17.53s\n",
      "Config: fc, LR: 0.0001, Epoch [19/20], Loss: 1.9033, Validation Macro F1: 0.2923, Time: 17.57s\n",
      "Config: fc, LR: 0.0001, Epoch [20/20], Loss: 1.8767, Validation Macro F1: 0.2986, Time: 17.47s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/10], Loss: 2.5082, Validation Macro F1: 0.1365, Time: 17.51s\n",
      "Config: fc, LR: 0.001, Epoch [2/10], Loss: 2.1614, Validation Macro F1: 0.2572, Time: 17.35s\n",
      "Config: fc, LR: 0.001, Epoch [3/10], Loss: 1.8710, Validation Macro F1: 0.3947, Time: 17.51s\n",
      "Config: fc, LR: 0.001, Epoch [4/10], Loss: 1.6609, Validation Macro F1: 0.4807, Time: 17.49s\n",
      "Config: fc, LR: 0.001, Epoch [5/10], Loss: 1.4573, Validation Macro F1: 0.5332, Time: 17.55s\n",
      "Config: fc, LR: 0.001, Epoch [6/10], Loss: 1.2992, Validation Macro F1: 0.5702, Time: 17.50s\n",
      "Config: fc, LR: 0.001, Epoch [7/10], Loss: 1.2116, Validation Macro F1: 0.5881, Time: 17.58s\n",
      "Config: fc, LR: 0.001, Epoch [8/10], Loss: 1.0828, Validation Macro F1: 0.6061, Time: 17.48s\n",
      "Config: fc, LR: 0.001, Epoch [9/10], Loss: 1.0414, Validation Macro F1: 0.6269, Time: 17.51s\n",
      "Config: fc, LR: 0.001, Epoch [10/10], Loss: 0.9902, Validation Macro F1: 0.6356, Time: 17.55s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/15], Loss: 2.4393, Validation Macro F1: 0.1901, Time: 17.53s\n",
      "Config: fc, LR: 0.001, Epoch [2/15], Loss: 2.1263, Validation Macro F1: 0.3035, Time: 17.53s\n",
      "Config: fc, LR: 0.001, Epoch [3/15], Loss: 1.8593, Validation Macro F1: 0.4011, Time: 17.46s\n",
      "Config: fc, LR: 0.001, Epoch [4/15], Loss: 1.6376, Validation Macro F1: 0.4724, Time: 17.54s\n",
      "Config: fc, LR: 0.001, Epoch [5/15], Loss: 1.4818, Validation Macro F1: 0.5344, Time: 17.52s\n",
      "Config: fc, LR: 0.001, Epoch [6/15], Loss: 1.2928, Validation Macro F1: 0.5754, Time: 17.54s\n",
      "Config: fc, LR: 0.001, Epoch [7/15], Loss: 1.1896, Validation Macro F1: 0.5861, Time: 17.50s\n",
      "Config: fc, LR: 0.001, Epoch [8/15], Loss: 1.0895, Validation Macro F1: 0.6025, Time: 17.55s\n",
      "Config: fc, LR: 0.001, Epoch [9/15], Loss: 1.0111, Validation Macro F1: 0.6246, Time: 17.53s\n",
      "Config: fc, LR: 0.001, Epoch [10/15], Loss: 0.9321, Validation Macro F1: 0.6350, Time: 17.55s\n",
      "Config: fc, LR: 0.001, Epoch [11/15], Loss: 0.9002, Validation Macro F1: 0.6420, Time: 17.56s\n",
      "Config: fc, LR: 0.001, Epoch [12/15], Loss: 0.8453, Validation Macro F1: 0.6560, Time: 17.45s\n",
      "Config: fc, LR: 0.001, Epoch [13/15], Loss: 0.8213, Validation Macro F1: 0.6517, Time: 17.56s\n",
      "Config: fc, LR: 0.001, Epoch [14/15], Loss: 0.7813, Validation Macro F1: 0.6515, Time: 17.55s\n",
      "Config: fc, LR: 0.001, Epoch [15/15], Loss: 0.7288, Validation Macro F1: 0.6595, Time: 17.52s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/20], Loss: 2.4819, Validation Macro F1: 0.1318, Time: 17.58s\n",
      "Config: fc, LR: 0.001, Epoch [2/20], Loss: 2.1542, Validation Macro F1: 0.2780, Time: 17.47s\n",
      "Config: fc, LR: 0.001, Epoch [3/20], Loss: 1.8368, Validation Macro F1: 0.4384, Time: 17.53s\n",
      "Config: fc, LR: 0.001, Epoch [4/20], Loss: 1.6267, Validation Macro F1: 0.5193, Time: 17.49s\n",
      "Config: fc, LR: 0.001, Epoch [5/20], Loss: 1.4331, Validation Macro F1: 0.5624, Time: 17.51s\n",
      "Config: fc, LR: 0.001, Epoch [6/20], Loss: 1.2925, Validation Macro F1: 0.5905, Time: 17.54s\n",
      "Config: fc, LR: 0.001, Epoch [7/20], Loss: 1.1585, Validation Macro F1: 0.6190, Time: 17.52s\n",
      "Config: fc, LR: 0.001, Epoch [8/20], Loss: 1.0725, Validation Macro F1: 0.6341, Time: 17.49s\n",
      "Config: fc, LR: 0.001, Epoch [9/20], Loss: 0.9882, Validation Macro F1: 0.6491, Time: 17.52s\n",
      "Config: fc, LR: 0.001, Epoch [10/20], Loss: 0.9608, Validation Macro F1: 0.6621, Time: 17.54s\n",
      "Config: fc, LR: 0.001, Epoch [11/20], Loss: 0.8681, Validation Macro F1: 0.6609, Time: 17.52s\n",
      "Config: fc, LR: 0.001, Epoch [12/20], Loss: 0.8442, Validation Macro F1: 0.6611, Time: 17.54s\n",
      "Config: fc, LR: 0.001, Epoch [13/20], Loss: 0.8250, Validation Macro F1: 0.6771, Time: 17.51s\n",
      "Config: fc, LR: 0.001, Epoch [14/20], Loss: 0.7948, Validation Macro F1: 0.6776, Time: 17.50s\n",
      "Config: fc, LR: 0.001, Epoch [15/20], Loss: 0.7880, Validation Macro F1: 0.6752, Time: 17.49s\n",
      "Config: fc, LR: 0.001, Epoch [16/20], Loss: 0.7313, Validation Macro F1: 0.6745, Time: 17.56s\n",
      "Config: fc, LR: 0.001, Epoch [17/20], Loss: 0.6984, Validation Macro F1: 0.6788, Time: 17.49s\n",
      "Config: fc, LR: 0.001, Epoch [18/20], Loss: 0.6790, Validation Macro F1: 0.6862, Time: 17.49s\n",
      "Config: fc, LR: 0.001, Epoch [19/20], Loss: 0.6525, Validation Macro F1: 0.6914, Time: 17.62s\n",
      "Config: fc, LR: 0.001, Epoch [20/20], Loss: 0.6612, Validation Macro F1: 0.6794, Time: 17.65s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/10], Loss: 3.0020, Validation Macro F1: 0.3065, Time: 17.57s\n",
      "Config: fc, LR: 0.01, Epoch [2/10], Loss: 1.6297, Validation Macro F1: 0.4862, Time: 17.55s\n",
      "Config: fc, LR: 0.01, Epoch [3/10], Loss: 1.1640, Validation Macro F1: 0.5902, Time: 17.47s\n",
      "Config: fc, LR: 0.01, Epoch [4/10], Loss: 0.9243, Validation Macro F1: 0.6252, Time: 17.56s\n",
      "Config: fc, LR: 0.01, Epoch [5/10], Loss: 0.7377, Validation Macro F1: 0.6572, Time: 17.49s\n",
      "Config: fc, LR: 0.01, Epoch [6/10], Loss: 0.6087, Validation Macro F1: 0.6929, Time: 17.54s\n",
      "Config: fc, LR: 0.01, Epoch [7/10], Loss: 0.5151, Validation Macro F1: 0.6848, Time: 17.52s\n",
      "Config: fc, LR: 0.01, Epoch [8/10], Loss: 0.4579, Validation Macro F1: 0.6916, Time: 17.51s\n",
      "Config: fc, LR: 0.01, Epoch [9/10], Loss: 0.4307, Validation Macro F1: 0.6886, Time: 17.53s\n",
      "Config: fc, LR: 0.01, Epoch [10/10], Loss: 0.3774, Validation Macro F1: 0.6784, Time: 17.59s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/15], Loss: 2.4801, Validation Macro F1: 0.3855, Time: 17.48s\n",
      "Config: fc, LR: 0.01, Epoch [2/15], Loss: 1.3326, Validation Macro F1: 0.6415, Time: 17.50s\n",
      "Config: fc, LR: 0.01, Epoch [3/15], Loss: 0.9953, Validation Macro F1: 0.7012, Time: 17.54s\n",
      "Config: fc, LR: 0.01, Epoch [4/15], Loss: 0.7538, Validation Macro F1: 0.6519, Time: 17.56s\n",
      "Config: fc, LR: 0.01, Epoch [5/15], Loss: 0.5795, Validation Macro F1: 0.6919, Time: 17.55s\n",
      "Config: fc, LR: 0.01, Epoch [6/15], Loss: 0.5812, Validation Macro F1: 0.7085, Time: 17.52s\n",
      "Config: fc, LR: 0.01, Epoch [7/15], Loss: 0.4503, Validation Macro F1: 0.6890, Time: 17.89s\n",
      "Config: fc, LR: 0.01, Epoch [8/15], Loss: 0.3861, Validation Macro F1: 0.6979, Time: 17.57s\n",
      "Config: fc, LR: 0.01, Epoch [9/15], Loss: 0.3809, Validation Macro F1: 0.6960, Time: 17.48s\n",
      "Config: fc, LR: 0.01, Epoch [10/15], Loss: 0.3555, Validation Macro F1: 0.7078, Time: 17.55s\n",
      "Config: fc, LR: 0.01, Epoch [11/15], Loss: 0.3200, Validation Macro F1: 0.6921, Time: 17.54s\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/20], Loss: 3.6170, Validation Macro F1: 0.2341, Time: 17.53s\n",
      "Config: fc, LR: 0.01, Epoch [2/20], Loss: 2.1660, Validation Macro F1: 0.3645, Time: 17.50s\n",
      "Config: fc, LR: 0.01, Epoch [3/20], Loss: 1.4011, Validation Macro F1: 0.6046, Time: 17.57s\n",
      "Config: fc, LR: 0.01, Epoch [4/20], Loss: 1.0520, Validation Macro F1: 0.6315, Time: 17.48s\n",
      "Config: fc, LR: 0.01, Epoch [5/20], Loss: 0.7862, Validation Macro F1: 0.6628, Time: 17.50s\n",
      "Config: fc, LR: 0.01, Epoch [6/20], Loss: 0.6400, Validation Macro F1: 0.6494, Time: 17.49s\n",
      "Config: fc, LR: 0.01, Epoch [7/20], Loss: 0.6187, Validation Macro F1: 0.6825, Time: 17.53s\n",
      "Config: fc, LR: 0.01, Epoch [8/20], Loss: 0.5218, Validation Macro F1: 0.6867, Time: 17.51s\n",
      "Config: fc, LR: 0.01, Epoch [9/20], Loss: 0.4826, Validation Macro F1: 0.7027, Time: 17.52s\n",
      "Config: fc, LR: 0.01, Epoch [10/20], Loss: 0.4038, Validation Macro F1: 0.6789, Time: 17.55s\n",
      "Config: fc, LR: 0.01, Epoch [11/20], Loss: 0.3548, Validation Macro F1: 0.7023, Time: 17.49s\n",
      "Config: fc, LR: 0.01, Epoch [12/20], Loss: 0.3838, Validation Macro F1: 0.7090, Time: 17.54s\n",
      "Config: fc, LR: 0.01, Epoch [13/20], Loss: 0.3589, Validation Macro F1: 0.6875, Time: 17.51s\n",
      "Config: fc, LR: 0.01, Epoch [14/20], Loss: 0.3092, Validation Macro F1: 0.7003, Time: 17.55s\n",
      "Config: fc, LR: 0.01, Epoch [15/20], Loss: 0.3331, Validation Macro F1: 0.6926, Time: 17.48s\n",
      "Config: fc, LR: 0.01, Epoch [16/20], Loss: 0.3099, Validation Macro F1: 0.6892, Time: 17.52s\n",
      "Config: fc, LR: 0.01, Epoch [17/20], Loss: 0.2534, Validation Macro F1: 0.7003, Time: 17.50s\n",
      "Early stopping at epoch 17\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 1e-05, Epoch [1/10], Loss: 2.5683, Validation Macro F1: 0.0941, Time: 17.54s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [2/10], Loss: 2.4610, Validation Macro F1: 0.1206, Time: 17.55s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [3/10], Loss: 2.3392, Validation Macro F1: 0.1518, Time: 17.49s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [4/10], Loss: 2.2164, Validation Macro F1: 0.1727, Time: 17.52s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [5/10], Loss: 2.1351, Validation Macro F1: 0.2042, Time: 17.54s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [6/10], Loss: 2.0441, Validation Macro F1: 0.2354, Time: 17.49s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [7/10], Loss: 1.9367, Validation Macro F1: 0.2587, Time: 17.52s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [8/10], Loss: 1.8690, Validation Macro F1: 0.2891, Time: 17.47s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [9/10], Loss: 1.7810, Validation Macro F1: 0.3104, Time: 17.56s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [10/10], Loss: 1.6978, Validation Macro F1: 0.3403, Time: 17.51s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 1e-05, Epoch [1/15], Loss: 2.5562, Validation Macro F1: 0.1024, Time: 17.53s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [2/15], Loss: 2.3974, Validation Macro F1: 0.1221, Time: 17.48s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [3/15], Loss: 2.3035, Validation Macro F1: 0.1437, Time: 17.54s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [4/15], Loss: 2.1817, Validation Macro F1: 0.1664, Time: 17.55s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [5/15], Loss: 2.1039, Validation Macro F1: 0.2009, Time: 17.64s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [6/15], Loss: 2.0116, Validation Macro F1: 0.2160, Time: 17.53s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [7/15], Loss: 1.9314, Validation Macro F1: 0.2487, Time: 17.56s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [8/15], Loss: 1.8445, Validation Macro F1: 0.2716, Time: 17.51s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [9/15], Loss: 1.7854, Validation Macro F1: 0.3015, Time: 17.51s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [10/15], Loss: 1.7262, Validation Macro F1: 0.3194, Time: 17.53s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [11/15], Loss: 1.6035, Validation Macro F1: 0.3508, Time: 17.55s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [12/15], Loss: 1.5456, Validation Macro F1: 0.3755, Time: 17.55s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [13/15], Loss: 1.5051, Validation Macro F1: 0.3838, Time: 17.51s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [14/15], Loss: 1.4383, Validation Macro F1: 0.4060, Time: 17.49s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [15/15], Loss: 1.3932, Validation Macro F1: 0.4301, Time: 17.56s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 1e-05, Epoch [1/20], Loss: 2.4884, Validation Macro F1: 0.1129, Time: 17.50s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [2/20], Loss: 2.4009, Validation Macro F1: 0.1384, Time: 17.57s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [3/20], Loss: 2.3084, Validation Macro F1: 0.1547, Time: 17.53s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [4/20], Loss: 2.1922, Validation Macro F1: 0.1737, Time: 17.49s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [5/20], Loss: 2.0952, Validation Macro F1: 0.1975, Time: 17.56s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [6/20], Loss: 2.0021, Validation Macro F1: 0.2162, Time: 17.50s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [7/20], Loss: 1.9331, Validation Macro F1: 0.2575, Time: 17.67s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [8/20], Loss: 1.8722, Validation Macro F1: 0.2866, Time: 17.54s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [9/20], Loss: 1.7615, Validation Macro F1: 0.3095, Time: 17.51s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [10/20], Loss: 1.6985, Validation Macro F1: 0.3414, Time: 17.52s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [11/20], Loss: 1.6462, Validation Macro F1: 0.3574, Time: 17.52s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [12/20], Loss: 1.5772, Validation Macro F1: 0.3813, Time: 17.54s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [13/20], Loss: 1.5206, Validation Macro F1: 0.4089, Time: 17.51s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [14/20], Loss: 1.4829, Validation Macro F1: 0.4318, Time: 17.54s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [15/20], Loss: 1.3987, Validation Macro F1: 0.4518, Time: 17.53s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [16/20], Loss: 1.3468, Validation Macro F1: 0.4655, Time: 17.51s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [17/20], Loss: 1.3141, Validation Macro F1: 0.4806, Time: 17.53s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [18/20], Loss: 1.2603, Validation Macro F1: 0.5115, Time: 17.50s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [19/20], Loss: 1.2182, Validation Macro F1: 0.5077, Time: 17.55s\n",
      "Config: fc+layer4, LR: 1e-05, Epoch [20/20], Loss: 1.1797, Validation Macro F1: 0.5181, Time: 17.53s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.0001, Epoch [1/10], Loss: 2.3264, Validation Macro F1: 0.2354, Time: 17.49s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [2/10], Loss: 1.5030, Validation Macro F1: 0.4425, Time: 17.50s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [3/10], Loss: 1.0477, Validation Macro F1: 0.5878, Time: 17.56s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [4/10], Loss: 0.7828, Validation Macro F1: 0.6603, Time: 17.54s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [5/10], Loss: 0.5965, Validation Macro F1: 0.6990, Time: 17.51s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [6/10], Loss: 0.4313, Validation Macro F1: 0.7166, Time: 17.49s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [7/10], Loss: 0.3531, Validation Macro F1: 0.7301, Time: 17.49s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [8/10], Loss: 0.2780, Validation Macro F1: 0.7303, Time: 17.55s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [9/10], Loss: 0.2121, Validation Macro F1: 0.7463, Time: 17.51s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [10/10], Loss: 0.1635, Validation Macro F1: 0.7538, Time: 17.54s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.0001, Epoch [1/15], Loss: 2.3216, Validation Macro F1: 0.3318, Time: 17.51s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [2/15], Loss: 1.4634, Validation Macro F1: 0.5457, Time: 17.45s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [3/15], Loss: 0.9870, Validation Macro F1: 0.6352, Time: 17.51s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [4/15], Loss: 0.7326, Validation Macro F1: 0.6904, Time: 17.52s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [5/15], Loss: 0.5513, Validation Macro F1: 0.7175, Time: 17.70s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [6/15], Loss: 0.4209, Validation Macro F1: 0.7335, Time: 17.65s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [7/15], Loss: 0.3060, Validation Macro F1: 0.7484, Time: 17.22s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [8/15], Loss: 0.2530, Validation Macro F1: 0.7591, Time: 17.59s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [9/15], Loss: 0.1800, Validation Macro F1: 0.7575, Time: 17.40s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [10/15], Loss: 0.1488, Validation Macro F1: 0.7581, Time: 17.59s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [11/15], Loss: 0.1114, Validation Macro F1: 0.7644, Time: 17.56s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [12/15], Loss: 0.0884, Validation Macro F1: 0.7657, Time: 17.43s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [13/15], Loss: 0.0766, Validation Macro F1: 0.7686, Time: 17.58s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [14/15], Loss: 0.0609, Validation Macro F1: 0.7675, Time: 17.60s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [15/15], Loss: 0.0490, Validation Macro F1: 0.7687, Time: 17.55s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.0001, Epoch [1/20], Loss: 2.3366, Validation Macro F1: 0.2574, Time: 17.39s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [2/20], Loss: 1.5069, Validation Macro F1: 0.4368, Time: 17.64s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [3/20], Loss: 1.0555, Validation Macro F1: 0.5779, Time: 17.51s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [4/20], Loss: 0.7852, Validation Macro F1: 0.6618, Time: 17.61s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [5/20], Loss: 0.6020, Validation Macro F1: 0.6967, Time: 17.54s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [6/20], Loss: 0.4658, Validation Macro F1: 0.7206, Time: 17.40s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [7/20], Loss: 0.3605, Validation Macro F1: 0.7310, Time: 17.55s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [8/20], Loss: 0.2807, Validation Macro F1: 0.7351, Time: 17.57s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [9/20], Loss: 0.2277, Validation Macro F1: 0.7480, Time: 17.80s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [10/20], Loss: 0.1682, Validation Macro F1: 0.7424, Time: 17.81s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [11/20], Loss: 0.1354, Validation Macro F1: 0.7478, Time: 17.88s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [12/20], Loss: 0.1150, Validation Macro F1: 0.7559, Time: 17.47s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [13/20], Loss: 0.0874, Validation Macro F1: 0.7556, Time: 17.53s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [14/20], Loss: 0.0764, Validation Macro F1: 0.7618, Time: 17.56s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [15/20], Loss: 0.0577, Validation Macro F1: 0.7603, Time: 17.92s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [16/20], Loss: 0.0466, Validation Macro F1: 0.7601, Time: 17.71s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [17/20], Loss: 0.0443, Validation Macro F1: 0.7628, Time: 17.52s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [18/20], Loss: 0.0337, Validation Macro F1: 0.7569, Time: 18.01s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [19/20], Loss: 0.0319, Validation Macro F1: 0.7628, Time: 17.70s\n",
      "Config: fc+layer4, LR: 0.0001, Epoch [20/20], Loss: 0.0320, Validation Macro F1: 0.7669, Time: 17.72s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.001, Epoch [1/10], Loss: 1.6052, Validation Macro F1: 0.4765, Time: 17.56s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [2/10], Loss: 0.3654, Validation Macro F1: 0.7224, Time: 17.81s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [3/10], Loss: 0.1724, Validation Macro F1: 0.7444, Time: 17.93s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [4/10], Loss: 0.0626, Validation Macro F1: 0.7462, Time: 18.28s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [5/10], Loss: 0.0501, Validation Macro F1: 0.7581, Time: 17.68s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [6/10], Loss: 0.0225, Validation Macro F1: 0.7618, Time: 17.92s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [7/10], Loss: 0.0154, Validation Macro F1: 0.7618, Time: 17.68s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [8/10], Loss: 0.0076, Validation Macro F1: 0.7690, Time: 17.99s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [9/10], Loss: 0.0136, Validation Macro F1: 0.7626, Time: 17.68s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [10/10], Loss: 0.0065, Validation Macro F1: 0.7653, Time: 18.16s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.001, Epoch [1/15], Loss: 1.5237, Validation Macro F1: 0.6048, Time: 18.09s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [2/15], Loss: 0.3207, Validation Macro F1: 0.6942, Time: 17.86s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [3/15], Loss: 0.1386, Validation Macro F1: 0.7338, Time: 17.89s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [4/15], Loss: 0.1057, Validation Macro F1: 0.7276, Time: 17.65s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [5/15], Loss: 0.0456, Validation Macro F1: 0.7404, Time: 17.70s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [6/15], Loss: 0.0341, Validation Macro F1: 0.7530, Time: 17.83s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [7/15], Loss: 0.0279, Validation Macro F1: 0.7622, Time: 18.05s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [8/15], Loss: 0.0154, Validation Macro F1: 0.7684, Time: 18.16s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [9/15], Loss: 0.0101, Validation Macro F1: 0.7600, Time: 18.18s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [10/15], Loss: 0.0144, Validation Macro F1: 0.7526, Time: 18.52s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [11/15], Loss: 0.0054, Validation Macro F1: 0.7427, Time: 18.42s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [12/15], Loss: 0.0083, Validation Macro F1: 0.7507, Time: 18.54s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [13/15], Loss: 0.0060, Validation Macro F1: 0.7590, Time: 18.24s\n",
      "Early stopping at epoch 13\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.001, Epoch [1/20], Loss: 1.7117, Validation Macro F1: 0.5915, Time: 18.18s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [2/20], Loss: 0.3606, Validation Macro F1: 0.7236, Time: 18.63s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [3/20], Loss: 0.1246, Validation Macro F1: 0.7614, Time: 18.09s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [4/20], Loss: 0.0649, Validation Macro F1: 0.7560, Time: 17.80s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [5/20], Loss: 0.0523, Validation Macro F1: 0.7555, Time: 17.64s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [6/20], Loss: 0.0291, Validation Macro F1: 0.7679, Time: 18.45s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [7/20], Loss: 0.0158, Validation Macro F1: 0.7655, Time: 17.53s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [8/20], Loss: 0.0082, Validation Macro F1: 0.7624, Time: 17.67s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [9/20], Loss: 0.0070, Validation Macro F1: 0.7562, Time: 17.81s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [10/20], Loss: 0.0077, Validation Macro F1: 0.7613, Time: 17.76s\n",
      "Config: fc+layer4, LR: 0.001, Epoch [11/20], Loss: 0.0113, Validation Macro F1: 0.7591, Time: 17.71s\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.01, Epoch [1/10], Loss: 3.8116, Validation Macro F1: 0.1333, Time: 17.96s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [2/10], Loss: 1.7673, Validation Macro F1: 0.1942, Time: 17.96s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [3/10], Loss: 1.1204, Validation Macro F1: 0.4475, Time: 17.96s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [4/10], Loss: 0.6549, Validation Macro F1: 0.5699, Time: 18.17s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [5/10], Loss: 0.3793, Validation Macro F1: 0.6254, Time: 18.31s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [6/10], Loss: 0.2932, Validation Macro F1: 0.7110, Time: 19.50s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [7/10], Loss: 0.1412, Validation Macro F1: 0.7328, Time: 19.17s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [8/10], Loss: 0.0883, Validation Macro F1: 0.7319, Time: 18.28s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [9/10], Loss: 0.0902, Validation Macro F1: 0.7089, Time: 18.43s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [10/10], Loss: 0.0852, Validation Macro F1: 0.7357, Time: 18.24s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.01, Epoch [1/15], Loss: 2.6124, Validation Macro F1: 0.1336, Time: 18.15s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [2/15], Loss: 1.1264, Validation Macro F1: 0.1282, Time: 18.06s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [3/15], Loss: 0.6026, Validation Macro F1: 0.3794, Time: 18.68s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [4/15], Loss: 0.3822, Validation Macro F1: 0.5882, Time: 18.63s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [5/15], Loss: 0.2492, Validation Macro F1: 0.7110, Time: 18.41s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [6/15], Loss: 0.1498, Validation Macro F1: 0.7092, Time: 18.33s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [7/15], Loss: 0.0742, Validation Macro F1: 0.7027, Time: 18.37s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [8/15], Loss: 0.0528, Validation Macro F1: 0.6996, Time: 18.37s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [9/15], Loss: 0.0717, Validation Macro F1: 0.7050, Time: 18.21s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [10/15], Loss: 0.0252, Validation Macro F1: 0.7306, Time: 18.37s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [11/15], Loss: 0.0411, Validation Macro F1: 0.7498, Time: 17.87s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [12/15], Loss: 0.0205, Validation Macro F1: 0.7465, Time: 18.22s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [13/15], Loss: 0.0172, Validation Macro F1: 0.7431, Time: 18.46s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [14/15], Loss: 0.0421, Validation Macro F1: 0.7313, Time: 18.53s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [15/15], Loss: 0.0380, Validation Macro F1: 0.7430, Time: 18.22s\n",
      "\n",
      "Configuration: fc+layer4, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer4, LR: 0.01, Epoch [1/20], Loss: 2.4771, Validation Macro F1: 0.0594, Time: 18.40s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [2/20], Loss: 1.5039, Validation Macro F1: 0.2340, Time: 18.25s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [3/20], Loss: 0.6790, Validation Macro F1: 0.3482, Time: 18.17s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [4/20], Loss: 0.4962, Validation Macro F1: 0.5057, Time: 18.17s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [5/20], Loss: 0.2670, Validation Macro F1: 0.6416, Time: 18.29s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [6/20], Loss: 0.2169, Validation Macro F1: 0.6978, Time: 18.05s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [7/20], Loss: 0.1199, Validation Macro F1: 0.7170, Time: 18.15s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [8/20], Loss: 0.1365, Validation Macro F1: 0.7165, Time: 18.33s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [9/20], Loss: 0.0757, Validation Macro F1: 0.7177, Time: 18.23s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [10/20], Loss: 0.0586, Validation Macro F1: 0.7227, Time: 18.09s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [11/20], Loss: 0.0340, Validation Macro F1: 0.7269, Time: 18.03s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [12/20], Loss: 0.0317, Validation Macro F1: 0.7162, Time: 18.23s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [13/20], Loss: 0.0410, Validation Macro F1: 0.7288, Time: 18.00s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [14/20], Loss: 0.0493, Validation Macro F1: 0.7307, Time: 18.11s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [15/20], Loss: 0.0184, Validation Macro F1: 0.7238, Time: 18.10s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [16/20], Loss: 0.0523, Validation Macro F1: 0.7329, Time: 18.00s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [17/20], Loss: 0.0421, Validation Macro F1: 0.7054, Time: 18.10s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [18/20], Loss: 0.0540, Validation Macro F1: 0.7061, Time: 18.08s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [19/20], Loss: 0.1047, Validation Macro F1: 0.7215, Time: 17.98s\n",
      "Config: fc+layer4, LR: 0.01, Epoch [20/20], Loss: 0.0239, Validation Macro F1: 0.7060, Time: 18.01s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [1/10], Loss: 2.5820, Validation Macro F1: 0.0723, Time: 17.95s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [2/10], Loss: 2.4620, Validation Macro F1: 0.0954, Time: 17.99s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [3/10], Loss: 2.3443, Validation Macro F1: 0.1209, Time: 18.00s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [4/10], Loss: 2.2260, Validation Macro F1: 0.1638, Time: 17.88s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [5/10], Loss: 2.1100, Validation Macro F1: 0.1996, Time: 18.17s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [6/10], Loss: 2.0093, Validation Macro F1: 0.2197, Time: 18.04s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [7/10], Loss: 1.9179, Validation Macro F1: 0.2608, Time: 17.92s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [8/10], Loss: 1.8376, Validation Macro F1: 0.2844, Time: 18.04s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [9/10], Loss: 1.7315, Validation Macro F1: 0.3071, Time: 18.03s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [10/10], Loss: 1.6509, Validation Macro F1: 0.3390, Time: 17.97s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [1/15], Loss: 2.5692, Validation Macro F1: 0.0960, Time: 18.01s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [2/15], Loss: 2.4131, Validation Macro F1: 0.1215, Time: 17.95s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [3/15], Loss: 2.3213, Validation Macro F1: 0.1428, Time: 17.89s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [4/15], Loss: 2.2149, Validation Macro F1: 0.1673, Time: 17.93s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [5/15], Loss: 2.0961, Validation Macro F1: 0.1942, Time: 18.02s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [6/15], Loss: 1.9828, Validation Macro F1: 0.2234, Time: 17.88s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [7/15], Loss: 1.8802, Validation Macro F1: 0.2564, Time: 17.90s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [8/15], Loss: 1.8310, Validation Macro F1: 0.2769, Time: 18.09s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [9/15], Loss: 1.7223, Validation Macro F1: 0.3087, Time: 18.12s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [10/15], Loss: 1.6315, Validation Macro F1: 0.3399, Time: 18.09s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [11/15], Loss: 1.5754, Validation Macro F1: 0.3556, Time: 17.95s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [12/15], Loss: 1.4878, Validation Macro F1: 0.3894, Time: 18.33s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [13/15], Loss: 1.4324, Validation Macro F1: 0.4088, Time: 18.22s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [14/15], Loss: 1.3677, Validation Macro F1: 0.4297, Time: 17.77s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [15/15], Loss: 1.2934, Validation Macro F1: 0.4607, Time: 17.33s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [1/20], Loss: 2.6519, Validation Macro F1: 0.0694, Time: 17.59s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [2/20], Loss: 2.5225, Validation Macro F1: 0.0911, Time: 17.74s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [3/20], Loss: 2.3632, Validation Macro F1: 0.0997, Time: 17.89s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [4/20], Loss: 2.2798, Validation Macro F1: 0.1206, Time: 18.15s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [5/20], Loss: 2.1668, Validation Macro F1: 0.1457, Time: 18.34s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [6/20], Loss: 2.0487, Validation Macro F1: 0.1639, Time: 18.52s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [7/20], Loss: 1.9726, Validation Macro F1: 0.1941, Time: 17.90s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [8/20], Loss: 1.8612, Validation Macro F1: 0.2227, Time: 18.04s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [9/20], Loss: 1.7725, Validation Macro F1: 0.2490, Time: 18.14s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [10/20], Loss: 1.6978, Validation Macro F1: 0.2810, Time: 18.07s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [11/20], Loss: 1.6085, Validation Macro F1: 0.3230, Time: 17.93s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [12/20], Loss: 1.5376, Validation Macro F1: 0.3531, Time: 17.57s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [13/20], Loss: 1.4450, Validation Macro F1: 0.3900, Time: 17.81s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [14/20], Loss: 1.3798, Validation Macro F1: 0.4018, Time: 17.68s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [15/20], Loss: 1.3204, Validation Macro F1: 0.4287, Time: 17.57s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [16/20], Loss: 1.2534, Validation Macro F1: 0.4554, Time: 17.67s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [17/20], Loss: 1.2132, Validation Macro F1: 0.4797, Time: 17.58s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [18/20], Loss: 1.1650, Validation Macro F1: 0.5035, Time: 17.49s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [19/20], Loss: 1.0919, Validation Macro F1: 0.5190, Time: 17.52s\n",
      "Config: fc+layer3+layer4, LR: 1e-05, Epoch [20/20], Loss: 1.0553, Validation Macro F1: 0.5310, Time: 17.59s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [1/10], Loss: 2.2216, Validation Macro F1: 0.3132, Time: 17.66s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [2/10], Loss: 1.3216, Validation Macro F1: 0.5163, Time: 17.53s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [3/10], Loss: 0.8656, Validation Macro F1: 0.6305, Time: 17.52s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [4/10], Loss: 0.6218, Validation Macro F1: 0.6883, Time: 17.57s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [5/10], Loss: 0.4356, Validation Macro F1: 0.7185, Time: 17.53s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [6/10], Loss: 0.3146, Validation Macro F1: 0.7367, Time: 17.51s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [7/10], Loss: 0.2329, Validation Macro F1: 0.7551, Time: 17.57s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [8/10], Loss: 0.1636, Validation Macro F1: 0.7611, Time: 17.52s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [9/10], Loss: 0.1138, Validation Macro F1: 0.7717, Time: 17.56s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [10/10], Loss: 0.0892, Validation Macro F1: 0.7779, Time: 17.50s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [1/15], Loss: 2.1865, Validation Macro F1: 0.3969, Time: 17.53s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [2/15], Loss: 1.2964, Validation Macro F1: 0.5793, Time: 17.54s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [3/15], Loss: 0.8539, Validation Macro F1: 0.6719, Time: 17.52s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [4/15], Loss: 0.5978, Validation Macro F1: 0.7101, Time: 17.57s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [5/15], Loss: 0.4103, Validation Macro F1: 0.7404, Time: 17.55s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [6/15], Loss: 0.2734, Validation Macro F1: 0.7575, Time: 17.49s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [7/15], Loss: 0.2036, Validation Macro F1: 0.7689, Time: 17.87s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [8/15], Loss: 0.1480, Validation Macro F1: 0.7793, Time: 18.23s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [9/15], Loss: 0.0999, Validation Macro F1: 0.7735, Time: 18.29s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [10/15], Loss: 0.0784, Validation Macro F1: 0.7793, Time: 18.39s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [11/15], Loss: 0.0564, Validation Macro F1: 0.7837, Time: 18.42s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [12/15], Loss: 0.0431, Validation Macro F1: 0.7863, Time: 18.23s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [13/15], Loss: 0.0347, Validation Macro F1: 0.7857, Time: 18.12s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [14/15], Loss: 0.0271, Validation Macro F1: 0.7902, Time: 18.27s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [15/15], Loss: 0.0280, Validation Macro F1: 0.7958, Time: 18.17s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [1/20], Loss: 2.5099, Validation Macro F1: 0.2794, Time: 18.41s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [2/20], Loss: 1.5460, Validation Macro F1: 0.5576, Time: 18.25s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [3/20], Loss: 0.9593, Validation Macro F1: 0.6477, Time: 18.19s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [4/20], Loss: 0.6619, Validation Macro F1: 0.7054, Time: 18.27s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [5/20], Loss: 0.4596, Validation Macro F1: 0.7338, Time: 18.00s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [6/20], Loss: 0.3380, Validation Macro F1: 0.7528, Time: 18.07s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [7/20], Loss: 0.2344, Validation Macro F1: 0.7587, Time: 18.06s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [8/20], Loss: 0.1615, Validation Macro F1: 0.7634, Time: 18.06s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [9/20], Loss: 0.1224, Validation Macro F1: 0.7702, Time: 18.11s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [10/20], Loss: 0.0872, Validation Macro F1: 0.7706, Time: 18.12s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [11/20], Loss: 0.0738, Validation Macro F1: 0.7796, Time: 18.07s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [12/20], Loss: 0.0614, Validation Macro F1: 0.7782, Time: 18.26s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [13/20], Loss: 0.0397, Validation Macro F1: 0.7848, Time: 18.18s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [14/20], Loss: 0.0382, Validation Macro F1: 0.7799, Time: 18.19s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [15/20], Loss: 0.0298, Validation Macro F1: 0.7877, Time: 18.78s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [16/20], Loss: 0.0262, Validation Macro F1: 0.7903, Time: 18.05s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [17/20], Loss: 0.0227, Validation Macro F1: 0.7908, Time: 18.05s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [18/20], Loss: 0.0196, Validation Macro F1: 0.7814, Time: 17.99s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [19/20], Loss: 0.0190, Validation Macro F1: 0.7869, Time: 17.56s\n",
      "Config: fc+layer3+layer4, LR: 0.0001, Epoch [20/20], Loss: 0.0164, Validation Macro F1: 0.7949, Time: 17.90s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [1/10], Loss: 1.4875, Validation Macro F1: 0.5220, Time: 17.90s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [2/10], Loss: 0.3527, Validation Macro F1: 0.6626, Time: 18.32s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [3/10], Loss: 0.1838, Validation Macro F1: 0.7242, Time: 18.42s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [4/10], Loss: 0.0905, Validation Macro F1: 0.6944, Time: 18.08s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [5/10], Loss: 0.0591, Validation Macro F1: 0.6654, Time: 18.16s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [6/10], Loss: 0.0650, Validation Macro F1: 0.6798, Time: 18.41s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [7/10], Loss: 0.0724, Validation Macro F1: 0.7037, Time: 17.80s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [8/10], Loss: 0.0542, Validation Macro F1: 0.7160, Time: 18.21s\n",
      "Early stopping at epoch 8\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [1/15], Loss: 1.5764, Validation Macro F1: 0.5272, Time: 18.36s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [2/15], Loss: 0.4075, Validation Macro F1: 0.4999, Time: 18.13s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [3/15], Loss: 0.1795, Validation Macro F1: 0.6059, Time: 17.85s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [4/15], Loss: 0.1313, Validation Macro F1: 0.6114, Time: 17.90s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [5/15], Loss: 0.1044, Validation Macro F1: 0.6551, Time: 18.23s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [6/15], Loss: 0.0633, Validation Macro F1: 0.7033, Time: 18.01s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [7/15], Loss: 0.0450, Validation Macro F1: 0.7127, Time: 18.09s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [8/15], Loss: 0.0277, Validation Macro F1: 0.7273, Time: 17.88s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [9/15], Loss: 0.0375, Validation Macro F1: 0.7157, Time: 17.89s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [10/15], Loss: 0.0443, Validation Macro F1: 0.7258, Time: 18.34s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [11/15], Loss: 0.0295, Validation Macro F1: 0.7299, Time: 18.55s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [12/15], Loss: 0.0402, Validation Macro F1: 0.7364, Time: 18.46s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [13/15], Loss: 0.0436, Validation Macro F1: 0.7387, Time: 18.32s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [14/15], Loss: 0.0140, Validation Macro F1: 0.7384, Time: 18.28s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [15/15], Loss: 0.0182, Validation Macro F1: 0.7174, Time: 18.28s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [1/20], Loss: 1.6566, Validation Macro F1: 0.3977, Time: 18.08s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [2/20], Loss: 0.3887, Validation Macro F1: 0.5799, Time: 18.08s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [3/20], Loss: 0.1837, Validation Macro F1: 0.6287, Time: 18.00s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [4/20], Loss: 0.1343, Validation Macro F1: 0.6947, Time: 18.49s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [5/20], Loss: 0.0707, Validation Macro F1: 0.6566, Time: 17.87s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [6/20], Loss: 0.0466, Validation Macro F1: 0.6870, Time: 17.86s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [7/20], Loss: 0.0400, Validation Macro F1: 0.7392, Time: 18.12s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [8/20], Loss: 0.0286, Validation Macro F1: 0.7337, Time: 18.31s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [9/20], Loss: 0.0171, Validation Macro F1: 0.7317, Time: 18.33s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [10/20], Loss: 0.0110, Validation Macro F1: 0.7376, Time: 17.67s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [11/20], Loss: 0.0069, Validation Macro F1: 0.7426, Time: 17.95s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [12/20], Loss: 0.0078, Validation Macro F1: 0.7466, Time: 18.37s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [13/20], Loss: 0.0106, Validation Macro F1: 0.7392, Time: 18.45s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [14/20], Loss: 0.0063, Validation Macro F1: 0.7133, Time: 18.66s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [15/20], Loss: 0.0212, Validation Macro F1: 0.7373, Time: 17.87s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [16/20], Loss: 0.0101, Validation Macro F1: 0.7278, Time: 17.89s\n",
      "Config: fc+layer3+layer4, LR: 0.001, Epoch [17/20], Loss: 0.0281, Validation Macro F1: 0.7230, Time: 17.88s\n",
      "Early stopping at epoch 17\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [1/10], Loss: 3.3914, Validation Macro F1: 0.0192, Time: 18.22s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [2/10], Loss: 1.6994, Validation Macro F1: 0.0490, Time: 17.79s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [3/10], Loss: 1.7090, Validation Macro F1: 0.0187, Time: 18.28s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [4/10], Loss: 1.3040, Validation Macro F1: 0.0564, Time: 18.17s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [5/10], Loss: 1.1345, Validation Macro F1: 0.1204, Time: 18.10s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [6/10], Loss: 0.8543, Validation Macro F1: 0.2579, Time: 17.77s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [7/10], Loss: 0.7731, Validation Macro F1: 0.3588, Time: 17.62s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [8/10], Loss: 0.6974, Validation Macro F1: 0.4856, Time: 17.46s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [9/10], Loss: 0.5885, Validation Macro F1: 0.4364, Time: 17.94s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [10/10], Loss: 0.5520, Validation Macro F1: 0.5759, Time: 18.41s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [1/15], Loss: 3.7285, Validation Macro F1: 0.0180, Time: 18.17s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [2/15], Loss: 2.9055, Validation Macro F1: 0.0693, Time: 18.78s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [3/15], Loss: 1.7528, Validation Macro F1: 0.0384, Time: 18.44s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [4/15], Loss: 1.3442, Validation Macro F1: 0.0911, Time: 18.18s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [5/15], Loss: 1.1584, Validation Macro F1: 0.1787, Time: 18.60s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [6/15], Loss: 1.0768, Validation Macro F1: 0.2648, Time: 18.41s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [7/15], Loss: 1.0250, Validation Macro F1: 0.3269, Time: 18.34s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [8/15], Loss: 0.8181, Validation Macro F1: 0.3522, Time: 18.51s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [9/15], Loss: 0.7420, Validation Macro F1: 0.4126, Time: 18.38s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [10/15], Loss: 0.6774, Validation Macro F1: 0.5144, Time: 18.29s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [11/15], Loss: 0.5561, Validation Macro F1: 0.4533, Time: 17.94s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [12/15], Loss: 0.4648, Validation Macro F1: 0.5609, Time: 18.35s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [13/15], Loss: 0.3412, Validation Macro F1: 0.5726, Time: 18.56s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [14/15], Loss: 0.2942, Validation Macro F1: 0.6124, Time: 18.52s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [15/15], Loss: 0.2770, Validation Macro F1: 0.5602, Time: 18.16s\n",
      "\n",
      "Configuration: fc+layer3+layer4, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [1/20], Loss: 3.3549, Validation Macro F1: 0.0450, Time: 18.41s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [2/20], Loss: 1.8253, Validation Macro F1: 0.0854, Time: 18.32s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [3/20], Loss: 1.4934, Validation Macro F1: 0.0222, Time: 18.30s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [4/20], Loss: 1.1266, Validation Macro F1: 0.0441, Time: 18.37s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [5/20], Loss: 0.8804, Validation Macro F1: 0.1827, Time: 18.05s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [6/20], Loss: 0.7580, Validation Macro F1: 0.2843, Time: 18.02s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [7/20], Loss: 0.7024, Validation Macro F1: 0.3294, Time: 18.06s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [8/20], Loss: 0.5386, Validation Macro F1: 0.3689, Time: 18.15s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [9/20], Loss: 0.4896, Validation Macro F1: 0.5679, Time: 18.27s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [10/20], Loss: 0.3809, Validation Macro F1: 0.6047, Time: 18.11s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [11/20], Loss: 0.2870, Validation Macro F1: 0.5636, Time: 17.93s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [12/20], Loss: 0.2129, Validation Macro F1: 0.5899, Time: 18.08s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [13/20], Loss: 0.2623, Validation Macro F1: 0.6342, Time: 18.56s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [14/20], Loss: 0.2031, Validation Macro F1: 0.6311, Time: 19.19s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [15/20], Loss: 0.2205, Validation Macro F1: 0.6390, Time: 18.26s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [16/20], Loss: 0.3282, Validation Macro F1: 0.6110, Time: 18.88s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [17/20], Loss: 0.1919, Validation Macro F1: 0.5916, Time: 18.68s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [18/20], Loss: 0.1424, Validation Macro F1: 0.6051, Time: 18.42s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [19/20], Loss: 0.1972, Validation Macro F1: 0.6327, Time: 18.57s\n",
      "Config: fc+layer3+layer4, LR: 0.01, Epoch [20/20], Loss: 0.1731, Validation Macro F1: 0.6306, Time: 18.82s\n",
      "Early stopping at epoch 20\n",
      "\n",
      "Best Configuration: fc+layer3+layer4, LR: 0.0001, Num Epochs: 15 with Macro F1 Score: 0.7957739401197126\n"
     ]
    }
   ],
   "source": [
    "# Example usage with ResNet18\n",
    "model_resnet18 = deepcopy(model0)  # assuming model0 is a pretrained resnet18\n",
    "model_resnet18 = model_resnet18.to(device)\n",
    "\n",
    "# Define learning rates and number of epochs for grid search\n",
    "resnet18_learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "resnet18_num_epochs_list = [10, 15, 20]\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Call the updated training function without the autoencoder\n",
    "train_model_with_grid_search(\n",
    "    model=model_resnet18,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_classes=num_classes,\n",
    "    learning_rates=resnet18_learning_rates,\n",
    "    num_epochs_list=resnet18_num_epochs_list,\n",
    "    log_filename='resnet18_training_log.csv',\n",
    "    model_name='resnet',\n",
    "    batch_size=batch_size,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\惟神君\\AppData\\Local\\Temp\\ipykernel_16392\\2960042565.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_resnet.load_state_dict(torch.load('logs/best_model_resnet.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ResNet18 model with pretrained weights\n",
    "best_model_resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the final fully connected layer to match the number of classes\n",
    "best_model_resnet.fc = nn.Linear(best_model_resnet.fc.in_features, num_classes)\n",
    "\n",
    "# Load the best model weights (ensure map_location if needed)\n",
    "best_model_resnet.load_state_dict(torch.load('logs/best_model_resnet.pth', map_location=device))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "best_model_resnet = best_model_resnet.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model_resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.9%\n"
     ]
    }
   ],
   "source": [
    "# Call the test functions\n",
    "test_model(best_model_resnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.7972120266791742\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.82      0.90      0.86       800\n",
      "     Class 1       0.82      0.85      0.84       800\n",
      "     Class 2       0.90      0.92      0.91       800\n",
      "     Class 3       0.73      0.65      0.69       800\n",
      "     Class 4       0.75      0.79      0.77       800\n",
      "     Class 5       0.71      0.62      0.66       800\n",
      "     Class 6       0.77      0.82      0.80       800\n",
      "     Class 7       0.75      0.81      0.78       800\n",
      "     Class 8       0.88      0.88      0.88       800\n",
      "     Class 9       0.85      0.75      0.80       800\n",
      "\n",
      "    accuracy                           0.80      8000\n",
      "   macro avg       0.80      0.80      0.80      8000\n",
      "weighted avg       0.80      0.80      0.80      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_f1(best_model_resnet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: features.0.0.weight, Shape: torch.Size([32, 3, 3, 3])\n",
      "Name: features.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.0.weight, Shape: torch.Size([32, 1, 3, 3])\n",
      "Name: features.1.0.block.0.1.weight, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.0.1.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.1.fc1.weight, Shape: torch.Size([8, 32, 1, 1])\n",
      "Name: features.1.0.block.1.fc1.bias, Shape: torch.Size([8])\n",
      "Name: features.1.0.block.1.fc2.weight, Shape: torch.Size([32, 8, 1, 1])\n",
      "Name: features.1.0.block.1.fc2.bias, Shape: torch.Size([32])\n",
      "Name: features.1.0.block.2.0.weight, Shape: torch.Size([16, 32, 1, 1])\n",
      "Name: features.1.0.block.2.1.weight, Shape: torch.Size([16])\n",
      "Name: features.1.0.block.2.1.bias, Shape: torch.Size([16])\n",
      "Name: features.2.0.block.0.0.weight, Shape: torch.Size([96, 16, 1, 1])\n",
      "Name: features.2.0.block.0.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.0.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.0.weight, Shape: torch.Size([96, 1, 3, 3])\n",
      "Name: features.2.0.block.1.1.weight, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.1.1.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.2.fc1.weight, Shape: torch.Size([4, 96, 1, 1])\n",
      "Name: features.2.0.block.2.fc1.bias, Shape: torch.Size([4])\n",
      "Name: features.2.0.block.2.fc2.weight, Shape: torch.Size([96, 4, 1, 1])\n",
      "Name: features.2.0.block.2.fc2.bias, Shape: torch.Size([96])\n",
      "Name: features.2.0.block.3.0.weight, Shape: torch.Size([24, 96, 1, 1])\n",
      "Name: features.2.0.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.0.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.2.1.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.0.weight, Shape: torch.Size([144, 1, 3, 3])\n",
      "Name: features.2.1.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.2.1.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.2.1.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.2.1.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.2.1.block.3.0.weight, Shape: torch.Size([24, 144, 1, 1])\n",
      "Name: features.2.1.block.3.1.weight, Shape: torch.Size([24])\n",
      "Name: features.2.1.block.3.1.bias, Shape: torch.Size([24])\n",
      "Name: features.3.0.block.0.0.weight, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: features.3.0.block.0.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.0.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.0.weight, Shape: torch.Size([144, 1, 5, 5])\n",
      "Name: features.3.0.block.1.1.weight, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.1.1.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.2.fc1.weight, Shape: torch.Size([6, 144, 1, 1])\n",
      "Name: features.3.0.block.2.fc1.bias, Shape: torch.Size([6])\n",
      "Name: features.3.0.block.2.fc2.weight, Shape: torch.Size([144, 6, 1, 1])\n",
      "Name: features.3.0.block.2.fc2.bias, Shape: torch.Size([144])\n",
      "Name: features.3.0.block.3.0.weight, Shape: torch.Size([40, 144, 1, 1])\n",
      "Name: features.3.0.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.0.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.3.1.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.0.weight, Shape: torch.Size([240, 1, 5, 5])\n",
      "Name: features.3.1.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.3.1.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.3.1.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.3.1.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.3.1.block.3.0.weight, Shape: torch.Size([40, 240, 1, 1])\n",
      "Name: features.3.1.block.3.1.weight, Shape: torch.Size([40])\n",
      "Name: features.3.1.block.3.1.bias, Shape: torch.Size([40])\n",
      "Name: features.4.0.block.0.0.weight, Shape: torch.Size([240, 40, 1, 1])\n",
      "Name: features.4.0.block.0.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.0.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.0.weight, Shape: torch.Size([240, 1, 3, 3])\n",
      "Name: features.4.0.block.1.1.weight, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.1.1.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.2.fc1.weight, Shape: torch.Size([10, 240, 1, 1])\n",
      "Name: features.4.0.block.2.fc1.bias, Shape: torch.Size([10])\n",
      "Name: features.4.0.block.2.fc2.weight, Shape: torch.Size([240, 10, 1, 1])\n",
      "Name: features.4.0.block.2.fc2.bias, Shape: torch.Size([240])\n",
      "Name: features.4.0.block.3.0.weight, Shape: torch.Size([80, 240, 1, 1])\n",
      "Name: features.4.0.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.0.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.1.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.1.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.1.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.1.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.1.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.1.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.1.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.1.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.4.2.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.0.weight, Shape: torch.Size([480, 1, 3, 3])\n",
      "Name: features.4.2.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.4.2.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.4.2.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.4.2.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.4.2.block.3.0.weight, Shape: torch.Size([80, 480, 1, 1])\n",
      "Name: features.4.2.block.3.1.weight, Shape: torch.Size([80])\n",
      "Name: features.4.2.block.3.1.bias, Shape: torch.Size([80])\n",
      "Name: features.5.0.block.0.0.weight, Shape: torch.Size([480, 80, 1, 1])\n",
      "Name: features.5.0.block.0.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.0.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.0.weight, Shape: torch.Size([480, 1, 5, 5])\n",
      "Name: features.5.0.block.1.1.weight, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.1.1.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.2.fc1.weight, Shape: torch.Size([20, 480, 1, 1])\n",
      "Name: features.5.0.block.2.fc1.bias, Shape: torch.Size([20])\n",
      "Name: features.5.0.block.2.fc2.weight, Shape: torch.Size([480, 20, 1, 1])\n",
      "Name: features.5.0.block.2.fc2.bias, Shape: torch.Size([480])\n",
      "Name: features.5.0.block.3.0.weight, Shape: torch.Size([112, 480, 1, 1])\n",
      "Name: features.5.0.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.0.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.1.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.1.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.1.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.1.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.1.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.1.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.1.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.1.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.5.2.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.5.2.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.5.2.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.5.2.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.5.2.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.5.2.block.3.0.weight, Shape: torch.Size([112, 672, 1, 1])\n",
      "Name: features.5.2.block.3.1.weight, Shape: torch.Size([112])\n",
      "Name: features.5.2.block.3.1.bias, Shape: torch.Size([112])\n",
      "Name: features.6.0.block.0.0.weight, Shape: torch.Size([672, 112, 1, 1])\n",
      "Name: features.6.0.block.0.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.0.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.0.weight, Shape: torch.Size([672, 1, 5, 5])\n",
      "Name: features.6.0.block.1.1.weight, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.1.1.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.2.fc1.weight, Shape: torch.Size([28, 672, 1, 1])\n",
      "Name: features.6.0.block.2.fc1.bias, Shape: torch.Size([28])\n",
      "Name: features.6.0.block.2.fc2.weight, Shape: torch.Size([672, 28, 1, 1])\n",
      "Name: features.6.0.block.2.fc2.bias, Shape: torch.Size([672])\n",
      "Name: features.6.0.block.3.0.weight, Shape: torch.Size([192, 672, 1, 1])\n",
      "Name: features.6.0.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.0.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.1.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.1.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.1.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.1.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.1.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.1.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.1.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.1.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.2.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.2.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.2.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.2.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.2.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.2.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.2.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.2.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.6.3.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.0.weight, Shape: torch.Size([1152, 1, 5, 5])\n",
      "Name: features.6.3.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.6.3.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.6.3.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.6.3.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.6.3.block.3.0.weight, Shape: torch.Size([192, 1152, 1, 1])\n",
      "Name: features.6.3.block.3.1.weight, Shape: torch.Size([192])\n",
      "Name: features.6.3.block.3.1.bias, Shape: torch.Size([192])\n",
      "Name: features.7.0.block.0.0.weight, Shape: torch.Size([1152, 192, 1, 1])\n",
      "Name: features.7.0.block.0.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.0.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.0.weight, Shape: torch.Size([1152, 1, 3, 3])\n",
      "Name: features.7.0.block.1.1.weight, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.1.1.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.2.fc1.weight, Shape: torch.Size([48, 1152, 1, 1])\n",
      "Name: features.7.0.block.2.fc1.bias, Shape: torch.Size([48])\n",
      "Name: features.7.0.block.2.fc2.weight, Shape: torch.Size([1152, 48, 1, 1])\n",
      "Name: features.7.0.block.2.fc2.bias, Shape: torch.Size([1152])\n",
      "Name: features.7.0.block.3.0.weight, Shape: torch.Size([320, 1152, 1, 1])\n",
      "Name: features.7.0.block.3.1.weight, Shape: torch.Size([320])\n",
      "Name: features.7.0.block.3.1.bias, Shape: torch.Size([320])\n",
      "Name: features.8.0.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "Name: features.8.1.weight, Shape: torch.Size([1280])\n",
      "Name: features.8.1.bias, Shape: torch.Size([1280])\n",
      "Name: classifier.1.weight, Shape: torch.Size([1000, 1280])\n",
      "Name: classifier.1.bias, Shape: torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\惟神君/.cache\\torch\\hub\\pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained EfficientNet-B0 model from torchvision hub\n",
    "model1 = torch.hub.load('pytorch/vision', 'efficientnet_b0', weights=\"EfficientNet_B0_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "for name, param in model1.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Transfer Learning with Grid Search...\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/10], Loss: 2.3544, Validation Macro F1: 0.0957, Time: 18.44s\n",
      "Config: fc, LR: 1e-05, Epoch [2/10], Loss: 2.3548, Validation Macro F1: 0.0916, Time: 18.35s\n",
      "Config: fc, LR: 1e-05, Epoch [3/10], Loss: 2.3615, Validation Macro F1: 0.1003, Time: 18.28s\n",
      "Config: fc, LR: 1e-05, Epoch [4/10], Loss: 2.3553, Validation Macro F1: 0.0978, Time: 18.36s\n",
      "Config: fc, LR: 1e-05, Epoch [5/10], Loss: 2.3451, Validation Macro F1: 0.1038, Time: 18.54s\n",
      "Config: fc, LR: 1e-05, Epoch [6/10], Loss: 2.3460, Validation Macro F1: 0.1019, Time: 18.23s\n",
      "Config: fc, LR: 1e-05, Epoch [7/10], Loss: 2.3471, Validation Macro F1: 0.1001, Time: 18.16s\n",
      "Config: fc, LR: 1e-05, Epoch [8/10], Loss: 2.3340, Validation Macro F1: 0.1058, Time: 18.25s\n",
      "Config: fc, LR: 1e-05, Epoch [9/10], Loss: 2.3192, Validation Macro F1: 0.0982, Time: 18.39s\n",
      "Config: fc, LR: 1e-05, Epoch [10/10], Loss: 2.3469, Validation Macro F1: 0.1105, Time: 18.33s\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/15], Loss: 2.3488, Validation Macro F1: 0.1115, Time: 18.29s\n",
      "Config: fc, LR: 1e-05, Epoch [2/15], Loss: 2.3388, Validation Macro F1: 0.1117, Time: 18.35s\n",
      "Config: fc, LR: 1e-05, Epoch [3/15], Loss: 2.3523, Validation Macro F1: 0.1119, Time: 18.34s\n",
      "Config: fc, LR: 1e-05, Epoch [4/15], Loss: 2.3341, Validation Macro F1: 0.1146, Time: 18.31s\n",
      "Config: fc, LR: 1e-05, Epoch [5/15], Loss: 2.3376, Validation Macro F1: 0.1163, Time: 18.23s\n",
      "Config: fc, LR: 1e-05, Epoch [6/15], Loss: 2.3535, Validation Macro F1: 0.1183, Time: 18.54s\n",
      "Config: fc, LR: 1e-05, Epoch [7/15], Loss: 2.3349, Validation Macro F1: 0.1205, Time: 18.31s\n",
      "Config: fc, LR: 1e-05, Epoch [8/15], Loss: 2.3291, Validation Macro F1: 0.1152, Time: 18.08s\n",
      "Config: fc, LR: 1e-05, Epoch [9/15], Loss: 2.3254, Validation Macro F1: 0.1229, Time: 18.04s\n",
      "Config: fc, LR: 1e-05, Epoch [10/15], Loss: 2.3484, Validation Macro F1: 0.1179, Time: 18.12s\n",
      "Config: fc, LR: 1e-05, Epoch [11/15], Loss: 2.3362, Validation Macro F1: 0.1252, Time: 18.09s\n",
      "Config: fc, LR: 1e-05, Epoch [12/15], Loss: 2.3217, Validation Macro F1: 0.1120, Time: 18.08s\n",
      "Config: fc, LR: 1e-05, Epoch [13/15], Loss: 2.3035, Validation Macro F1: 0.1168, Time: 18.23s\n",
      "Config: fc, LR: 1e-05, Epoch [14/15], Loss: 2.3163, Validation Macro F1: 0.1130, Time: 18.23s\n",
      "Config: fc, LR: 1e-05, Epoch [15/15], Loss: 2.3082, Validation Macro F1: 0.1239, Time: 18.47s\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/20], Loss: 2.3550, Validation Macro F1: 0.0947, Time: 18.35s\n",
      "Config: fc, LR: 1e-05, Epoch [2/20], Loss: 2.3610, Validation Macro F1: 0.0863, Time: 18.51s\n",
      "Config: fc, LR: 1e-05, Epoch [3/20], Loss: 2.3564, Validation Macro F1: 0.0952, Time: 18.13s\n",
      "Config: fc, LR: 1e-05, Epoch [4/20], Loss: 2.3503, Validation Macro F1: 0.0879, Time: 17.90s\n",
      "Config: fc, LR: 1e-05, Epoch [5/20], Loss: 2.3645, Validation Macro F1: 0.0960, Time: 17.80s\n",
      "Config: fc, LR: 1e-05, Epoch [6/20], Loss: 2.3716, Validation Macro F1: 0.0868, Time: 17.70s\n",
      "Config: fc, LR: 1e-05, Epoch [7/20], Loss: 2.3618, Validation Macro F1: 0.0993, Time: 18.53s\n",
      "Config: fc, LR: 1e-05, Epoch [8/20], Loss: 2.3594, Validation Macro F1: 0.0923, Time: 17.88s\n",
      "Config: fc, LR: 1e-05, Epoch [9/20], Loss: 2.3741, Validation Macro F1: 0.0951, Time: 17.86s\n",
      "Config: fc, LR: 1e-05, Epoch [10/20], Loss: 2.3556, Validation Macro F1: 0.0943, Time: 18.47s\n",
      "Config: fc, LR: 1e-05, Epoch [11/20], Loss: 2.3543, Validation Macro F1: 0.0960, Time: 18.31s\n",
      "Config: fc, LR: 1e-05, Epoch [12/20], Loss: 2.3473, Validation Macro F1: 0.0982, Time: 18.38s\n",
      "Early stopping at epoch 12\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/10], Loss: 2.3497, Validation Macro F1: 0.0912, Time: 18.33s\n",
      "Config: fc, LR: 0.0001, Epoch [2/10], Loss: 2.3290, Validation Macro F1: 0.1102, Time: 18.22s\n",
      "Config: fc, LR: 0.0001, Epoch [3/10], Loss: 2.3026, Validation Macro F1: 0.1284, Time: 18.18s\n",
      "Config: fc, LR: 0.0001, Epoch [4/10], Loss: 2.2826, Validation Macro F1: 0.1366, Time: 18.08s\n",
      "Config: fc, LR: 0.0001, Epoch [5/10], Loss: 2.2585, Validation Macro F1: 0.1495, Time: 18.04s\n",
      "Config: fc, LR: 0.0001, Epoch [6/10], Loss: 2.2261, Validation Macro F1: 0.1718, Time: 18.07s\n",
      "Config: fc, LR: 0.0001, Epoch [7/10], Loss: 2.1999, Validation Macro F1: 0.1833, Time: 18.12s\n",
      "Config: fc, LR: 0.0001, Epoch [8/10], Loss: 2.1758, Validation Macro F1: 0.2029, Time: 18.25s\n",
      "Config: fc, LR: 0.0001, Epoch [9/10], Loss: 2.1878, Validation Macro F1: 0.2165, Time: 18.04s\n",
      "Config: fc, LR: 0.0001, Epoch [10/10], Loss: 2.1436, Validation Macro F1: 0.2372, Time: 18.09s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/15], Loss: 2.3760, Validation Macro F1: 0.1031, Time: 18.07s\n",
      "Config: fc, LR: 0.0001, Epoch [2/15], Loss: 2.3477, Validation Macro F1: 0.1145, Time: 18.07s\n",
      "Config: fc, LR: 0.0001, Epoch [3/15], Loss: 2.3183, Validation Macro F1: 0.1271, Time: 18.21s\n",
      "Config: fc, LR: 0.0001, Epoch [4/15], Loss: 2.3024, Validation Macro F1: 0.1330, Time: 18.09s\n",
      "Config: fc, LR: 0.0001, Epoch [5/15], Loss: 2.2814, Validation Macro F1: 0.1468, Time: 18.20s\n",
      "Config: fc, LR: 0.0001, Epoch [6/15], Loss: 2.2802, Validation Macro F1: 0.1658, Time: 18.13s\n",
      "Config: fc, LR: 0.0001, Epoch [7/15], Loss: 2.2120, Validation Macro F1: 0.1719, Time: 18.24s\n",
      "Config: fc, LR: 0.0001, Epoch [8/15], Loss: 2.2041, Validation Macro F1: 0.1931, Time: 18.34s\n",
      "Config: fc, LR: 0.0001, Epoch [9/15], Loss: 2.2008, Validation Macro F1: 0.2144, Time: 18.13s\n",
      "Config: fc, LR: 0.0001, Epoch [10/15], Loss: 2.1702, Validation Macro F1: 0.2244, Time: 18.15s\n",
      "Config: fc, LR: 0.0001, Epoch [11/15], Loss: 2.1366, Validation Macro F1: 0.2323, Time: 18.05s\n",
      "Config: fc, LR: 0.0001, Epoch [12/15], Loss: 2.1154, Validation Macro F1: 0.2582, Time: 18.08s\n",
      "Config: fc, LR: 0.0001, Epoch [13/15], Loss: 2.0893, Validation Macro F1: 0.2691, Time: 18.09s\n",
      "Config: fc, LR: 0.0001, Epoch [14/15], Loss: 2.0873, Validation Macro F1: 0.2791, Time: 18.13s\n",
      "Config: fc, LR: 0.0001, Epoch [15/15], Loss: 2.0491, Validation Macro F1: 0.2926, Time: 18.04s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/20], Loss: 2.3295, Validation Macro F1: 0.1330, Time: 18.20s\n",
      "Config: fc, LR: 0.0001, Epoch [2/20], Loss: 2.3235, Validation Macro F1: 0.1424, Time: 18.22s\n",
      "Config: fc, LR: 0.0001, Epoch [3/20], Loss: 2.2893, Validation Macro F1: 0.1655, Time: 18.30s\n",
      "Config: fc, LR: 0.0001, Epoch [4/20], Loss: 2.2731, Validation Macro F1: 0.1664, Time: 18.06s\n",
      "Config: fc, LR: 0.0001, Epoch [5/20], Loss: 2.2541, Validation Macro F1: 0.1752, Time: 18.08s\n",
      "Config: fc, LR: 0.0001, Epoch [6/20], Loss: 2.2224, Validation Macro F1: 0.1963, Time: 18.26s\n",
      "Config: fc, LR: 0.0001, Epoch [7/20], Loss: 2.1938, Validation Macro F1: 0.2161, Time: 18.04s\n",
      "Config: fc, LR: 0.0001, Epoch [8/20], Loss: 2.1932, Validation Macro F1: 0.2269, Time: 18.28s\n",
      "Config: fc, LR: 0.0001, Epoch [9/20], Loss: 2.1386, Validation Macro F1: 0.2424, Time: 18.11s\n",
      "Config: fc, LR: 0.0001, Epoch [10/20], Loss: 2.1235, Validation Macro F1: 0.2565, Time: 18.17s\n",
      "Config: fc, LR: 0.0001, Epoch [11/20], Loss: 2.1213, Validation Macro F1: 0.2733, Time: 18.05s\n",
      "Config: fc, LR: 0.0001, Epoch [12/20], Loss: 2.0787, Validation Macro F1: 0.2825, Time: 18.14s\n",
      "Config: fc, LR: 0.0001, Epoch [13/20], Loss: 2.0614, Validation Macro F1: 0.3071, Time: 17.97s\n",
      "Config: fc, LR: 0.0001, Epoch [14/20], Loss: 2.0519, Validation Macro F1: 0.3119, Time: 18.04s\n",
      "Config: fc, LR: 0.0001, Epoch [15/20], Loss: 2.0446, Validation Macro F1: 0.3266, Time: 18.14s\n",
      "Config: fc, LR: 0.0001, Epoch [16/20], Loss: 2.0154, Validation Macro F1: 0.3335, Time: 18.21s\n",
      "Config: fc, LR: 0.0001, Epoch [17/20], Loss: 1.9924, Validation Macro F1: 0.3516, Time: 18.09s\n",
      "Config: fc, LR: 0.0001, Epoch [18/20], Loss: 1.9607, Validation Macro F1: 0.3604, Time: 18.01s\n",
      "Config: fc, LR: 0.0001, Epoch [19/20], Loss: 1.9539, Validation Macro F1: 0.3738, Time: 18.10s\n",
      "Config: fc, LR: 0.0001, Epoch [20/20], Loss: 1.9399, Validation Macro F1: 0.3831, Time: 18.09s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/10], Loss: 2.3118, Validation Macro F1: 0.2466, Time: 18.09s\n",
      "Config: fc, LR: 0.001, Epoch [2/10], Loss: 2.0599, Validation Macro F1: 0.3881, Time: 18.07s\n",
      "Config: fc, LR: 0.001, Epoch [3/10], Loss: 1.8416, Validation Macro F1: 0.4932, Time: 18.06s\n",
      "Config: fc, LR: 0.001, Epoch [4/10], Loss: 1.6946, Validation Macro F1: 0.5542, Time: 18.16s\n",
      "Config: fc, LR: 0.001, Epoch [5/10], Loss: 1.5411, Validation Macro F1: 0.5844, Time: 18.15s\n",
      "Config: fc, LR: 0.001, Epoch [6/10], Loss: 1.3975, Validation Macro F1: 0.5955, Time: 18.07s\n",
      "Config: fc, LR: 0.001, Epoch [7/10], Loss: 1.2905, Validation Macro F1: 0.6141, Time: 18.04s\n",
      "Config: fc, LR: 0.001, Epoch [8/10], Loss: 1.2111, Validation Macro F1: 0.6205, Time: 18.14s\n",
      "Config: fc, LR: 0.001, Epoch [9/10], Loss: 1.1243, Validation Macro F1: 0.6272, Time: 18.16s\n",
      "Config: fc, LR: 0.001, Epoch [10/10], Loss: 1.0405, Validation Macro F1: 0.6249, Time: 18.04s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/15], Loss: 2.3418, Validation Macro F1: 0.1987, Time: 18.05s\n",
      "Config: fc, LR: 0.001, Epoch [2/15], Loss: 2.0893, Validation Macro F1: 0.3743, Time: 18.27s\n",
      "Config: fc, LR: 0.001, Epoch [3/15], Loss: 1.8654, Validation Macro F1: 0.4687, Time: 18.15s\n",
      "Config: fc, LR: 0.001, Epoch [4/15], Loss: 1.7329, Validation Macro F1: 0.5328, Time: 18.08s\n",
      "Config: fc, LR: 0.001, Epoch [5/15], Loss: 1.5329, Validation Macro F1: 0.5718, Time: 18.05s\n",
      "Config: fc, LR: 0.001, Epoch [6/15], Loss: 1.4069, Validation Macro F1: 0.5899, Time: 18.15s\n",
      "Config: fc, LR: 0.001, Epoch [7/15], Loss: 1.3131, Validation Macro F1: 0.6027, Time: 18.08s\n",
      "Config: fc, LR: 0.001, Epoch [8/15], Loss: 1.2106, Validation Macro F1: 0.6184, Time: 18.10s\n",
      "Config: fc, LR: 0.001, Epoch [9/15], Loss: 1.1232, Validation Macro F1: 0.6170, Time: 18.05s\n",
      "Config: fc, LR: 0.001, Epoch [10/15], Loss: 1.0610, Validation Macro F1: 0.6316, Time: 18.12s\n",
      "Config: fc, LR: 0.001, Epoch [11/15], Loss: 0.9850, Validation Macro F1: 0.6264, Time: 18.02s\n",
      "Config: fc, LR: 0.001, Epoch [12/15], Loss: 0.9327, Validation Macro F1: 0.6411, Time: 18.08s\n",
      "Config: fc, LR: 0.001, Epoch [13/15], Loss: 0.8963, Validation Macro F1: 0.6263, Time: 18.06s\n",
      "Config: fc, LR: 0.001, Epoch [14/15], Loss: 0.8821, Validation Macro F1: 0.6348, Time: 18.12s\n",
      "Config: fc, LR: 0.001, Epoch [15/15], Loss: 0.7995, Validation Macro F1: 0.6443, Time: 18.05s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/20], Loss: 2.2804, Validation Macro F1: 0.2382, Time: 18.19s\n",
      "Config: fc, LR: 0.001, Epoch [2/20], Loss: 2.0437, Validation Macro F1: 0.3687, Time: 18.09s\n",
      "Config: fc, LR: 0.001, Epoch [3/20], Loss: 1.8481, Validation Macro F1: 0.4825, Time: 18.02s\n",
      "Config: fc, LR: 0.001, Epoch [4/20], Loss: 1.6865, Validation Macro F1: 0.5337, Time: 18.00s\n",
      "Config: fc, LR: 0.001, Epoch [5/20], Loss: 1.5148, Validation Macro F1: 0.5786, Time: 18.23s\n",
      "Config: fc, LR: 0.001, Epoch [6/20], Loss: 1.3902, Validation Macro F1: 0.5916, Time: 18.15s\n",
      "Config: fc, LR: 0.001, Epoch [7/20], Loss: 1.3195, Validation Macro F1: 0.6075, Time: 18.05s\n",
      "Config: fc, LR: 0.001, Epoch [8/20], Loss: 1.2079, Validation Macro F1: 0.6190, Time: 18.08s\n",
      "Config: fc, LR: 0.001, Epoch [9/20], Loss: 1.1376, Validation Macro F1: 0.6244, Time: 18.07s\n",
      "Config: fc, LR: 0.001, Epoch [10/20], Loss: 1.0601, Validation Macro F1: 0.6263, Time: 17.97s\n",
      "Config: fc, LR: 0.001, Epoch [11/20], Loss: 0.9812, Validation Macro F1: 0.6273, Time: 18.07s\n",
      "Config: fc, LR: 0.001, Epoch [12/20], Loss: 0.9488, Validation Macro F1: 0.6230, Time: 18.14s\n",
      "Config: fc, LR: 0.001, Epoch [13/20], Loss: 0.8981, Validation Macro F1: 0.6355, Time: 18.14s\n",
      "Config: fc, LR: 0.001, Epoch [14/20], Loss: 0.8524, Validation Macro F1: 0.6380, Time: 18.07s\n",
      "Config: fc, LR: 0.001, Epoch [15/20], Loss: 0.8282, Validation Macro F1: 0.6495, Time: 18.11s\n",
      "Config: fc, LR: 0.001, Epoch [16/20], Loss: 0.7770, Validation Macro F1: 0.6475, Time: 18.27s\n",
      "Config: fc, LR: 0.001, Epoch [17/20], Loss: 0.7515, Validation Macro F1: 0.6444, Time: 18.08s\n",
      "Config: fc, LR: 0.001, Epoch [18/20], Loss: 0.6808, Validation Macro F1: 0.6476, Time: 18.02s\n",
      "Config: fc, LR: 0.001, Epoch [19/20], Loss: 0.7013, Validation Macro F1: 0.6464, Time: 18.12s\n",
      "Config: fc, LR: 0.001, Epoch [20/20], Loss: 0.6821, Validation Macro F1: 0.6467, Time: 18.05s\n",
      "Early stopping at epoch 20\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/10], Loss: 1.9399, Validation Macro F1: 0.6340, Time: 18.11s\n",
      "Config: fc, LR: 0.01, Epoch [2/10], Loss: 0.9691, Validation Macro F1: 0.6762, Time: 18.11s\n",
      "Config: fc, LR: 0.01, Epoch [3/10], Loss: 0.6549, Validation Macro F1: 0.6629, Time: 18.03s\n",
      "Config: fc, LR: 0.01, Epoch [4/10], Loss: 0.5707, Validation Macro F1: 0.6622, Time: 18.12s\n",
      "Config: fc, LR: 0.01, Epoch [5/10], Loss: 0.4903, Validation Macro F1: 0.6721, Time: 18.28s\n",
      "Config: fc, LR: 0.01, Epoch [6/10], Loss: 0.4117, Validation Macro F1: 0.6629, Time: 18.16s\n",
      "Config: fc, LR: 0.01, Epoch [7/10], Loss: 0.4203, Validation Macro F1: 0.6486, Time: 18.01s\n",
      "Early stopping at epoch 7\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/15], Loss: 1.9571, Validation Macro F1: 0.5918, Time: 18.14s\n",
      "Config: fc, LR: 0.01, Epoch [2/15], Loss: 0.9296, Validation Macro F1: 0.6804, Time: 18.34s\n",
      "Config: fc, LR: 0.01, Epoch [3/15], Loss: 0.6484, Validation Macro F1: 0.6743, Time: 18.14s\n",
      "Config: fc, LR: 0.01, Epoch [4/15], Loss: 0.5383, Validation Macro F1: 0.6808, Time: 18.21s\n",
      "Config: fc, LR: 0.01, Epoch [5/15], Loss: 0.4518, Validation Macro F1: 0.6669, Time: 18.01s\n",
      "Config: fc, LR: 0.01, Epoch [6/15], Loss: 0.4090, Validation Macro F1: 0.6601, Time: 18.34s\n",
      "Config: fc, LR: 0.01, Epoch [7/15], Loss: 0.3252, Validation Macro F1: 0.6478, Time: 18.10s\n",
      "Config: fc, LR: 0.01, Epoch [8/15], Loss: 0.3268, Validation Macro F1: 0.6492, Time: 18.10s\n",
      "Config: fc, LR: 0.01, Epoch [9/15], Loss: 0.3235, Validation Macro F1: 0.6196, Time: 18.02s\n",
      "Early stopping at epoch 9\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/20], Loss: 1.8973, Validation Macro F1: 0.6337, Time: 18.07s\n",
      "Config: fc, LR: 0.01, Epoch [2/20], Loss: 0.9811, Validation Macro F1: 0.6831, Time: 18.07s\n",
      "Config: fc, LR: 0.01, Epoch [3/20], Loss: 0.6767, Validation Macro F1: 0.6747, Time: 18.03s\n",
      "Config: fc, LR: 0.01, Epoch [4/20], Loss: 0.5972, Validation Macro F1: 0.6833, Time: 18.07s\n",
      "Config: fc, LR: 0.01, Epoch [5/20], Loss: 0.4625, Validation Macro F1: 0.6755, Time: 18.04s\n",
      "Config: fc, LR: 0.01, Epoch [6/20], Loss: 0.4192, Validation Macro F1: 0.6610, Time: 18.04s\n",
      "Config: fc, LR: 0.01, Epoch [7/20], Loss: 0.3860, Validation Macro F1: 0.6575, Time: 18.06s\n",
      "Config: fc, LR: 0.01, Epoch [8/20], Loss: 0.3739, Validation Macro F1: 0.6571, Time: 18.20s\n",
      "Config: fc, LR: 0.01, Epoch [9/20], Loss: 0.2624, Validation Macro F1: 0.6422, Time: 18.16s\n",
      "Early stopping at epoch 9\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 1e-05, Epoch [1/10], Loss: 2.3259, Validation Macro F1: 0.1260, Time: 18.18s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [2/10], Loss: 2.3282, Validation Macro F1: 0.1148, Time: 18.14s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [3/10], Loss: 2.3215, Validation Macro F1: 0.1164, Time: 18.05s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [4/10], Loss: 2.3200, Validation Macro F1: 0.1226, Time: 18.03s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [5/10], Loss: 2.3163, Validation Macro F1: 0.1281, Time: 18.13s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [6/10], Loss: 2.3074, Validation Macro F1: 0.1302, Time: 18.29s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [7/10], Loss: 2.2985, Validation Macro F1: 0.1286, Time: 18.45s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [8/10], Loss: 2.3179, Validation Macro F1: 0.1250, Time: 18.55s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [9/10], Loss: 2.2833, Validation Macro F1: 0.1388, Time: 17.98s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [10/10], Loss: 2.2898, Validation Macro F1: 0.1373, Time: 17.71s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 1e-05, Epoch [1/15], Loss: 2.3687, Validation Macro F1: 0.0897, Time: 17.66s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [2/15], Loss: 2.3351, Validation Macro F1: 0.0961, Time: 18.19s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [3/15], Loss: 2.3475, Validation Macro F1: 0.1060, Time: 18.02s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [4/15], Loss: 2.3317, Validation Macro F1: 0.1068, Time: 18.36s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [5/15], Loss: 2.3344, Validation Macro F1: 0.1113, Time: 17.81s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [6/15], Loss: 2.3477, Validation Macro F1: 0.1149, Time: 17.63s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [7/15], Loss: 2.3301, Validation Macro F1: 0.1174, Time: 18.08s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [8/15], Loss: 2.3183, Validation Macro F1: 0.1271, Time: 18.14s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [9/15], Loss: 2.3164, Validation Macro F1: 0.1244, Time: 18.06s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [10/15], Loss: 2.3049, Validation Macro F1: 0.1285, Time: 18.34s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [11/15], Loss: 2.3087, Validation Macro F1: 0.1298, Time: 18.12s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [12/15], Loss: 2.3063, Validation Macro F1: 0.1271, Time: 18.11s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [13/15], Loss: 2.2941, Validation Macro F1: 0.1360, Time: 18.35s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [14/15], Loss: 2.3069, Validation Macro F1: 0.1365, Time: 18.41s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [15/15], Loss: 2.3093, Validation Macro F1: 0.1358, Time: 17.86s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 1e-05, Epoch [1/20], Loss: 2.3619, Validation Macro F1: 0.1157, Time: 17.98s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [2/20], Loss: 2.3193, Validation Macro F1: 0.1223, Time: 18.48s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [3/20], Loss: 2.3219, Validation Macro F1: 0.1197, Time: 18.10s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [4/20], Loss: 2.3144, Validation Macro F1: 0.1188, Time: 18.03s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [5/20], Loss: 2.3153, Validation Macro F1: 0.1282, Time: 18.49s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [6/20], Loss: 2.3225, Validation Macro F1: 0.1185, Time: 17.80s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [7/20], Loss: 2.3013, Validation Macro F1: 0.1272, Time: 17.54s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [8/20], Loss: 2.3169, Validation Macro F1: 0.1214, Time: 17.56s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [9/20], Loss: 2.2891, Validation Macro F1: 0.1219, Time: 17.51s\n",
      "Config: fc+features8, LR: 1e-05, Epoch [10/20], Loss: 2.3108, Validation Macro F1: 0.1229, Time: 17.66s\n",
      "Early stopping at epoch 10\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.0001, Epoch [1/10], Loss: 2.3649, Validation Macro F1: 0.0984, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [2/10], Loss: 2.3086, Validation Macro F1: 0.1411, Time: 17.97s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [3/10], Loss: 2.2599, Validation Macro F1: 0.1695, Time: 17.90s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [4/10], Loss: 2.1997, Validation Macro F1: 0.2006, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [5/10], Loss: 2.1587, Validation Macro F1: 0.2412, Time: 18.70s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [6/10], Loss: 2.1283, Validation Macro F1: 0.2704, Time: 17.93s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [7/10], Loss: 2.0409, Validation Macro F1: 0.3055, Time: 17.68s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [8/10], Loss: 1.9949, Validation Macro F1: 0.3377, Time: 17.47s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [9/10], Loss: 1.9802, Validation Macro F1: 0.3635, Time: 18.65s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [10/10], Loss: 1.9290, Validation Macro F1: 0.4021, Time: 18.40s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.0001, Epoch [1/15], Loss: 2.3192, Validation Macro F1: 0.1109, Time: 18.25s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [2/15], Loss: 2.2690, Validation Macro F1: 0.1425, Time: 17.91s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [3/15], Loss: 2.2259, Validation Macro F1: 0.1920, Time: 18.04s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [4/15], Loss: 2.1820, Validation Macro F1: 0.2156, Time: 18.17s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [5/15], Loss: 2.1276, Validation Macro F1: 0.2533, Time: 18.13s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [6/15], Loss: 2.0886, Validation Macro F1: 0.2784, Time: 18.16s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [7/15], Loss: 2.0017, Validation Macro F1: 0.3191, Time: 17.94s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [8/15], Loss: 1.9836, Validation Macro F1: 0.3538, Time: 18.06s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [9/15], Loss: 1.9484, Validation Macro F1: 0.3764, Time: 18.02s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [10/15], Loss: 1.8856, Validation Macro F1: 0.4070, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [11/15], Loss: 1.8609, Validation Macro F1: 0.4235, Time: 18.09s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [12/15], Loss: 1.8096, Validation Macro F1: 0.4435, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [13/15], Loss: 1.7628, Validation Macro F1: 0.4654, Time: 18.06s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [14/15], Loss: 1.7290, Validation Macro F1: 0.4696, Time: 17.91s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [15/15], Loss: 1.7082, Validation Macro F1: 0.4950, Time: 18.13s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.0001, Epoch [1/20], Loss: 2.3084, Validation Macro F1: 0.1913, Time: 18.05s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [2/20], Loss: 2.2396, Validation Macro F1: 0.2123, Time: 18.14s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [3/20], Loss: 2.1936, Validation Macro F1: 0.2350, Time: 18.09s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [4/20], Loss: 2.1460, Validation Macro F1: 0.2648, Time: 18.17s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [5/20], Loss: 2.0920, Validation Macro F1: 0.2929, Time: 18.08s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [6/20], Loss: 2.0614, Validation Macro F1: 0.3205, Time: 18.14s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [7/20], Loss: 2.0072, Validation Macro F1: 0.3583, Time: 18.04s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [8/20], Loss: 1.9450, Validation Macro F1: 0.3831, Time: 18.11s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [9/20], Loss: 1.9035, Validation Macro F1: 0.3996, Time: 17.97s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [10/20], Loss: 1.8596, Validation Macro F1: 0.4218, Time: 18.19s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [11/20], Loss: 1.8192, Validation Macro F1: 0.4406, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [12/20], Loss: 1.7700, Validation Macro F1: 0.4480, Time: 18.07s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [13/20], Loss: 1.7483, Validation Macro F1: 0.4661, Time: 18.13s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [14/20], Loss: 1.7176, Validation Macro F1: 0.4796, Time: 18.00s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [15/20], Loss: 1.6526, Validation Macro F1: 0.4982, Time: 18.23s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [16/20], Loss: 1.6268, Validation Macro F1: 0.5083, Time: 18.07s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [17/20], Loss: 1.6271, Validation Macro F1: 0.5221, Time: 18.14s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [18/20], Loss: 1.5627, Validation Macro F1: 0.5285, Time: 18.06s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [19/20], Loss: 1.5143, Validation Macro F1: 0.5339, Time: 18.05s\n",
      "Config: fc+features8, LR: 0.0001, Epoch [20/20], Loss: 1.5023, Validation Macro F1: 0.5502, Time: 17.99s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.001, Epoch [1/10], Loss: 2.1781, Validation Macro F1: 0.4522, Time: 18.04s\n",
      "Config: fc+features8, LR: 0.001, Epoch [2/10], Loss: 1.7215, Validation Macro F1: 0.6090, Time: 17.95s\n",
      "Config: fc+features8, LR: 0.001, Epoch [3/10], Loss: 1.3490, Validation Macro F1: 0.6677, Time: 18.02s\n",
      "Config: fc+features8, LR: 0.001, Epoch [4/10], Loss: 1.0745, Validation Macro F1: 0.6878, Time: 18.02s\n",
      "Config: fc+features8, LR: 0.001, Epoch [5/10], Loss: 0.8926, Validation Macro F1: 0.6857, Time: 18.06s\n",
      "Config: fc+features8, LR: 0.001, Epoch [6/10], Loss: 0.7184, Validation Macro F1: 0.6892, Time: 18.05s\n",
      "Config: fc+features8, LR: 0.001, Epoch [7/10], Loss: 0.6160, Validation Macro F1: 0.6988, Time: 18.37s\n",
      "Config: fc+features8, LR: 0.001, Epoch [8/10], Loss: 0.5117, Validation Macro F1: 0.6984, Time: 18.28s\n",
      "Config: fc+features8, LR: 0.001, Epoch [9/10], Loss: 0.4317, Validation Macro F1: 0.7000, Time: 17.97s\n",
      "Config: fc+features8, LR: 0.001, Epoch [10/10], Loss: 0.4122, Validation Macro F1: 0.6910, Time: 18.06s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.001, Epoch [1/15], Loss: 2.2328, Validation Macro F1: 0.4146, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.001, Epoch [2/15], Loss: 1.7306, Validation Macro F1: 0.5832, Time: 18.10s\n",
      "Config: fc+features8, LR: 0.001, Epoch [3/15], Loss: 1.3651, Validation Macro F1: 0.6423, Time: 18.03s\n",
      "Config: fc+features8, LR: 0.001, Epoch [4/15], Loss: 1.0849, Validation Macro F1: 0.6811, Time: 17.96s\n",
      "Config: fc+features8, LR: 0.001, Epoch [5/15], Loss: 0.8975, Validation Macro F1: 0.6783, Time: 18.04s\n",
      "Config: fc+features8, LR: 0.001, Epoch [6/15], Loss: 0.7349, Validation Macro F1: 0.6847, Time: 18.10s\n",
      "Config: fc+features8, LR: 0.001, Epoch [7/15], Loss: 0.6068, Validation Macro F1: 0.6874, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.001, Epoch [8/15], Loss: 0.5397, Validation Macro F1: 0.6877, Time: 17.98s\n",
      "Config: fc+features8, LR: 0.001, Epoch [9/15], Loss: 0.4388, Validation Macro F1: 0.6849, Time: 17.88s\n",
      "Config: fc+features8, LR: 0.001, Epoch [10/15], Loss: 0.4201, Validation Macro F1: 0.6789, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.001, Epoch [11/15], Loss: 0.3578, Validation Macro F1: 0.6828, Time: 18.09s\n",
      "Config: fc+features8, LR: 0.001, Epoch [12/15], Loss: 0.3085, Validation Macro F1: 0.6839, Time: 17.93s\n",
      "Config: fc+features8, LR: 0.001, Epoch [13/15], Loss: 0.3118, Validation Macro F1: 0.6879, Time: 18.08s\n",
      "Config: fc+features8, LR: 0.001, Epoch [14/15], Loss: 0.2473, Validation Macro F1: 0.6836, Time: 18.07s\n",
      "Config: fc+features8, LR: 0.001, Epoch [15/15], Loss: 0.2288, Validation Macro F1: 0.6812, Time: 18.00s\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.001, Epoch [1/20], Loss: 2.2635, Validation Macro F1: 0.4230, Time: 18.04s\n",
      "Config: fc+features8, LR: 0.001, Epoch [2/20], Loss: 1.7570, Validation Macro F1: 0.5944, Time: 17.97s\n",
      "Config: fc+features8, LR: 0.001, Epoch [3/20], Loss: 1.3974, Validation Macro F1: 0.6518, Time: 17.92s\n",
      "Config: fc+features8, LR: 0.001, Epoch [4/20], Loss: 1.0967, Validation Macro F1: 0.6758, Time: 18.17s\n",
      "Config: fc+features8, LR: 0.001, Epoch [5/20], Loss: 0.9093, Validation Macro F1: 0.6900, Time: 17.98s\n",
      "Config: fc+features8, LR: 0.001, Epoch [6/20], Loss: 0.7398, Validation Macro F1: 0.6987, Time: 18.02s\n",
      "Config: fc+features8, LR: 0.001, Epoch [7/20], Loss: 0.6124, Validation Macro F1: 0.6975, Time: 18.07s\n",
      "Config: fc+features8, LR: 0.001, Epoch [8/20], Loss: 0.5537, Validation Macro F1: 0.6956, Time: 18.05s\n",
      "Config: fc+features8, LR: 0.001, Epoch [9/20], Loss: 0.4527, Validation Macro F1: 0.6919, Time: 18.09s\n",
      "Config: fc+features8, LR: 0.001, Epoch [10/20], Loss: 0.4217, Validation Macro F1: 0.6836, Time: 18.11s\n",
      "Config: fc+features8, LR: 0.001, Epoch [11/20], Loss: 0.3371, Validation Macro F1: 0.6848, Time: 17.98s\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.01, Epoch [1/10], Loss: 1.6688, Validation Macro F1: 0.7327, Time: 18.10s\n",
      "Config: fc+features8, LR: 0.01, Epoch [2/10], Loss: 0.4861, Validation Macro F1: 0.7208, Time: 18.06s\n",
      "Config: fc+features8, LR: 0.01, Epoch [3/10], Loss: 0.3345, Validation Macro F1: 0.6839, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.01, Epoch [4/10], Loss: 0.2437, Validation Macro F1: 0.6649, Time: 18.11s\n",
      "Config: fc+features8, LR: 0.01, Epoch [5/10], Loss: 0.2251, Validation Macro F1: 0.6655, Time: 17.97s\n",
      "Config: fc+features8, LR: 0.01, Epoch [6/10], Loss: 0.1200, Validation Macro F1: 0.6665, Time: 18.05s\n",
      "Early stopping at epoch 6\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.01, Epoch [1/15], Loss: 1.6522, Validation Macro F1: 0.7067, Time: 17.85s\n",
      "Config: fc+features8, LR: 0.01, Epoch [2/15], Loss: 0.5020, Validation Macro F1: 0.7386, Time: 17.96s\n",
      "Config: fc+features8, LR: 0.01, Epoch [3/15], Loss: 0.2359, Validation Macro F1: 0.7212, Time: 17.86s\n",
      "Config: fc+features8, LR: 0.01, Epoch [4/15], Loss: 0.3108, Validation Macro F1: 0.6929, Time: 17.96s\n",
      "Config: fc+features8, LR: 0.01, Epoch [5/15], Loss: 0.1629, Validation Macro F1: 0.6635, Time: 18.01s\n",
      "Config: fc+features8, LR: 0.01, Epoch [6/15], Loss: 0.1469, Validation Macro F1: 0.6524, Time: 18.12s\n",
      "Config: fc+features8, LR: 0.01, Epoch [7/15], Loss: 0.1990, Validation Macro F1: 0.6580, Time: 18.07s\n",
      "Early stopping at epoch 7\n",
      "\n",
      "Configuration: fc+features8, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features8, LR: 0.01, Epoch [1/20], Loss: 1.7429, Validation Macro F1: 0.6991, Time: 18.14s\n",
      "Config: fc+features8, LR: 0.01, Epoch [2/20], Loss: 0.4839, Validation Macro F1: 0.7296, Time: 18.11s\n",
      "Config: fc+features8, LR: 0.01, Epoch [3/20], Loss: 0.2647, Validation Macro F1: 0.7151, Time: 18.06s\n",
      "Config: fc+features8, LR: 0.01, Epoch [4/20], Loss: 0.2027, Validation Macro F1: 0.6919, Time: 17.91s\n",
      "Config: fc+features8, LR: 0.01, Epoch [5/20], Loss: 0.1795, Validation Macro F1: 0.6767, Time: 17.96s\n",
      "Config: fc+features8, LR: 0.01, Epoch [6/20], Loss: 0.1611, Validation Macro F1: 0.6719, Time: 18.08s\n",
      "Config: fc+features8, LR: 0.01, Epoch [7/20], Loss: 0.1396, Validation Macro F1: 0.6417, Time: 18.05s\n",
      "Early stopping at epoch 7\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 1e-05, Epoch [1/10], Loss: 2.3734, Validation Macro F1: 0.0964, Time: 18.19s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [2/10], Loss: 2.3785, Validation Macro F1: 0.1057, Time: 18.07s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [3/10], Loss: 2.3659, Validation Macro F1: 0.1063, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [4/10], Loss: 2.3714, Validation Macro F1: 0.1058, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [5/10], Loss: 2.3374, Validation Macro F1: 0.1071, Time: 18.01s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [6/10], Loss: 2.3456, Validation Macro F1: 0.1155, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [7/10], Loss: 2.3329, Validation Macro F1: 0.1151, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [8/10], Loss: 2.3415, Validation Macro F1: 0.1227, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [9/10], Loss: 2.3030, Validation Macro F1: 0.1265, Time: 18.21s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [10/10], Loss: 2.3059, Validation Macro F1: 0.1264, Time: 18.22s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 1e-05, Epoch [1/15], Loss: 2.3654, Validation Macro F1: 0.1086, Time: 17.90s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [2/15], Loss: 2.3496, Validation Macro F1: 0.1196, Time: 18.04s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [3/15], Loss: 2.3568, Validation Macro F1: 0.1251, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [4/15], Loss: 2.3104, Validation Macro F1: 0.1251, Time: 17.98s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [5/15], Loss: 2.3176, Validation Macro F1: 0.1293, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [6/15], Loss: 2.3060, Validation Macro F1: 0.1410, Time: 17.90s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [7/15], Loss: 2.3150, Validation Macro F1: 0.1314, Time: 18.08s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [8/15], Loss: 2.2939, Validation Macro F1: 0.1397, Time: 18.11s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [9/15], Loss: 2.2857, Validation Macro F1: 0.1418, Time: 17.97s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [10/15], Loss: 2.2752, Validation Macro F1: 0.1494, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [11/15], Loss: 2.2675, Validation Macro F1: 0.1507, Time: 18.20s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [12/15], Loss: 2.2672, Validation Macro F1: 0.1497, Time: 18.02s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [13/15], Loss: 2.2524, Validation Macro F1: 0.1651, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [14/15], Loss: 2.2398, Validation Macro F1: 0.1624, Time: 17.91s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [15/15], Loss: 2.2325, Validation Macro F1: 0.1771, Time: 18.18s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 1e-05, Epoch [1/20], Loss: 2.3958, Validation Macro F1: 0.0921, Time: 17.97s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [2/20], Loss: 2.3973, Validation Macro F1: 0.1016, Time: 17.93s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [3/20], Loss: 2.3445, Validation Macro F1: 0.1079, Time: 18.04s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [4/20], Loss: 2.3592, Validation Macro F1: 0.0966, Time: 18.13s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [5/20], Loss: 2.3365, Validation Macro F1: 0.1097, Time: 18.04s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [6/20], Loss: 2.3478, Validation Macro F1: 0.1165, Time: 17.94s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [7/20], Loss: 2.3231, Validation Macro F1: 0.1187, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [8/20], Loss: 2.3278, Validation Macro F1: 0.1227, Time: 17.92s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [9/20], Loss: 2.3082, Validation Macro F1: 0.1209, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [10/20], Loss: 2.2932, Validation Macro F1: 0.1268, Time: 18.08s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [11/20], Loss: 2.2824, Validation Macro F1: 0.1386, Time: 18.10s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [12/20], Loss: 2.2761, Validation Macro F1: 0.1333, Time: 18.10s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [13/20], Loss: 2.2720, Validation Macro F1: 0.1361, Time: 17.99s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [14/20], Loss: 2.2746, Validation Macro F1: 0.1446, Time: 18.10s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [15/20], Loss: 2.2551, Validation Macro F1: 0.1490, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [16/20], Loss: 2.2471, Validation Macro F1: 0.1609, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [17/20], Loss: 2.2199, Validation Macro F1: 0.1692, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [18/20], Loss: 2.2469, Validation Macro F1: 0.1607, Time: 17.91s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [19/20], Loss: 2.2212, Validation Macro F1: 0.1784, Time: 18.22s\n",
      "Config: fc+features7+features8, LR: 1e-05, Epoch [20/20], Loss: 2.1993, Validation Macro F1: 0.1749, Time: 17.97s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.0001, Epoch [1/10], Loss: 2.3228, Validation Macro F1: 0.1524, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [2/10], Loss: 2.2157, Validation Macro F1: 0.2130, Time: 18.13s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [3/10], Loss: 2.1413, Validation Macro F1: 0.2710, Time: 17.98s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [4/10], Loss: 2.0520, Validation Macro F1: 0.3327, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [5/10], Loss: 1.9604, Validation Macro F1: 0.3891, Time: 18.21s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [6/10], Loss: 1.8989, Validation Macro F1: 0.4269, Time: 17.98s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [7/10], Loss: 1.8263, Validation Macro F1: 0.4697, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [8/10], Loss: 1.7278, Validation Macro F1: 0.4908, Time: 17.96s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [9/10], Loss: 1.6431, Validation Macro F1: 0.5234, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [10/10], Loss: 1.5671, Validation Macro F1: 0.5393, Time: 18.06s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.0001, Epoch [1/15], Loss: 2.3320, Validation Macro F1: 0.1562, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [2/15], Loss: 2.2402, Validation Macro F1: 0.2255, Time: 18.11s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [3/15], Loss: 2.1408, Validation Macro F1: 0.2819, Time: 17.96s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [4/15], Loss: 2.0584, Validation Macro F1: 0.3464, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [5/15], Loss: 1.9788, Validation Macro F1: 0.3934, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [6/15], Loss: 1.8763, Validation Macro F1: 0.4387, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [7/15], Loss: 1.8241, Validation Macro F1: 0.4617, Time: 18.14s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [8/15], Loss: 1.7193, Validation Macro F1: 0.4900, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [9/15], Loss: 1.6563, Validation Macro F1: 0.5133, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [10/15], Loss: 1.5852, Validation Macro F1: 0.5354, Time: 18.18s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [11/15], Loss: 1.5229, Validation Macro F1: 0.5520, Time: 18.27s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [12/15], Loss: 1.4388, Validation Macro F1: 0.5639, Time: 18.01s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [13/15], Loss: 1.3795, Validation Macro F1: 0.5772, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [14/15], Loss: 1.2998, Validation Macro F1: 0.5897, Time: 18.15s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [15/15], Loss: 1.2495, Validation Macro F1: 0.6028, Time: 18.03s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.0001, Epoch [1/20], Loss: 2.3379, Validation Macro F1: 0.1801, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [2/20], Loss: 2.2274, Validation Macro F1: 0.2402, Time: 18.16s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [3/20], Loss: 2.1144, Validation Macro F1: 0.3122, Time: 18.17s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [4/20], Loss: 2.0596, Validation Macro F1: 0.3532, Time: 18.07s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [5/20], Loss: 1.9691, Validation Macro F1: 0.4096, Time: 18.11s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [6/20], Loss: 1.8827, Validation Macro F1: 0.4527, Time: 18.06s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [7/20], Loss: 1.8089, Validation Macro F1: 0.4779, Time: 18.08s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [8/20], Loss: 1.7435, Validation Macro F1: 0.4953, Time: 18.17s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [9/20], Loss: 1.6372, Validation Macro F1: 0.5299, Time: 18.01s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [10/20], Loss: 1.5722, Validation Macro F1: 0.5378, Time: 18.10s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [11/20], Loss: 1.4807, Validation Macro F1: 0.5615, Time: 18.18s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [12/20], Loss: 1.4321, Validation Macro F1: 0.5784, Time: 18.10s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [13/20], Loss: 1.3589, Validation Macro F1: 0.5901, Time: 18.19s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [14/20], Loss: 1.2869, Validation Macro F1: 0.5979, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [15/20], Loss: 1.2368, Validation Macro F1: 0.5981, Time: 18.06s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [16/20], Loss: 1.1792, Validation Macro F1: 0.6127, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [17/20], Loss: 1.1058, Validation Macro F1: 0.6220, Time: 18.07s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [18/20], Loss: 1.0694, Validation Macro F1: 0.6320, Time: 18.05s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [19/20], Loss: 1.0133, Validation Macro F1: 0.6440, Time: 18.04s\n",
      "Config: fc+features7+features8, LR: 0.0001, Epoch [20/20], Loss: 0.9528, Validation Macro F1: 0.6407, Time: 18.04s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.001, Epoch [1/10], Loss: 2.1734, Validation Macro F1: 0.5558, Time: 17.99s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [2/10], Loss: 1.3905, Validation Macro F1: 0.6879, Time: 18.08s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [3/10], Loss: 0.8892, Validation Macro F1: 0.7098, Time: 18.01s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [4/10], Loss: 0.6098, Validation Macro F1: 0.7278, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [5/10], Loss: 0.4308, Validation Macro F1: 0.7286, Time: 17.99s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [6/10], Loss: 0.2900, Validation Macro F1: 0.7326, Time: 18.06s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [7/10], Loss: 0.2189, Validation Macro F1: 0.7299, Time: 18.03s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [8/10], Loss: 0.1404, Validation Macro F1: 0.7337, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [9/10], Loss: 0.1440, Validation Macro F1: 0.7244, Time: 17.96s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [10/10], Loss: 0.0810, Validation Macro F1: 0.7200, Time: 18.04s\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.001, Epoch [1/15], Loss: 2.1330, Validation Macro F1: 0.5636, Time: 18.02s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [2/15], Loss: 1.3404, Validation Macro F1: 0.6795, Time: 18.02s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [3/15], Loss: 0.8834, Validation Macro F1: 0.7177, Time: 17.99s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [4/15], Loss: 0.5593, Validation Macro F1: 0.7311, Time: 18.04s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [5/15], Loss: 0.4060, Validation Macro F1: 0.7438, Time: 17.98s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [6/15], Loss: 0.2741, Validation Macro F1: 0.7314, Time: 17.95s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [7/15], Loss: 0.2221, Validation Macro F1: 0.7435, Time: 18.00s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [8/15], Loss: 0.1478, Validation Macro F1: 0.7346, Time: 18.09s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [9/15], Loss: 0.1089, Validation Macro F1: 0.7299, Time: 18.11s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [10/15], Loss: 0.1120, Validation Macro F1: 0.7204, Time: 18.23s\n",
      "Early stopping at epoch 10\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.001, Epoch [1/20], Loss: 2.1348, Validation Macro F1: 0.5518, Time: 18.18s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [2/20], Loss: 1.3757, Validation Macro F1: 0.6858, Time: 18.01s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [3/20], Loss: 0.8745, Validation Macro F1: 0.7266, Time: 18.66s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [4/20], Loss: 0.5954, Validation Macro F1: 0.7459, Time: 17.98s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [5/20], Loss: 0.4003, Validation Macro F1: 0.7400, Time: 17.66s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [6/20], Loss: 0.2822, Validation Macro F1: 0.7434, Time: 17.96s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [7/20], Loss: 0.1857, Validation Macro F1: 0.7320, Time: 17.59s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [8/20], Loss: 0.1454, Validation Macro F1: 0.7332, Time: 18.42s\n",
      "Config: fc+features7+features8, LR: 0.001, Epoch [9/20], Loss: 0.1039, Validation Macro F1: 0.7298, Time: 18.35s\n",
      "Early stopping at epoch 9\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.01, Epoch [1/10], Loss: 1.5721, Validation Macro F1: 0.6403, Time: 18.16s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [2/10], Loss: 0.6339, Validation Macro F1: 0.6442, Time: 18.33s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [3/10], Loss: 0.4206, Validation Macro F1: 0.6650, Time: 18.48s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [4/10], Loss: 0.2702, Validation Macro F1: 0.6127, Time: 18.19s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [5/10], Loss: 0.3071, Validation Macro F1: 0.6515, Time: 18.10s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [6/10], Loss: 0.2627, Validation Macro F1: 0.6511, Time: 18.27s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [7/10], Loss: 0.3489, Validation Macro F1: 0.6488, Time: 18.40s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [8/10], Loss: 0.1925, Validation Macro F1: 0.6394, Time: 17.99s\n",
      "Early stopping at epoch 8\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.01, Epoch [1/15], Loss: 1.6760, Validation Macro F1: 0.5896, Time: 18.24s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [2/15], Loss: 0.7068, Validation Macro F1: 0.6688, Time: 18.17s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [3/15], Loss: 0.5205, Validation Macro F1: 0.6195, Time: 18.38s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [4/15], Loss: 0.2438, Validation Macro F1: 0.6705, Time: 17.70s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [5/15], Loss: 0.2335, Validation Macro F1: 0.6698, Time: 17.96s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [6/15], Loss: 0.1808, Validation Macro F1: 0.6548, Time: 17.60s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [7/15], Loss: 0.1616, Validation Macro F1: 0.6549, Time: 17.59s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [8/15], Loss: 0.1977, Validation Macro F1: 0.6715, Time: 17.95s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [9/15], Loss: 0.1160, Validation Macro F1: 0.6442, Time: 18.70s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [10/15], Loss: 0.2091, Validation Macro F1: 0.6597, Time: 17.87s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [11/15], Loss: 0.1689, Validation Macro F1: 0.6581, Time: 17.79s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [12/15], Loss: 0.1650, Validation Macro F1: 0.6641, Time: 18.38s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [13/15], Loss: 0.1516, Validation Macro F1: 0.6667, Time: 18.47s\n",
      "Early stopping at epoch 13\n",
      "\n",
      "Configuration: fc+features7+features8, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+features7+features8, LR: 0.01, Epoch [1/20], Loss: 1.6623, Validation Macro F1: 0.5834, Time: 18.31s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [2/20], Loss: 0.6127, Validation Macro F1: 0.6122, Time: 18.61s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [3/20], Loss: 0.4724, Validation Macro F1: 0.6628, Time: 18.06s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [4/20], Loss: 0.3642, Validation Macro F1: 0.6462, Time: 18.33s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [5/20], Loss: 0.3088, Validation Macro F1: 0.6576, Time: 18.15s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [6/20], Loss: 0.2483, Validation Macro F1: 0.6498, Time: 18.34s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [7/20], Loss: 0.2530, Validation Macro F1: 0.6505, Time: 18.24s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [8/20], Loss: 0.2635, Validation Macro F1: 0.6698, Time: 18.26s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [9/20], Loss: 0.1845, Validation Macro F1: 0.6551, Time: 18.48s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [10/20], Loss: 0.1497, Validation Macro F1: 0.6541, Time: 17.85s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [11/20], Loss: 0.1259, Validation Macro F1: 0.6565, Time: 17.94s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [12/20], Loss: 0.1962, Validation Macro F1: 0.6599, Time: 18.07s\n",
      "Config: fc+features7+features8, LR: 0.01, Epoch [13/20], Loss: 0.0748, Validation Macro F1: 0.6585, Time: 18.07s\n",
      "Early stopping at epoch 13\n",
      "\n",
      "Best Configuration: fc+features7+features8, LR: 0.001, Num Epochs: 20 with Macro F1 Score: 0.7458727997571163\n"
     ]
    }
   ],
   "source": [
    "# Example usage with EfficientNetB0\n",
    "model_efficientnetb0 = deepcopy(model1)  # assuming model1 is a pretrained efficientnetb0\n",
    "model_efficientnetb0 = model_efficientnetb0.to(device)\n",
    "\n",
    "# Define learning rates and number of epochs for grid search\n",
    "efficientnet_learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "efficientnet_num_epochs_list = [10, 15, 20]\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Call the updated training function without the autoencoder\n",
    "train_model_with_grid_search(\n",
    "    model=model_efficientnetb0,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_classes=num_classes,\n",
    "    learning_rates=efficientnet_learning_rates,\n",
    "    num_epochs_list=efficientnet_num_epochs_list,\n",
    "    log_filename='efficientnetb0_training_log.csv',\n",
    "    model_name='efficientnet',\n",
    "    batch_size=batch_size,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\惟神君\\AppData\\Local\\Temp\\ipykernel_16392\\3387589344.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_efficientnet.load_state_dict(torch.load('logs/best_model_efficientnet.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the EfficientNetB0 model with pretrained weights\n",
    "best_model_efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Replace the final classifier layer to match the number of classes\n",
    "best_model_efficientnet.classifier[1] = nn.Linear(best_model_efficientnet.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load the best model weights onto the appropriate device\n",
    "best_model_efficientnet.load_state_dict(torch.load('logs/best_model_efficientnet.pth', map_location=device))\n",
    "\n",
    "# Move the model to the device (if not already on the correct device)\n",
    "best_model_efficientnet = best_model_efficientnet.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model_efficientnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.925%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(best_model_efficientnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.7584632465836025\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.82      0.69      0.75       800\n",
      "     Class 1       0.86      0.71      0.78       800\n",
      "     Class 2       0.83      0.92      0.88       800\n",
      "     Class 3       0.63      0.69      0.66       800\n",
      "     Class 4       0.68      0.77      0.72       800\n",
      "     Class 5       0.78      0.61      0.68       800\n",
      "     Class 6       0.70      0.85      0.77       800\n",
      "     Class 7       0.81      0.75      0.78       800\n",
      "     Class 8       0.81      0.83      0.82       800\n",
      "     Class 9       0.74      0.77      0.75       800\n",
      "\n",
      "    accuracy                           0.76      8000\n",
      "   macro avg       0.77      0.76      0.76      8000\n",
      "weighted avg       0.77      0.76      0.76      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(best_model_efficientnet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image size to 224x224 to match the input size of ViT\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_unlabelled = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation sets without redownloading data\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=False)\n",
    "\n",
    "# Use 10% of the data for training (simulating a low data scenario)\n",
    "num_train = int(len(trainval_set) * 0.1)\n",
    "\n",
    "# Split data into train/validation sets with a fixed random seed\n",
    "torch.manual_seed(0)  # Ensure reproducibility\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set) - num_train])\n",
    "\n",
    "# Load test set without redownloading data\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=False)\n",
    "\n",
    "# Determine the size of the subset (1/1000 of the full dataset)\n",
    "subset_size = len(unlabelled_set) // 1000  # This will be 100 samples\n",
    "\n",
    "# Randomly select indices for the subset\n",
    "random_indices = random.sample(range(len(unlabelled_set)), subset_size)\n",
    "\n",
    "# Create a subset of the unlabelled dataset\n",
    "unlabelled_subset = Subset(unlabelled_set, random_indices)\n",
    "\n",
    "# Now, create the DataLoader using the subset\n",
    "unlabelled_loader = DataLoader(unlabelled_subset, shuffle=True, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for train, validation, and test sets\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: class_token, Shape: torch.Size([1, 1, 768])\n",
      "Name: conv_proj.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Name: conv_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.pos_embedding, Shape: torch.Size([1, 197, 768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_0.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_1.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_2.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_3.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_4.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_5.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_6.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_7.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_8.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_9.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_10.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_1.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Shape: torch.Size([2304])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.weight, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.ln_2.bias, Shape: torch.Size([768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.weight, Shape: torch.Size([3072, 768])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.0.bias, Shape: torch.Size([3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.weight, Shape: torch.Size([768, 3072])\n",
      "Name: encoder.layers.encoder_layer_11.mlp.3.bias, Shape: torch.Size([768])\n",
      "Name: encoder.ln.weight, Shape: torch.Size([768])\n",
      "Name: encoder.ln.bias, Shape: torch.Size([768])\n",
      "Name: heads.head.weight, Shape: torch.Size([1000, 768])\n",
      "Name: heads.head.bias, Shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Vision Transformer (ViT) model from torchvision models\n",
    "model2 = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Print the model structure to verify the changes\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"Name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Transfer Learning with Grid Search...\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/10], Loss: 2.3624, Validation Macro F1: 0.1000, Time: 39.54s\n",
      "Config: fc, LR: 1e-05, Epoch [2/10], Loss: 2.3526, Validation Macro F1: 0.1069, Time: 36.82s\n",
      "Config: fc, LR: 1e-05, Epoch [3/10], Loss: 2.3451, Validation Macro F1: 0.1051, Time: 35.68s\n",
      "Config: fc, LR: 1e-05, Epoch [4/10], Loss: 2.3372, Validation Macro F1: 0.1114, Time: 36.29s\n",
      "Config: fc, LR: 1e-05, Epoch [5/10], Loss: 2.3298, Validation Macro F1: 0.1122, Time: 35.80s\n",
      "Config: fc, LR: 1e-05, Epoch [6/10], Loss: 2.3345, Validation Macro F1: 0.1169, Time: 37.39s\n",
      "Config: fc, LR: 1e-05, Epoch [7/10], Loss: 2.3244, Validation Macro F1: 0.1180, Time: 38.21s\n",
      "Config: fc, LR: 1e-05, Epoch [8/10], Loss: 2.3157, Validation Macro F1: 0.1231, Time: 37.75s\n",
      "Config: fc, LR: 1e-05, Epoch [9/10], Loss: 2.3051, Validation Macro F1: 0.1264, Time: 43.02s\n",
      "Config: fc, LR: 1e-05, Epoch [10/10], Loss: 2.3062, Validation Macro F1: 0.1316, Time: 36.66s\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/15], Loss: 2.3326, Validation Macro F1: 0.0975, Time: 41.18s\n",
      "Config: fc, LR: 1e-05, Epoch [2/15], Loss: 2.3324, Validation Macro F1: 0.1027, Time: 39.70s\n",
      "Config: fc, LR: 1e-05, Epoch [3/15], Loss: 2.3258, Validation Macro F1: 0.1037, Time: 41.45s\n",
      "Config: fc, LR: 1e-05, Epoch [4/15], Loss: 2.3180, Validation Macro F1: 0.1032, Time: 36.82s\n",
      "Config: fc, LR: 1e-05, Epoch [5/15], Loss: 2.3075, Validation Macro F1: 0.1094, Time: 42.32s\n",
      "Config: fc, LR: 1e-05, Epoch [6/15], Loss: 2.3011, Validation Macro F1: 0.1117, Time: 37.51s\n",
      "Config: fc, LR: 1e-05, Epoch [7/15], Loss: 2.2995, Validation Macro F1: 0.1188, Time: 35.60s\n",
      "Config: fc, LR: 1e-05, Epoch [8/15], Loss: 2.2935, Validation Macro F1: 0.1227, Time: 39.96s\n",
      "Config: fc, LR: 1e-05, Epoch [9/15], Loss: 2.2898, Validation Macro F1: 0.1236, Time: 34.96s\n",
      "Config: fc, LR: 1e-05, Epoch [10/15], Loss: 2.2821, Validation Macro F1: 0.1286, Time: 35.06s\n",
      "Config: fc, LR: 1e-05, Epoch [11/15], Loss: 2.2752, Validation Macro F1: 0.1280, Time: 42.17s\n",
      "Config: fc, LR: 1e-05, Epoch [12/15], Loss: 2.2719, Validation Macro F1: 0.1333, Time: 37.23s\n",
      "Config: fc, LR: 1e-05, Epoch [13/15], Loss: 2.2715, Validation Macro F1: 0.1365, Time: 42.09s\n",
      "Config: fc, LR: 1e-05, Epoch [14/15], Loss: 2.2605, Validation Macro F1: 0.1425, Time: 42.31s\n",
      "Config: fc, LR: 1e-05, Epoch [15/15], Loss: 2.2537, Validation Macro F1: 0.1415, Time: 42.36s\n",
      "\n",
      "Configuration: fc, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 1e-05, Epoch [1/20], Loss: 2.3362, Validation Macro F1: 0.1179, Time: 42.40s\n",
      "Config: fc, LR: 1e-05, Epoch [2/20], Loss: 2.3283, Validation Macro F1: 0.1186, Time: 43.43s\n",
      "Config: fc, LR: 1e-05, Epoch [3/20], Loss: 2.3282, Validation Macro F1: 0.1202, Time: 41.38s\n",
      "Config: fc, LR: 1e-05, Epoch [4/20], Loss: 2.3234, Validation Macro F1: 0.1267, Time: 42.86s\n",
      "Config: fc, LR: 1e-05, Epoch [5/20], Loss: 2.3088, Validation Macro F1: 0.1302, Time: 42.81s\n",
      "Config: fc, LR: 1e-05, Epoch [6/20], Loss: 2.3075, Validation Macro F1: 0.1303, Time: 43.79s\n",
      "Config: fc, LR: 1e-05, Epoch [7/20], Loss: 2.3070, Validation Macro F1: 0.1279, Time: 43.69s\n",
      "Config: fc, LR: 1e-05, Epoch [8/20], Loss: 2.2999, Validation Macro F1: 0.1341, Time: 43.54s\n",
      "Config: fc, LR: 1e-05, Epoch [9/20], Loss: 2.2886, Validation Macro F1: 0.1360, Time: 42.20s\n",
      "Config: fc, LR: 1e-05, Epoch [10/20], Loss: 2.2854, Validation Macro F1: 0.1375, Time: 35.64s\n",
      "Config: fc, LR: 1e-05, Epoch [11/20], Loss: 2.2771, Validation Macro F1: 0.1425, Time: 42.02s\n",
      "Config: fc, LR: 1e-05, Epoch [12/20], Loss: 2.2668, Validation Macro F1: 0.1462, Time: 37.90s\n",
      "Config: fc, LR: 1e-05, Epoch [13/20], Loss: 2.2704, Validation Macro F1: 0.1474, Time: 36.46s\n",
      "Config: fc, LR: 1e-05, Epoch [14/20], Loss: 2.2630, Validation Macro F1: 0.1546, Time: 36.56s\n",
      "Config: fc, LR: 1e-05, Epoch [15/20], Loss: 2.2569, Validation Macro F1: 0.1557, Time: 38.92s\n",
      "Config: fc, LR: 1e-05, Epoch [16/20], Loss: 2.2506, Validation Macro F1: 0.1575, Time: 42.30s\n",
      "Config: fc, LR: 1e-05, Epoch [17/20], Loss: 2.2447, Validation Macro F1: 0.1646, Time: 40.65s\n",
      "Config: fc, LR: 1e-05, Epoch [18/20], Loss: 2.2407, Validation Macro F1: 0.1618, Time: 41.25s\n",
      "Config: fc, LR: 1e-05, Epoch [19/20], Loss: 2.2311, Validation Macro F1: 0.1702, Time: 37.99s\n",
      "Config: fc, LR: 1e-05, Epoch [20/20], Loss: 2.2314, Validation Macro F1: 0.1771, Time: 37.47s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/10], Loss: 2.4447, Validation Macro F1: 0.0555, Time: 41.61s\n",
      "Config: fc, LR: 0.0001, Epoch [2/10], Loss: 2.3915, Validation Macro F1: 0.0772, Time: 35.63s\n",
      "Config: fc, LR: 0.0001, Epoch [3/10], Loss: 2.3204, Validation Macro F1: 0.1106, Time: 35.46s\n",
      "Config: fc, LR: 0.0001, Epoch [4/10], Loss: 2.2697, Validation Macro F1: 0.1426, Time: 33.86s\n",
      "Config: fc, LR: 0.0001, Epoch [5/10], Loss: 2.2107, Validation Macro F1: 0.1807, Time: 33.70s\n",
      "Config: fc, LR: 0.0001, Epoch [6/10], Loss: 2.1549, Validation Macro F1: 0.2364, Time: 33.68s\n",
      "Config: fc, LR: 0.0001, Epoch [7/10], Loss: 2.1054, Validation Macro F1: 0.2938, Time: 33.71s\n",
      "Config: fc, LR: 0.0001, Epoch [8/10], Loss: 2.0484, Validation Macro F1: 0.3467, Time: 34.30s\n",
      "Config: fc, LR: 0.0001, Epoch [9/10], Loss: 1.9961, Validation Macro F1: 0.4032, Time: 38.09s\n",
      "Config: fc, LR: 0.0001, Epoch [10/10], Loss: 1.9413, Validation Macro F1: 0.4568, Time: 39.91s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/15], Loss: 2.2715, Validation Macro F1: 0.1465, Time: 40.97s\n",
      "Config: fc, LR: 0.0001, Epoch [2/15], Loss: 2.2052, Validation Macro F1: 0.1911, Time: 42.50s\n",
      "Config: fc, LR: 0.0001, Epoch [3/15], Loss: 2.1563, Validation Macro F1: 0.2442, Time: 41.19s\n",
      "Config: fc, LR: 0.0001, Epoch [4/15], Loss: 2.1045, Validation Macro F1: 0.2942, Time: 40.23s\n",
      "Config: fc, LR: 0.0001, Epoch [5/15], Loss: 2.0464, Validation Macro F1: 0.3530, Time: 40.10s\n",
      "Config: fc, LR: 0.0001, Epoch [6/15], Loss: 1.9878, Validation Macro F1: 0.4081, Time: 38.98s\n",
      "Config: fc, LR: 0.0001, Epoch [7/15], Loss: 1.9409, Validation Macro F1: 0.4676, Time: 35.39s\n",
      "Config: fc, LR: 0.0001, Epoch [8/15], Loss: 1.8800, Validation Macro F1: 0.5256, Time: 38.53s\n",
      "Config: fc, LR: 0.0001, Epoch [9/15], Loss: 1.8305, Validation Macro F1: 0.5689, Time: 40.89s\n",
      "Config: fc, LR: 0.0001, Epoch [10/15], Loss: 1.7871, Validation Macro F1: 0.6129, Time: 37.21s\n",
      "Config: fc, LR: 0.0001, Epoch [11/15], Loss: 1.7293, Validation Macro F1: 0.6473, Time: 34.85s\n",
      "Config: fc, LR: 0.0001, Epoch [12/15], Loss: 1.6868, Validation Macro F1: 0.6779, Time: 35.46s\n",
      "Config: fc, LR: 0.0001, Epoch [13/15], Loss: 1.6385, Validation Macro F1: 0.7072, Time: 34.79s\n",
      "Config: fc, LR: 0.0001, Epoch [14/15], Loss: 1.5850, Validation Macro F1: 0.7299, Time: 40.12s\n",
      "Config: fc, LR: 0.0001, Epoch [15/15], Loss: 1.5488, Validation Macro F1: 0.7602, Time: 36.61s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.0001, Epoch [1/20], Loss: 2.3507, Validation Macro F1: 0.0925, Time: 34.69s\n",
      "Config: fc, LR: 0.0001, Epoch [2/20], Loss: 2.2906, Validation Macro F1: 0.1220, Time: 37.74s\n",
      "Config: fc, LR: 0.0001, Epoch [3/20], Loss: 2.2264, Validation Macro F1: 0.1507, Time: 35.85s\n",
      "Config: fc, LR: 0.0001, Epoch [4/20], Loss: 2.1701, Validation Macro F1: 0.1834, Time: 35.96s\n",
      "Config: fc, LR: 0.0001, Epoch [5/20], Loss: 2.1173, Validation Macro F1: 0.2251, Time: 36.72s\n",
      "Config: fc, LR: 0.0001, Epoch [6/20], Loss: 2.0590, Validation Macro F1: 0.2740, Time: 37.22s\n",
      "Config: fc, LR: 0.0001, Epoch [7/20], Loss: 2.0072, Validation Macro F1: 0.3256, Time: 35.10s\n",
      "Config: fc, LR: 0.0001, Epoch [8/20], Loss: 1.9606, Validation Macro F1: 0.3822, Time: 39.63s\n",
      "Config: fc, LR: 0.0001, Epoch [9/20], Loss: 1.8976, Validation Macro F1: 0.4360, Time: 41.55s\n",
      "Config: fc, LR: 0.0001, Epoch [10/20], Loss: 1.8560, Validation Macro F1: 0.4889, Time: 36.61s\n",
      "Config: fc, LR: 0.0001, Epoch [11/20], Loss: 1.8048, Validation Macro F1: 0.5467, Time: 34.87s\n",
      "Config: fc, LR: 0.0001, Epoch [12/20], Loss: 1.7510, Validation Macro F1: 0.5867, Time: 35.70s\n",
      "Config: fc, LR: 0.0001, Epoch [13/20], Loss: 1.7096, Validation Macro F1: 0.6282, Time: 34.05s\n",
      "Config: fc, LR: 0.0001, Epoch [14/20], Loss: 1.6570, Validation Macro F1: 0.6773, Time: 33.87s\n",
      "Config: fc, LR: 0.0001, Epoch [15/20], Loss: 1.6213, Validation Macro F1: 0.6999, Time: 33.53s\n",
      "Config: fc, LR: 0.0001, Epoch [16/20], Loss: 1.5678, Validation Macro F1: 0.7298, Time: 37.56s\n",
      "Config: fc, LR: 0.0001, Epoch [17/20], Loss: 1.5253, Validation Macro F1: 0.7589, Time: 39.67s\n",
      "Config: fc, LR: 0.0001, Epoch [18/20], Loss: 1.4828, Validation Macro F1: 0.7777, Time: 34.12s\n",
      "Config: fc, LR: 0.0001, Epoch [19/20], Loss: 1.4397, Validation Macro F1: 0.7986, Time: 40.13s\n",
      "Config: fc, LR: 0.0001, Epoch [20/20], Loss: 1.4033, Validation Macro F1: 0.8121, Time: 36.05s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/10], Loss: 2.1475, Validation Macro F1: 0.6034, Time: 36.60s\n",
      "Config: fc, LR: 0.001, Epoch [2/10], Loss: 1.6075, Validation Macro F1: 0.8173, Time: 38.96s\n",
      "Config: fc, LR: 0.001, Epoch [3/10], Loss: 1.1820, Validation Macro F1: 0.8811, Time: 39.03s\n",
      "Config: fc, LR: 0.001, Epoch [4/10], Loss: 0.8725, Validation Macro F1: 0.9122, Time: 39.77s\n",
      "Config: fc, LR: 0.001, Epoch [5/10], Loss: 0.6559, Validation Macro F1: 0.9322, Time: 42.20s\n",
      "Config: fc, LR: 0.001, Epoch [6/10], Loss: 0.5095, Validation Macro F1: 0.9431, Time: 43.06s\n",
      "Config: fc, LR: 0.001, Epoch [7/10], Loss: 0.4003, Validation Macro F1: 0.9488, Time: 35.62s\n",
      "Config: fc, LR: 0.001, Epoch [8/10], Loss: 0.3283, Validation Macro F1: 0.9538, Time: 35.94s\n",
      "Config: fc, LR: 0.001, Epoch [9/10], Loss: 0.2785, Validation Macro F1: 0.9583, Time: 37.42s\n",
      "Config: fc, LR: 0.001, Epoch [10/10], Loss: 0.2305, Validation Macro F1: 0.9593, Time: 35.46s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/15], Loss: 2.1076, Validation Macro F1: 0.6068, Time: 39.64s\n",
      "Config: fc, LR: 0.001, Epoch [2/15], Loss: 1.5843, Validation Macro F1: 0.8479, Time: 35.63s\n",
      "Config: fc, LR: 0.001, Epoch [3/15], Loss: 1.1498, Validation Macro F1: 0.9058, Time: 33.61s\n",
      "Config: fc, LR: 0.001, Epoch [4/15], Loss: 0.8353, Validation Macro F1: 0.9302, Time: 33.54s\n",
      "Config: fc, LR: 0.001, Epoch [5/15], Loss: 0.6243, Validation Macro F1: 0.9410, Time: 33.55s\n",
      "Config: fc, LR: 0.001, Epoch [6/15], Loss: 0.4767, Validation Macro F1: 0.9508, Time: 33.73s\n",
      "Config: fc, LR: 0.001, Epoch [7/15], Loss: 0.3840, Validation Macro F1: 0.9579, Time: 33.73s\n",
      "Config: fc, LR: 0.001, Epoch [8/15], Loss: 0.3059, Validation Macro F1: 0.9577, Time: 33.53s\n",
      "Config: fc, LR: 0.001, Epoch [9/15], Loss: 0.2619, Validation Macro F1: 0.9600, Time: 33.93s\n",
      "Config: fc, LR: 0.001, Epoch [10/15], Loss: 0.2216, Validation Macro F1: 0.9618, Time: 34.40s\n",
      "Config: fc, LR: 0.001, Epoch [11/15], Loss: 0.1896, Validation Macro F1: 0.9649, Time: 34.33s\n",
      "Config: fc, LR: 0.001, Epoch [12/15], Loss: 0.1640, Validation Macro F1: 0.9671, Time: 33.70s\n",
      "Config: fc, LR: 0.001, Epoch [13/15], Loss: 0.1460, Validation Macro F1: 0.9675, Time: 33.46s\n",
      "Config: fc, LR: 0.001, Epoch [14/15], Loss: 0.1365, Validation Macro F1: 0.9660, Time: 33.72s\n",
      "Config: fc, LR: 0.001, Epoch [15/15], Loss: 0.1236, Validation Macro F1: 0.9656, Time: 35.44s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.001, Epoch [1/20], Loss: 2.1269, Validation Macro F1: 0.5887, Time: 33.73s\n",
      "Config: fc, LR: 0.001, Epoch [2/20], Loss: 1.5855, Validation Macro F1: 0.8649, Time: 34.02s\n",
      "Config: fc, LR: 0.001, Epoch [3/20], Loss: 1.1676, Validation Macro F1: 0.9180, Time: 33.69s\n",
      "Config: fc, LR: 0.001, Epoch [4/20], Loss: 0.8481, Validation Macro F1: 0.9386, Time: 33.66s\n",
      "Config: fc, LR: 0.001, Epoch [5/20], Loss: 0.6226, Validation Macro F1: 0.9482, Time: 33.49s\n",
      "Config: fc, LR: 0.001, Epoch [6/20], Loss: 0.4707, Validation Macro F1: 0.9528, Time: 33.59s\n",
      "Config: fc, LR: 0.001, Epoch [7/20], Loss: 0.3720, Validation Macro F1: 0.9566, Time: 33.54s\n",
      "Config: fc, LR: 0.001, Epoch [8/20], Loss: 0.3101, Validation Macro F1: 0.9610, Time: 33.48s\n",
      "Config: fc, LR: 0.001, Epoch [9/20], Loss: 0.2550, Validation Macro F1: 0.9662, Time: 33.47s\n",
      "Config: fc, LR: 0.001, Epoch [10/20], Loss: 0.2143, Validation Macro F1: 0.9646, Time: 33.48s\n",
      "Config: fc, LR: 0.001, Epoch [11/20], Loss: 0.1875, Validation Macro F1: 0.9682, Time: 33.94s\n",
      "Config: fc, LR: 0.001, Epoch [12/20], Loss: 0.1686, Validation Macro F1: 0.9671, Time: 33.41s\n",
      "Config: fc, LR: 0.001, Epoch [13/20], Loss: 0.1459, Validation Macro F1: 0.9686, Time: 33.67s\n",
      "Config: fc, LR: 0.001, Epoch [14/20], Loss: 0.1353, Validation Macro F1: 0.9687, Time: 33.68s\n",
      "Config: fc, LR: 0.001, Epoch [15/20], Loss: 0.1226, Validation Macro F1: 0.9704, Time: 33.47s\n",
      "Config: fc, LR: 0.001, Epoch [16/20], Loss: 0.1068, Validation Macro F1: 0.9700, Time: 33.46s\n",
      "Config: fc, LR: 0.001, Epoch [17/20], Loss: 0.1009, Validation Macro F1: 0.9693, Time: 33.48s\n",
      "Config: fc, LR: 0.001, Epoch [18/20], Loss: 0.0951, Validation Macro F1: 0.9702, Time: 33.70s\n",
      "Config: fc, LR: 0.001, Epoch [19/20], Loss: 0.0828, Validation Macro F1: 0.9713, Time: 33.77s\n",
      "Config: fc, LR: 0.001, Epoch [20/20], Loss: 0.0807, Validation Macro F1: 0.9718, Time: 33.74s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/10], Loss: 1.1019, Validation Macro F1: 0.9615, Time: 33.48s\n",
      "Config: fc, LR: 0.01, Epoch [2/10], Loss: 0.1014, Validation Macro F1: 0.9664, Time: 33.46s\n",
      "Config: fc, LR: 0.01, Epoch [3/10], Loss: 0.0425, Validation Macro F1: 0.9682, Time: 33.66s\n",
      "Config: fc, LR: 0.01, Epoch [4/10], Loss: 0.0154, Validation Macro F1: 0.9682, Time: 33.49s\n",
      "Config: fc, LR: 0.01, Epoch [5/10], Loss: 0.0074, Validation Macro F1: 0.9662, Time: 33.46s\n",
      "Config: fc, LR: 0.01, Epoch [6/10], Loss: 0.0054, Validation Macro F1: 0.9715, Time: 33.47s\n",
      "Config: fc, LR: 0.01, Epoch [7/10], Loss: 0.0022, Validation Macro F1: 0.9680, Time: 33.52s\n",
      "Config: fc, LR: 0.01, Epoch [8/10], Loss: 0.0027, Validation Macro F1: 0.9700, Time: 33.47s\n",
      "Config: fc, LR: 0.01, Epoch [9/10], Loss: 0.0014, Validation Macro F1: 0.9702, Time: 33.67s\n",
      "Config: fc, LR: 0.01, Epoch [10/10], Loss: 0.0020, Validation Macro F1: 0.9709, Time: 33.66s\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/15], Loss: 1.1412, Validation Macro F1: 0.9598, Time: 33.70s\n",
      "Config: fc, LR: 0.01, Epoch [2/15], Loss: 0.0994, Validation Macro F1: 0.9673, Time: 33.87s\n",
      "Config: fc, LR: 0.01, Epoch [3/15], Loss: 0.0353, Validation Macro F1: 0.9705, Time: 33.64s\n",
      "Config: fc, LR: 0.01, Epoch [4/15], Loss: 0.0115, Validation Macro F1: 0.9716, Time: 33.45s\n",
      "Config: fc, LR: 0.01, Epoch [5/15], Loss: 0.0061, Validation Macro F1: 0.9709, Time: 33.49s\n",
      "Config: fc, LR: 0.01, Epoch [6/15], Loss: 0.0106, Validation Macro F1: 0.9737, Time: 33.65s\n",
      "Config: fc, LR: 0.01, Epoch [7/15], Loss: 0.0016, Validation Macro F1: 0.9726, Time: 33.47s\n",
      "Config: fc, LR: 0.01, Epoch [8/15], Loss: 0.0070, Validation Macro F1: 0.9697, Time: 33.93s\n",
      "Config: fc, LR: 0.01, Epoch [9/15], Loss: 0.0024, Validation Macro F1: 0.9704, Time: 33.67s\n",
      "Config: fc, LR: 0.01, Epoch [10/15], Loss: 0.0012, Validation Macro F1: 0.9720, Time: 33.48s\n",
      "Config: fc, LR: 0.01, Epoch [11/15], Loss: 0.0013, Validation Macro F1: 0.9709, Time: 33.50s\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Configuration: fc, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc, LR: 0.01, Epoch [1/20], Loss: 1.1151, Validation Macro F1: 0.9595, Time: 33.62s\n",
      "Config: fc, LR: 0.01, Epoch [2/20], Loss: 0.1093, Validation Macro F1: 0.9655, Time: 33.69s\n",
      "Config: fc, LR: 0.01, Epoch [3/20], Loss: 0.0362, Validation Macro F1: 0.9680, Time: 33.47s\n",
      "Config: fc, LR: 0.01, Epoch [4/20], Loss: 0.0118, Validation Macro F1: 0.9695, Time: 33.79s\n",
      "Config: fc, LR: 0.01, Epoch [5/20], Loss: 0.0072, Validation Macro F1: 0.9673, Time: 33.74s\n",
      "Config: fc, LR: 0.01, Epoch [6/20], Loss: 0.0060, Validation Macro F1: 0.9720, Time: 33.48s\n",
      "Config: fc, LR: 0.01, Epoch [7/20], Loss: 0.0057, Validation Macro F1: 0.9693, Time: 33.64s\n",
      "Config: fc, LR: 0.01, Epoch [8/20], Loss: 0.0026, Validation Macro F1: 0.9722, Time: 33.50s\n",
      "Config: fc, LR: 0.01, Epoch [9/20], Loss: 0.0015, Validation Macro F1: 0.9709, Time: 33.44s\n",
      "Config: fc, LR: 0.01, Epoch [10/20], Loss: 0.0006, Validation Macro F1: 0.9704, Time: 33.51s\n",
      "Config: fc, LR: 0.01, Epoch [11/20], Loss: 0.0008, Validation Macro F1: 0.9706, Time: 33.65s\n",
      "Config: fc, LR: 0.01, Epoch [12/20], Loss: 0.0009, Validation Macro F1: 0.9706, Time: 33.68s\n",
      "Config: fc, LR: 0.01, Epoch [13/20], Loss: 0.0014, Validation Macro F1: 0.9713, Time: 33.66s\n",
      "Early stopping at epoch 13\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 1e-05, Epoch [1/10], Loss: 2.3731, Validation Macro F1: 0.1010, Time: 33.98s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [2/10], Loss: 2.3292, Validation Macro F1: 0.1291, Time: 33.85s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [3/10], Loss: 2.2749, Validation Macro F1: 0.1554, Time: 33.84s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [4/10], Loss: 2.2341, Validation Macro F1: 0.1825, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [5/10], Loss: 2.1910, Validation Macro F1: 0.2145, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [6/10], Loss: 2.1425, Validation Macro F1: 0.2450, Time: 34.09s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [7/10], Loss: 2.0938, Validation Macro F1: 0.2910, Time: 34.05s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [8/10], Loss: 2.0574, Validation Macro F1: 0.3222, Time: 34.04s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [9/10], Loss: 1.9992, Validation Macro F1: 0.3622, Time: 34.11s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [10/10], Loss: 1.9590, Validation Macro F1: 0.4140, Time: 33.84s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 1e-05, Epoch [1/15], Loss: 2.3020, Validation Macro F1: 0.1548, Time: 34.04s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [2/15], Loss: 2.2539, Validation Macro F1: 0.1818, Time: 34.05s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [3/15], Loss: 2.2025, Validation Macro F1: 0.2119, Time: 33.89s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [4/15], Loss: 2.1587, Validation Macro F1: 0.2516, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [5/15], Loss: 2.1141, Validation Macro F1: 0.2898, Time: 34.03s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [6/15], Loss: 2.0706, Validation Macro F1: 0.3320, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [7/15], Loss: 2.0250, Validation Macro F1: 0.3732, Time: 34.48s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [8/15], Loss: 1.9846, Validation Macro F1: 0.4186, Time: 35.86s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [9/15], Loss: 1.9328, Validation Macro F1: 0.4563, Time: 35.07s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [10/15], Loss: 1.8845, Validation Macro F1: 0.5001, Time: 36.23s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [11/15], Loss: 1.8333, Validation Macro F1: 0.5372, Time: 36.53s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [12/15], Loss: 1.7948, Validation Macro F1: 0.5696, Time: 35.98s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [13/15], Loss: 1.7322, Validation Macro F1: 0.6088, Time: 35.43s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [14/15], Loss: 1.6952, Validation Macro F1: 0.6396, Time: 35.92s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [15/15], Loss: 1.6355, Validation Macro F1: 0.6693, Time: 37.13s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 1e-05, Epoch [1/20], Loss: 2.3143, Validation Macro F1: 0.1602, Time: 36.14s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [2/20], Loss: 2.2675, Validation Macro F1: 0.1827, Time: 35.73s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [3/20], Loss: 2.2271, Validation Macro F1: 0.2055, Time: 35.70s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [4/20], Loss: 2.1777, Validation Macro F1: 0.2272, Time: 35.53s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [5/20], Loss: 2.1280, Validation Macro F1: 0.2675, Time: 35.92s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [6/20], Loss: 2.0935, Validation Macro F1: 0.2930, Time: 35.64s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [7/20], Loss: 2.0440, Validation Macro F1: 0.3332, Time: 35.77s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [8/20], Loss: 2.0034, Validation Macro F1: 0.3707, Time: 35.62s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [9/20], Loss: 1.9570, Validation Macro F1: 0.4061, Time: 35.70s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [10/20], Loss: 1.9145, Validation Macro F1: 0.4431, Time: 35.57s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [11/20], Loss: 1.8675, Validation Macro F1: 0.4793, Time: 35.52s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [12/20], Loss: 1.8154, Validation Macro F1: 0.5079, Time: 35.71s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [13/20], Loss: 1.7705, Validation Macro F1: 0.5507, Time: 35.47s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [14/20], Loss: 1.7104, Validation Macro F1: 0.5829, Time: 35.51s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [15/20], Loss: 1.6652, Validation Macro F1: 0.6170, Time: 35.70s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [16/20], Loss: 1.6018, Validation Macro F1: 0.6513, Time: 35.41s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [17/20], Loss: 1.5451, Validation Macro F1: 0.6906, Time: 35.46s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [18/20], Loss: 1.4868, Validation Macro F1: 0.7132, Time: 35.73s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [19/20], Loss: 1.4272, Validation Macro F1: 0.7351, Time: 35.66s\n",
      "Config: fc+encoder11, LR: 1e-05, Epoch [20/20], Loss: 1.3640, Validation Macro F1: 0.7649, Time: 35.74s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.0001, Epoch [1/10], Loss: 2.2053, Validation Macro F1: 0.4142, Time: 35.67s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [2/10], Loss: 1.7557, Validation Macro F1: 0.7204, Time: 35.78s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [3/10], Loss: 1.2749, Validation Macro F1: 0.8799, Time: 35.67s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [4/10], Loss: 0.8286, Validation Macro F1: 0.9290, Time: 35.45s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [5/10], Loss: 0.5123, Validation Macro F1: 0.9495, Time: 35.57s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [6/10], Loss: 0.3219, Validation Macro F1: 0.9601, Time: 35.69s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [7/10], Loss: 0.2021, Validation Macro F1: 0.9663, Time: 35.52s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [8/10], Loss: 0.1368, Validation Macro F1: 0.9694, Time: 35.32s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [9/10], Loss: 0.0925, Validation Macro F1: 0.9705, Time: 35.73s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [10/10], Loss: 0.0717, Validation Macro F1: 0.9725, Time: 35.03s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.0001, Epoch [1/15], Loss: 2.2317, Validation Macro F1: 0.4036, Time: 34.93s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [2/15], Loss: 1.8047, Validation Macro F1: 0.6575, Time: 35.17s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [3/15], Loss: 1.3647, Validation Macro F1: 0.8328, Time: 35.14s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [4/15], Loss: 0.9245, Validation Macro F1: 0.9214, Time: 35.13s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [5/15], Loss: 0.5915, Validation Macro F1: 0.9491, Time: 34.93s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [6/15], Loss: 0.3774, Validation Macro F1: 0.9605, Time: 34.95s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [7/15], Loss: 0.2361, Validation Macro F1: 0.9634, Time: 35.15s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [8/15], Loss: 0.1570, Validation Macro F1: 0.9667, Time: 35.15s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [9/15], Loss: 0.1112, Validation Macro F1: 0.9698, Time: 35.53s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [10/15], Loss: 0.0784, Validation Macro F1: 0.9713, Time: 35.15s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [11/15], Loss: 0.0569, Validation Macro F1: 0.9713, Time: 35.01s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [12/15], Loss: 0.0446, Validation Macro F1: 0.9718, Time: 35.09s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [13/15], Loss: 0.0384, Validation Macro F1: 0.9740, Time: 35.87s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [14/15], Loss: 0.0298, Validation Macro F1: 0.9740, Time: 35.19s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [15/15], Loss: 0.0244, Validation Macro F1: 0.9747, Time: 35.18s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.0001, Epoch [1/20], Loss: 2.2235, Validation Macro F1: 0.3876, Time: 35.10s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [2/20], Loss: 1.7623, Validation Macro F1: 0.6300, Time: 36.11s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [3/20], Loss: 1.3107, Validation Macro F1: 0.8472, Time: 34.18s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [4/20], Loss: 0.8914, Validation Macro F1: 0.9300, Time: 35.27s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [5/20], Loss: 0.5960, Validation Macro F1: 0.9518, Time: 34.17s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [6/20], Loss: 0.3680, Validation Macro F1: 0.9576, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [7/20], Loss: 0.2341, Validation Macro F1: 0.9638, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [8/20], Loss: 0.1551, Validation Macro F1: 0.9662, Time: 34.24s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [9/20], Loss: 0.1074, Validation Macro F1: 0.9700, Time: 34.09s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [10/20], Loss: 0.0782, Validation Macro F1: 0.9707, Time: 34.09s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [11/20], Loss: 0.0612, Validation Macro F1: 0.9734, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [12/20], Loss: 0.0461, Validation Macro F1: 0.9716, Time: 34.29s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [13/20], Loss: 0.0331, Validation Macro F1: 0.9742, Time: 34.10s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [14/20], Loss: 0.0291, Validation Macro F1: 0.9745, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [15/20], Loss: 0.0248, Validation Macro F1: 0.9758, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [16/20], Loss: 0.0208, Validation Macro F1: 0.9747, Time: 34.23s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [17/20], Loss: 0.0198, Validation Macro F1: 0.9740, Time: 34.73s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [18/20], Loss: 0.0159, Validation Macro F1: 0.9751, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [19/20], Loss: 0.0159, Validation Macro F1: 0.9729, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.0001, Epoch [20/20], Loss: 0.0138, Validation Macro F1: 0.9756, Time: 34.08s\n",
      "Early stopping at epoch 20\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.001, Epoch [1/10], Loss: 1.2567, Validation Macro F1: 0.9556, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [2/10], Loss: 0.0848, Validation Macro F1: 0.9702, Time: 34.11s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [3/10], Loss: 0.0141, Validation Macro F1: 0.9728, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [4/10], Loss: 0.0089, Validation Macro F1: 0.9711, Time: 34.09s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [5/10], Loss: 0.0029, Validation Macro F1: 0.9720, Time: 34.10s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [6/10], Loss: 0.0013, Validation Macro F1: 0.9765, Time: 34.14s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [7/10], Loss: 0.0012, Validation Macro F1: 0.9733, Time: 35.31s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [8/10], Loss: 0.0016, Validation Macro F1: 0.9745, Time: 35.02s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [9/10], Loss: 0.0006, Validation Macro F1: 0.9751, Time: 35.80s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [10/10], Loss: 0.0006, Validation Macro F1: 0.9741, Time: 34.13s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.001, Epoch [1/15], Loss: 1.4457, Validation Macro F1: 0.9473, Time: 34.14s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [2/15], Loss: 0.1153, Validation Macro F1: 0.9694, Time: 34.05s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [3/15], Loss: 0.0209, Validation Macro F1: 0.9727, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [4/15], Loss: 0.0086, Validation Macro F1: 0.9722, Time: 34.26s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [5/15], Loss: 0.0049, Validation Macro F1: 0.9738, Time: 34.10s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [6/15], Loss: 0.0031, Validation Macro F1: 0.9751, Time: 34.10s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [7/15], Loss: 0.0009, Validation Macro F1: 0.9733, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [8/15], Loss: 0.0006, Validation Macro F1: 0.9716, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [9/15], Loss: 0.0005, Validation Macro F1: 0.9716, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [10/15], Loss: 0.0006, Validation Macro F1: 0.9740, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [11/15], Loss: 0.0006, Validation Macro F1: 0.9729, Time: 34.28s\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.001, Epoch [1/20], Loss: 1.4927, Validation Macro F1: 0.9511, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [2/20], Loss: 0.1048, Validation Macro F1: 0.9740, Time: 35.11s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [3/20], Loss: 0.0187, Validation Macro F1: 0.9718, Time: 35.57s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [4/20], Loss: 0.0090, Validation Macro F1: 0.9770, Time: 34.77s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [5/20], Loss: 0.0036, Validation Macro F1: 0.9783, Time: 35.86s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [6/20], Loss: 0.0027, Validation Macro F1: 0.9736, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [7/20], Loss: 0.0014, Validation Macro F1: 0.9752, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [8/20], Loss: 0.0007, Validation Macro F1: 0.9758, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [9/20], Loss: 0.0005, Validation Macro F1: 0.9758, Time: 34.14s\n",
      "Config: fc+encoder11, LR: 0.001, Epoch [10/20], Loss: 0.0005, Validation Macro F1: 0.9756, Time: 34.02s\n",
      "Early stopping at epoch 10\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.01, Epoch [1/10], Loss: 2.4305, Validation Macro F1: 0.4543, Time: 33.89s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [2/10], Loss: 0.6873, Validation Macro F1: 0.9426, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [3/10], Loss: 0.0664, Validation Macro F1: 0.9572, Time: 34.25s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [4/10], Loss: 0.0362, Validation Macro F1: 0.9596, Time: 33.89s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [5/10], Loss: 0.0256, Validation Macro F1: 0.9609, Time: 34.05s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [6/10], Loss: 0.0182, Validation Macro F1: 0.9632, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [7/10], Loss: 0.0220, Validation Macro F1: 0.9598, Time: 34.11s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [8/10], Loss: 0.0139, Validation Macro F1: 0.9527, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [9/10], Loss: 0.0141, Validation Macro F1: 0.9559, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [10/10], Loss: 0.0313, Validation Macro F1: 0.9654, Time: 33.88s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.01, Epoch [1/15], Loss: 2.0226, Validation Macro F1: 0.3298, Time: 34.03s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [2/15], Loss: 1.3533, Validation Macro F1: 0.9200, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [3/15], Loss: 0.1980, Validation Macro F1: 0.9515, Time: 34.09s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [4/15], Loss: 0.0647, Validation Macro F1: 0.9529, Time: 34.06s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [5/15], Loss: 0.0426, Validation Macro F1: 0.9453, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [6/15], Loss: 0.0389, Validation Macro F1: 0.9618, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [7/15], Loss: 0.0264, Validation Macro F1: 0.9499, Time: 34.57s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [8/15], Loss: 0.0206, Validation Macro F1: 0.9631, Time: 34.17s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [9/15], Loss: 0.0069, Validation Macro F1: 0.9538, Time: 33.91s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [10/15], Loss: 0.0119, Validation Macro F1: 0.9615, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [11/15], Loss: 0.0030, Validation Macro F1: 0.9657, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [12/15], Loss: 0.0077, Validation Macro F1: 0.9579, Time: 34.13s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [13/15], Loss: 0.0166, Validation Macro F1: 0.9590, Time: 34.18s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [14/15], Loss: 0.0047, Validation Macro F1: 0.9569, Time: 33.98s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [15/15], Loss: 0.0025, Validation Macro F1: 0.9614, Time: 34.04s\n",
      "\n",
      "Configuration: fc+encoder11, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder11, LR: 0.01, Epoch [1/20], Loss: 2.7548, Validation Macro F1: 0.3013, Time: 33.89s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [2/20], Loss: 2.3435, Validation Macro F1: 0.7457, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [3/20], Loss: 0.4555, Validation Macro F1: 0.9220, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [4/20], Loss: 0.1363, Validation Macro F1: 0.9291, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [5/20], Loss: 0.0909, Validation Macro F1: 0.9496, Time: 34.30s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [6/20], Loss: 0.0511, Validation Macro F1: 0.9438, Time: 34.05s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [7/20], Loss: 0.0815, Validation Macro F1: 0.9589, Time: 33.87s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [8/20], Loss: 0.0126, Validation Macro F1: 0.9532, Time: 33.89s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [9/20], Loss: 0.0195, Validation Macro F1: 0.9533, Time: 33.88s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [10/20], Loss: 0.0401, Validation Macro F1: 0.9559, Time: 34.07s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [11/20], Loss: 0.0204, Validation Macro F1: 0.9584, Time: 34.08s\n",
      "Config: fc+encoder11, LR: 0.01, Epoch [12/20], Loss: 0.0124, Validation Macro F1: 0.9547, Time: 34.09s\n",
      "Early stopping at epoch 12\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 1e-05, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [1/10], Loss: 2.3079, Validation Macro F1: 0.1551, Time: 34.51s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [2/10], Loss: 2.2142, Validation Macro F1: 0.2009, Time: 34.84s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [3/10], Loss: 2.1303, Validation Macro F1: 0.2578, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [4/10], Loss: 2.0533, Validation Macro F1: 0.3155, Time: 34.06s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [5/10], Loss: 1.9614, Validation Macro F1: 0.3893, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [6/10], Loss: 1.8663, Validation Macro F1: 0.4723, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [7/10], Loss: 1.7684, Validation Macro F1: 0.5451, Time: 34.25s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [8/10], Loss: 1.6658, Validation Macro F1: 0.6076, Time: 34.48s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [9/10], Loss: 1.5722, Validation Macro F1: 0.6724, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [10/10], Loss: 1.4711, Validation Macro F1: 0.7243, Time: 34.25s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 1e-05, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [1/15], Loss: 2.2432, Validation Macro F1: 0.2095, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [2/15], Loss: 2.1569, Validation Macro F1: 0.2679, Time: 34.03s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [3/15], Loss: 2.0711, Validation Macro F1: 0.3275, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [4/15], Loss: 2.0035, Validation Macro F1: 0.3841, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [5/15], Loss: 1.9172, Validation Macro F1: 0.4381, Time: 34.51s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [6/15], Loss: 1.8305, Validation Macro F1: 0.5010, Time: 34.41s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [7/15], Loss: 1.7516, Validation Macro F1: 0.5520, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [8/15], Loss: 1.6592, Validation Macro F1: 0.5957, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [9/15], Loss: 1.5710, Validation Macro F1: 0.6544, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [10/15], Loss: 1.4779, Validation Macro F1: 0.6940, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [11/15], Loss: 1.3829, Validation Macro F1: 0.7336, Time: 34.35s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [12/15], Loss: 1.2883, Validation Macro F1: 0.7794, Time: 34.58s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [13/15], Loss: 1.2019, Validation Macro F1: 0.8102, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [14/15], Loss: 1.1214, Validation Macro F1: 0.8396, Time: 34.45s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [15/15], Loss: 1.0337, Validation Macro F1: 0.8686, Time: 34.12s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 1e-05, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [1/20], Loss: 2.3612, Validation Macro F1: 0.1234, Time: 34.45s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [2/20], Loss: 2.2557, Validation Macro F1: 0.1657, Time: 34.44s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [3/20], Loss: 2.1878, Validation Macro F1: 0.2122, Time: 34.52s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [4/20], Loss: 2.1119, Validation Macro F1: 0.2676, Time: 34.46s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [5/20], Loss: 2.0198, Validation Macro F1: 0.3321, Time: 34.07s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [6/20], Loss: 1.9357, Validation Macro F1: 0.3951, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [7/20], Loss: 1.8427, Validation Macro F1: 0.4561, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [8/20], Loss: 1.7629, Validation Macro F1: 0.5167, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [9/20], Loss: 1.6549, Validation Macro F1: 0.5857, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [10/20], Loss: 1.5655, Validation Macro F1: 0.6474, Time: 34.50s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [11/20], Loss: 1.4611, Validation Macro F1: 0.7006, Time: 34.47s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [12/20], Loss: 1.3669, Validation Macro F1: 0.7588, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [13/20], Loss: 1.2647, Validation Macro F1: 0.8016, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [14/20], Loss: 1.1815, Validation Macro F1: 0.8285, Time: 34.11s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [15/20], Loss: 1.0955, Validation Macro F1: 0.8606, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [16/20], Loss: 1.0074, Validation Macro F1: 0.8757, Time: 34.24s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [17/20], Loss: 0.9309, Validation Macro F1: 0.8950, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [18/20], Loss: 0.8518, Validation Macro F1: 0.9104, Time: 34.46s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [19/20], Loss: 0.7784, Validation Macro F1: 0.9258, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 1e-05, Epoch [20/20], Loss: 0.7173, Validation Macro F1: 0.9317, Time: 34.27s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.0001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [1/10], Loss: 2.1035, Validation Macro F1: 0.6821, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [2/10], Loss: 1.2749, Validation Macro F1: 0.8758, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [3/10], Loss: 0.6492, Validation Macro F1: 0.9374, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [4/10], Loss: 0.3232, Validation Macro F1: 0.9565, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [5/10], Loss: 0.1671, Validation Macro F1: 0.9693, Time: 34.44s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [6/10], Loss: 0.0897, Validation Macro F1: 0.9662, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [7/10], Loss: 0.0526, Validation Macro F1: 0.9695, Time: 34.50s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [8/10], Loss: 0.0338, Validation Macro F1: 0.9720, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [9/10], Loss: 0.0221, Validation Macro F1: 0.9745, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [10/10], Loss: 0.0162, Validation Macro F1: 0.9729, Time: 34.24s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.0001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [1/15], Loss: 2.0741, Validation Macro F1: 0.6173, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [2/15], Loss: 1.2525, Validation Macro F1: 0.9170, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [3/15], Loss: 0.6427, Validation Macro F1: 0.9579, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [4/15], Loss: 0.3123, Validation Macro F1: 0.9633, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [5/15], Loss: 0.1576, Validation Macro F1: 0.9691, Time: 34.07s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [6/15], Loss: 0.0846, Validation Macro F1: 0.9720, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [7/15], Loss: 0.0546, Validation Macro F1: 0.9738, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [8/15], Loss: 0.0326, Validation Macro F1: 0.9729, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [9/15], Loss: 0.0223, Validation Macro F1: 0.9735, Time: 34.47s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [10/15], Loss: 0.0162, Validation Macro F1: 0.9744, Time: 34.95s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [11/15], Loss: 0.0134, Validation Macro F1: 0.9733, Time: 34.63s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [12/15], Loss: 0.0103, Validation Macro F1: 0.9764, Time: 34.48s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [13/15], Loss: 0.0085, Validation Macro F1: 0.9769, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [14/15], Loss: 0.0078, Validation Macro F1: 0.9765, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [15/15], Loss: 0.0067, Validation Macro F1: 0.9751, Time: 34.48s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.0001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [1/20], Loss: 2.0894, Validation Macro F1: 0.6250, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [2/20], Loss: 1.2608, Validation Macro F1: 0.9176, Time: 34.23s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [3/20], Loss: 0.6258, Validation Macro F1: 0.9536, Time: 34.13s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [4/20], Loss: 0.3024, Validation Macro F1: 0.9632, Time: 34.50s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [5/20], Loss: 0.1430, Validation Macro F1: 0.9698, Time: 34.45s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [6/20], Loss: 0.0801, Validation Macro F1: 0.9682, Time: 34.52s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [7/20], Loss: 0.0477, Validation Macro F1: 0.9718, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [8/20], Loss: 0.0295, Validation Macro F1: 0.9722, Time: 34.05s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [9/20], Loss: 0.0195, Validation Macro F1: 0.9731, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [10/20], Loss: 0.0147, Validation Macro F1: 0.9718, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [11/20], Loss: 0.0117, Validation Macro F1: 0.9742, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [12/20], Loss: 0.0092, Validation Macro F1: 0.9740, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [13/20], Loss: 0.0080, Validation Macro F1: 0.9749, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [14/20], Loss: 0.0071, Validation Macro F1: 0.9749, Time: 34.51s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [15/20], Loss: 0.0070, Validation Macro F1: 0.9760, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [16/20], Loss: 0.0057, Validation Macro F1: 0.9749, Time: 34.24s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [17/20], Loss: 0.0059, Validation Macro F1: 0.9747, Time: 34.13s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [18/20], Loss: 0.0050, Validation Macro F1: 0.9736, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [19/20], Loss: 0.0049, Validation Macro F1: 0.9749, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.0001, Epoch [20/20], Loss: 0.0045, Validation Macro F1: 0.9740, Time: 34.31s\n",
      "Early stopping at epoch 20\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.001, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [1/10], Loss: 1.2155, Validation Macro F1: 0.9640, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [2/10], Loss: 0.0460, Validation Macro F1: 0.9671, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [3/10], Loss: 0.0093, Validation Macro F1: 0.9711, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [4/10], Loss: 0.0027, Validation Macro F1: 0.9719, Time: 34.24s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [5/10], Loss: 0.0026, Validation Macro F1: 0.9716, Time: 34.14s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [6/10], Loss: 0.0068, Validation Macro F1: 0.9725, Time: 34.11s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [7/10], Loss: 0.0045, Validation Macro F1: 0.9606, Time: 34.25s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [8/10], Loss: 0.0047, Validation Macro F1: 0.9620, Time: 34.34s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [9/10], Loss: 0.0014, Validation Macro F1: 0.9626, Time: 34.25s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [10/10], Loss: 0.0023, Validation Macro F1: 0.9705, Time: 34.33s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.001, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [1/15], Loss: 1.5932, Validation Macro F1: 0.9448, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [2/15], Loss: 0.1613, Validation Macro F1: 0.9643, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [3/15], Loss: 0.0543, Validation Macro F1: 0.9712, Time: 34.06s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [4/15], Loss: 0.0170, Validation Macro F1: 0.9654, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [5/15], Loss: 0.0076, Validation Macro F1: 0.9648, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [6/15], Loss: 0.0039, Validation Macro F1: 0.9719, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [7/15], Loss: 0.0014, Validation Macro F1: 0.9668, Time: 34.07s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [8/15], Loss: 0.0016, Validation Macro F1: 0.9660, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [9/15], Loss: 0.0010, Validation Macro F1: 0.9674, Time: 34.33s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [10/15], Loss: 0.0006, Validation Macro F1: 0.9688, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [11/15], Loss: 0.0044, Validation Macro F1: 0.9673, Time: 34.11s\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.001, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [1/20], Loss: 1.3700, Validation Macro F1: 0.9564, Time: 34.11s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [2/20], Loss: 0.0723, Validation Macro F1: 0.9689, Time: 35.24s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [3/20], Loss: 0.0134, Validation Macro F1: 0.9536, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [4/20], Loss: 0.0107, Validation Macro F1: 0.9723, Time: 34.12s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [5/20], Loss: 0.0019, Validation Macro F1: 0.9670, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [6/20], Loss: 0.0055, Validation Macro F1: 0.9701, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [7/20], Loss: 0.0009, Validation Macro F1: 0.9663, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [8/20], Loss: 0.0007, Validation Macro F1: 0.9573, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.001, Epoch [9/20], Loss: 0.0034, Validation Macro F1: 0.9707, Time: 34.31s\n",
      "Early stopping at epoch 9\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.01, Num Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [1/10], Loss: 3.2879, Validation Macro F1: 0.2555, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [2/10], Loss: 3.3281, Validation Macro F1: 0.0692, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [3/10], Loss: 2.0931, Validation Macro F1: 0.4642, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [4/10], Loss: 1.0540, Validation Macro F1: 0.6102, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [5/10], Loss: 0.5927, Validation Macro F1: 0.7960, Time: 34.09s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [6/10], Loss: 0.2406, Validation Macro F1: 0.9240, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [7/10], Loss: 0.1116, Validation Macro F1: 0.9320, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [8/10], Loss: 0.0670, Validation Macro F1: 0.9408, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [9/10], Loss: 0.0244, Validation Macro F1: 0.9473, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [10/10], Loss: 0.0212, Validation Macro F1: 0.9480, Time: 34.11s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.01, Num Epochs: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [1/15], Loss: 3.3979, Validation Macro F1: 0.0794, Time: 34.24s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [2/15], Loss: 3.0934, Validation Macro F1: 0.2328, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [3/15], Loss: 1.8679, Validation Macro F1: 0.4414, Time: 34.12s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [4/15], Loss: 1.1840, Validation Macro F1: 0.5177, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [5/15], Loss: 0.8099, Validation Macro F1: 0.6696, Time: 34.23s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [6/15], Loss: 0.5926, Validation Macro F1: 0.7122, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [7/15], Loss: 0.3805, Validation Macro F1: 0.8526, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [8/15], Loss: 0.2385, Validation Macro F1: 0.9103, Time: 34.12s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [9/15], Loss: 0.1350, Validation Macro F1: 0.9208, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [10/15], Loss: 0.0653, Validation Macro F1: 0.9239, Time: 34.10s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [11/15], Loss: 0.0395, Validation Macro F1: 0.9341, Time: 34.48s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [12/15], Loss: 0.0223, Validation Macro F1: 0.9264, Time: 34.25s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [13/15], Loss: 0.0132, Validation Macro F1: 0.9179, Time: 34.12s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [14/15], Loss: 0.0258, Validation Macro F1: 0.9267, Time: 34.27s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [15/15], Loss: 0.0523, Validation Macro F1: 0.9313, Time: 34.15s\n",
      "\n",
      "Configuration: fc+encoder10+encoder11, Learning Rate: 0.01, Num Epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [1/20], Loss: 3.7331, Validation Macro F1: 0.1381, Time: 34.28s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [2/20], Loss: 4.3614, Validation Macro F1: 0.0884, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [3/20], Loss: 3.3803, Validation Macro F1: 0.1980, Time: 34.52s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [4/20], Loss: 1.8632, Validation Macro F1: 0.3098, Time: 34.41s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [5/20], Loss: 1.3158, Validation Macro F1: 0.5601, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [6/20], Loss: 0.8540, Validation Macro F1: 0.6637, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [7/20], Loss: 0.5303, Validation Macro F1: 0.7390, Time: 34.31s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [8/20], Loss: 0.3184, Validation Macro F1: 0.8722, Time: 34.25s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [9/20], Loss: 0.1959, Validation Macro F1: 0.9215, Time: 34.30s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [10/20], Loss: 0.1204, Validation Macro F1: 0.9427, Time: 34.51s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [11/20], Loss: 0.0503, Validation Macro F1: 0.9413, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [12/20], Loss: 0.0297, Validation Macro F1: 0.9472, Time: 34.12s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [13/20], Loss: 0.0146, Validation Macro F1: 0.9452, Time: 34.23s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [14/20], Loss: 0.0159, Validation Macro F1: 0.9474, Time: 34.13s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [15/20], Loss: 0.0067, Validation Macro F1: 0.9471, Time: 34.29s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [16/20], Loss: 0.0104, Validation Macro F1: 0.9470, Time: 34.08s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [17/20], Loss: 0.0049, Validation Macro F1: 0.9469, Time: 34.32s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [18/20], Loss: 0.0158, Validation Macro F1: 0.9417, Time: 34.26s\n",
      "Config: fc+encoder10+encoder11, LR: 0.01, Epoch [19/20], Loss: 0.0021, Validation Macro F1: 0.9445, Time: 34.30s\n",
      "Early stopping at epoch 19\n",
      "\n",
      "Best Configuration: fc+encoder11, LR: 0.001, Num Epochs: 20 with Macro F1 Score: 0.978274911488878\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Vision Transformer (ViT)\n",
    "model_vit = deepcopy(model2)  # assuming model2 is a pretrained Vision Transformer (ViT)\n",
    "model_vit = model_vit.to(device)\n",
    "\n",
    "# Define learning rates and number of epochs for grid search\n",
    "vit_learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "vit_num_epochs_list = [10, 15, 20]\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Call the updated training function without the autoencoder\n",
    "train_model_with_grid_search(\n",
    "    model=model_vit,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_classes=num_classes,\n",
    "    learning_rates=vit_learning_rates,\n",
    "    num_epochs_list=vit_num_epochs_list,\n",
    "    log_filename='vit_training_log.csv',\n",
    "    model_name='vit',\n",
    "    batch_size=batch_size,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "G:\\Anaconda3\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\惟神君\\AppData\\Local\\Temp\\ipykernel_16392\\432782011.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_vit.load_state_dict(torch.load('logs/best_model_vit.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ViT model with pretrained weights\n",
    "best_model_vit = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Replace the final head to match the number of classes\n",
    "best_model_vit.heads.head = nn.Linear(best_model_vit.heads.head.in_features, num_classes)\n",
    "\n",
    "# Load the best model weights onto the appropriate device\n",
    "best_model_vit.load_state_dict(torch.load('logs/best_model_vit.pth', map_location=device))\n",
    "\n",
    "# Move the model to the device (if not already on the correct device)\n",
    "best_model_vit = best_model_vit.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model_vit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.9125%\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test_model(best_model_vit, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score: 0.9791382683901115\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.99      0.99       800\n",
      "     Class 1       0.99      0.99      0.99       800\n",
      "     Class 2       1.00      0.97      0.98       800\n",
      "     Class 3       0.98      0.96      0.97       800\n",
      "     Class 4       0.97      0.95      0.96       800\n",
      "     Class 5       0.96      0.98      0.97       800\n",
      "     Class 6       0.95      0.98      0.96       800\n",
      "     Class 7       0.99      0.99      0.99       800\n",
      "     Class 8       0.99      0.98      0.99       800\n",
      "     Class 9       0.97      1.00      0.98       800\n",
      "\n",
      "    accuracy                           0.98      8000\n",
      "   macro avg       0.98      0.98      0.98      8000\n",
      "weighted avg       0.98      0.98      0.98      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to calculate and print F1-scores\n",
    "test_model_with_f1(best_model_vit, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
